"""
NoCç»“æœåˆ†æå™¨
é€šç”¨çš„NoCæ€§èƒ½åˆ†æå·¥å…·ï¼ŒåŒ…å«å¸¦å®½ã€å»¶è¿Ÿã€æµé‡åˆ†æç­‰åŠŸèƒ½
æ”¯æŒå¤šç§NoCæ‹“æ‰‘ï¼ˆCrossRingã€Meshç­‰ï¼‰
"""

from typing import Dict, List, Optional, Any
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Rectangle, FancyArrowPatch
from matplotlib import font_manager
import sys
import matplotlib
import logging

# è®¾ç½®matplotlibå­—ä½“ç®¡ç†å™¨çš„æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
logging.getLogger("matplotlib.font_manager").setLevel(logging.ERROR)

# ç¦ç”¨å­—ä½“ç¼ºå¤±è­¦å‘Š
import warnings

warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")

if sys.platform == "darwin":  # macOS çš„ç³»ç»Ÿæ ‡è¯†æ˜¯ 'darwin'
    matplotlib.use("macosx")  # ä»…åœ¨ macOS ä¸Šä½¿ç”¨è¯¥åç«¯

# å¯¼å…¥è·¨å¹³å°å­—ä½“é…ç½®
from src.utils.font_config import configure_matplotlib_fonts

# é…ç½®matplotlibå­—ä½“
configure_matplotlib_fonts()

import networkx as nx
import logging
import time
import os
import json
from dataclasses import dataclass
from enum import Enum


class RequestType(Enum):
    """è¯·æ±‚ç±»å‹"""

    READ = "read"
    WRITE = "write"
    ALL = "all"


@dataclass
class RequestInfo:
    """è¯·æ±‚ä¿¡æ¯æ•°æ®ç»“æ„ï¼ˆä¸è€ç‰ˆæœ¬å…¼å®¹ï¼‰"""

    packet_id: str
    start_time: int  # ns
    end_time: int  # ns
    rn_end_time: int  # ns (RNç«¯å£ç»“æŸæ—¶é—´)
    sn_end_time: int  # ns (SNç«¯å£ç»“æŸæ—¶é—´)
    req_type: str  # 'read' or 'write'
    source_node: int
    dest_node: int
    source_type: str
    dest_type: str
    burst_length: int
    total_bytes: int
    cmd_latency: int
    data_latency: int
    transaction_latency: int

    # è¯¦ç»†çš„cycleæ—¶é—´æˆ³å­—æ®µï¼ˆä¿æŒcycleæ ¼å¼ï¼‰
    cmd_entry_cake0_cycle: int = -1  # cmdè¿›å…¥Cake0
    cmd_entry_noc_from_cake0_cycle: int = -1  # cmdä»Cake0è¿›å…¥NoC
    cmd_entry_noc_from_cake1_cycle: int = -1  # cmdä»Cake1è¿›å…¥NoC
    cmd_received_by_cake0_cycle: int = -1  # cmdä»NoCè¿›å…¥Cake0
    cmd_received_by_cake1_cycle: int = -1  # cmdä»NoCè¿›å…¥Cake1
    data_entry_noc_from_cake0_cycle: int = -1  # dataä»Cake0è¿›å…¥NoC(å†™)
    data_entry_noc_from_cake1_cycle: int = -1  # dataä»Cake1è¿›å…¥NoC(è¯»)
    data_received_complete_cycle: int = -1  # dataæ¥æ”¶å®Œæˆ


@dataclass
class WorkingInterval:
    """å·¥ä½œåŒºé—´æ•°æ®ç»“æ„ï¼ˆä¸è€ç‰ˆæœ¬å…¼å®¹ï¼‰"""

    start_time: int
    end_time: int
    duration: int
    flit_count: int
    total_bytes: int
    request_count: int

    @property
    def bandwidth(self) -> float:
        """åŒºé—´å†…å¹³å‡å¸¦å®½ (GB/s)"""
        return self.total_bytes / self.duration if self.duration > 0 else 0.0


class ResultAnalyzer:
    """é€šç”¨NoCç»“æœåˆ†æå™¨"""

    def __init__(self):
        # å­—ä½“é…ç½®å·²åœ¨æ¨¡å—çº§åˆ«é€šè¿‡configure_matplotlib_fonts()å®Œæˆ

        # ç”¨äºå­˜å‚¨å¸¦å®½æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä»¿ç…§æ—§ç‰ˆæœ¬ï¼‰
        self.bandwidth_time_series = defaultdict(lambda: {"time": [], "start_times": [], "bytes": []})

    def convert_tracker_to_request_info(self, request_tracker, config) -> List[RequestInfo]:
        """è½¬æ¢RequestTrackeræ•°æ®ä¸ºRequestInfoæ ¼å¼ï¼ˆä½¿ç”¨æ­£ç¡®çš„å»¶è¿Ÿè®¡ç®—ï¼‰"""
        requests = []

        # è·å–é…ç½®å‚æ•°
        network_frequency = getattr(config.basic_config, "NETWORK_FREQUENCY", 2.0) if hasattr(config, "basic_config") else 2.0
        # cycleæ—¶é—´ï¼š1000 / frequency (ns per cycle) - ä¾‹å¦‚2GHz = 0.5ns per cycle
        cycle_time_ns = 1000.0 / (network_frequency * 1000)  # frequencyæ˜¯GHzï¼Œè½¬æ¢ä¸ºns

        for req_id, lifecycle in request_tracker.completed_requests.items():
            # ä½¿ç”¨å®é™…çš„æ—¶é—´æˆ³
            # æ—¶é—´è½¬æ¢ï¼šcycle -> ns
            start_time = int(lifecycle.created_cycle * cycle_time_ns)
            end_time = int(lifecycle.completed_cycle * cycle_time_ns)

            # æå–source_typeå’Œdest_type
            source_type = "unknown"
            dest_type = "unknown"

            # ç›´æ¥ä»æ‰€æœ‰flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            cmd_entry_cake0_cycle = np.inf
            cmd_entry_noc_from_cake0_cycle = np.inf
            cmd_entry_noc_from_cake1_cycle = np.inf
            cmd_received_by_cake0_cycle = np.inf
            cmd_received_by_cake1_cycle = np.inf
            data_entry_noc_from_cake0_cycle = np.inf
            data_entry_noc_from_cake1_cycle = np.inf
            data_received_complete_cycle = np.inf

            # ä»request flitsä¸­æ”¶é›†æ—¶é—´æˆ³å’ŒIPç±»å‹
            for flit in lifecycle.request_flits:
                # æå–IPç±»å‹ä¿¡æ¯
                if hasattr(flit, "source_type") and source_type == "unknown":
                    source_type = flit.source_type
                if hasattr(flit, "destination_type") and dest_type == "unknown":
                    dest_type = flit.destination_type

                # æ”¶é›†æ—¶é—´æˆ³
                if hasattr(flit, "cmd_entry_cake0_cycle") and flit.cmd_entry_cake0_cycle < np.inf:
                    cmd_entry_cake0_cycle = min(cmd_entry_cake0_cycle, flit.cmd_entry_cake0_cycle)
                if hasattr(flit, "cmd_entry_noc_from_cake0_cycle") and flit.cmd_entry_noc_from_cake0_cycle < np.inf:
                    cmd_entry_noc_from_cake0_cycle = min(cmd_entry_noc_from_cake0_cycle, flit.cmd_entry_noc_from_cake0_cycle)
                if hasattr(flit, "cmd_entry_noc_from_cake1_cycle") and flit.cmd_entry_noc_from_cake1_cycle < np.inf:
                    cmd_entry_noc_from_cake1_cycle = min(cmd_entry_noc_from_cake1_cycle, flit.cmd_entry_noc_from_cake1_cycle)
                if hasattr(flit, "cmd_received_by_cake0_cycle") and flit.cmd_received_by_cake0_cycle < np.inf:
                    cmd_received_by_cake0_cycle = min(cmd_received_by_cake0_cycle, flit.cmd_received_by_cake0_cycle)
                if hasattr(flit, "cmd_received_by_cake1_cycle") and flit.cmd_received_by_cake1_cycle < np.inf:
                    cmd_received_by_cake1_cycle = min(cmd_received_by_cake1_cycle, flit.cmd_received_by_cake1_cycle)

            # ä»response flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            for flit in lifecycle.response_flits:
                if hasattr(flit, "cmd_received_by_cake0_cycle") and flit.cmd_received_by_cake0_cycle < np.inf:
                    cmd_received_by_cake0_cycle = min(cmd_received_by_cake0_cycle, flit.cmd_received_by_cake0_cycle)
                if hasattr(flit, "cmd_received_by_cake1_cycle") and flit.cmd_received_by_cake1_cycle < np.inf:
                    cmd_received_by_cake1_cycle = min(cmd_received_by_cake1_cycle, flit.cmd_received_by_cake1_cycle)

            # ä»data flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            for flit in lifecycle.data_flits:
                if hasattr(flit, "data_entry_noc_from_cake0_cycle") and flit.data_entry_noc_from_cake0_cycle < np.inf:
                    data_entry_noc_from_cake0_cycle = min(data_entry_noc_from_cake0_cycle, flit.data_entry_noc_from_cake0_cycle)
                if hasattr(flit, "data_entry_noc_from_cake1_cycle") and flit.data_entry_noc_from_cake1_cycle < np.inf:
                    data_entry_noc_from_cake1_cycle = min(data_entry_noc_from_cake1_cycle, flit.data_entry_noc_from_cake1_cycle)
                if hasattr(flit, "data_received_complete_cycle") and flit.data_received_complete_cycle < np.inf:
                    data_received_complete_cycle = min(data_received_complete_cycle, flit.data_received_complete_cycle)

            # æŒ‰ç…§BaseFlitçš„calculate_latenciesæ–¹æ³•è®¡ç®—å»¶è¿Ÿ
            cmd_latency = np.inf
            data_latency = np.inf
            transaction_latency = np.inf

            # å‘½ä»¤å»¶è¿Ÿï¼šæ ¹æ®è¯»å†™ç±»å‹ä¸åŒ
            if lifecycle.op_type == "read":
                # è¯»æ“ä½œï¼šcmd_received_by_cake1_cycle - cmd_entry_noc_from_cake0_cycle
                if cmd_entry_noc_from_cake0_cycle < np.inf and cmd_received_by_cake1_cycle < np.inf:
                    cmd_latency = cmd_received_by_cake1_cycle - cmd_entry_noc_from_cake0_cycle
            elif lifecycle.op_type == "write":
                # å†™æ“ä½œï¼šcmd_received_by_cake0_cycle - cmd_entry_noc_from_cake0_cycle
                if cmd_entry_noc_from_cake0_cycle < np.inf and cmd_received_by_cake0_cycle < np.inf:
                    cmd_latency = cmd_received_by_cake0_cycle - cmd_entry_noc_from_cake0_cycle

            # æ•°æ®å»¶è¿Ÿï¼šæ ¹æ®è¯»å†™ç±»å‹ä¸åŒ
            if lifecycle.op_type == "read":
                # è¯»æ“ä½œï¼šdata_received_complete_cycle - data_entry_noc_from_cake1_cycle
                if data_entry_noc_from_cake1_cycle < np.inf and data_received_complete_cycle < np.inf:
                    data_latency = data_received_complete_cycle - data_entry_noc_from_cake1_cycle
            elif lifecycle.op_type == "write":
                # å†™æ“ä½œï¼šdata_received_complete_cycle - data_entry_noc_from_cake0_cycle
                if data_entry_noc_from_cake0_cycle < np.inf and data_received_complete_cycle < np.inf:
                    data_latency = data_received_complete_cycle - data_entry_noc_from_cake0_cycle

            # äº‹åŠ¡å»¶è¿Ÿï¼šdata_received_complete_cycle - cmd_entry_cake0_cycle
            if cmd_entry_cake0_cycle < np.inf and data_received_complete_cycle < np.inf:
                transaction_latency = data_received_complete_cycle - cmd_entry_cake0_cycle

            # å°†cycleå»¶è¿Ÿè½¬æ¢ä¸ºns
            cmd_latency_ns = int(cmd_latency * cycle_time_ns) if cmd_latency < np.inf else 0
            data_latency_ns = int(data_latency * cycle_time_ns) if data_latency < np.inf else 0
            transaction_latency_ns = int(transaction_latency * cycle_time_ns) if transaction_latency < np.inf else 0

            # è®¡ç®—RNå’ŒSNç«¯å£ç»“æŸæ—¶é—´ï¼ˆæŒ‰ç…§æ—§ç‰ˆæœ¬é€»è¾‘åŒºåˆ†è¯»å†™æ“ä½œï¼‰
            if lifecycle.op_type == "read":
                # è¯»è¯·æ±‚ï¼šRNæ”¶åˆ°æ•°æ®æ—¶ç»“æŸï¼ŒSNå‘å‡ºæ•°æ®æ—¶ç»“æŸ
                rn_end_time = int(data_received_complete_cycle * cycle_time_ns) if data_received_complete_cycle < np.inf else end_time
                sn_end_time = int(data_entry_noc_from_cake1_cycle * cycle_time_ns) if data_entry_noc_from_cake1_cycle < np.inf else end_time
            else:  # write
                # å†™è¯·æ±‚ï¼šRNå‘å‡ºæ•°æ®æ—¶ç»“æŸï¼ŒSNæ”¶åˆ°æ•°æ®æ—¶ç»“æŸ
                rn_end_time = int(data_entry_noc_from_cake0_cycle * cycle_time_ns) if data_entry_noc_from_cake0_cycle < np.inf else end_time
                sn_end_time = int(data_received_complete_cycle * cycle_time_ns) if data_received_complete_cycle < np.inf else end_time

            # è®¡ç®—å­—èŠ‚æ•°
            burst_length = lifecycle.burst_size
            total_bytes = burst_length * 128  # 128å­—èŠ‚/flit

            # ç›´æ¥ä¿å­˜cycleæ—¶é—´æˆ³ï¼ˆä¸è½¬æ¢ä¸ºnsï¼‰
            cmd_entry_cake0_cycle_val = int(cmd_entry_cake0_cycle) if cmd_entry_cake0_cycle < np.inf else -1
            cmd_entry_noc_from_cake0_cycle_val = int(cmd_entry_noc_from_cake0_cycle) if cmd_entry_noc_from_cake0_cycle < np.inf else -1
            cmd_entry_noc_from_cake1_cycle_val = int(cmd_entry_noc_from_cake1_cycle) if cmd_entry_noc_from_cake1_cycle < np.inf else -1
            cmd_received_by_cake0_cycle_val = int(cmd_received_by_cake0_cycle) if cmd_received_by_cake0_cycle < np.inf else -1
            cmd_received_by_cake1_cycle_val = int(cmd_received_by_cake1_cycle) if cmd_received_by_cake1_cycle < np.inf else -1
            data_entry_noc_from_cake0_cycle_val = int(data_entry_noc_from_cake0_cycle) if data_entry_noc_from_cake0_cycle < np.inf else -1
            data_entry_noc_from_cake1_cycle_val = int(data_entry_noc_from_cake1_cycle) if data_entry_noc_from_cake1_cycle < np.inf else -1
            data_received_complete_cycle_val = int(data_received_complete_cycle) if data_received_complete_cycle < np.inf else -1

            request_info = RequestInfo(
                packet_id=str(req_id),
                start_time=start_time,
                end_time=end_time,
                rn_end_time=rn_end_time,
                sn_end_time=sn_end_time,
                req_type=lifecycle.op_type,
                source_node=lifecycle.source,
                dest_node=lifecycle.destination,
                source_type=source_type,
                dest_type=dest_type,
                burst_length=burst_length,
                total_bytes=total_bytes,
                cmd_latency=cmd_latency_ns,
                data_latency=data_latency_ns,
                transaction_latency=transaction_latency_ns,
                # æ·»åŠ è¯¦ç»†çš„cycleæ—¶é—´æˆ³
                cmd_entry_cake0_cycle=cmd_entry_cake0_cycle_val,
                cmd_entry_noc_from_cake0_cycle=cmd_entry_noc_from_cake0_cycle_val,
                cmd_entry_noc_from_cake1_cycle=cmd_entry_noc_from_cake1_cycle_val,
                cmd_received_by_cake0_cycle=cmd_received_by_cake0_cycle_val,
                cmd_received_by_cake1_cycle=cmd_received_by_cake1_cycle_val,
                data_entry_noc_from_cake0_cycle=data_entry_noc_from_cake0_cycle_val,
                data_entry_noc_from_cake1_cycle=data_entry_noc_from_cake1_cycle_val,
                data_received_complete_cycle=data_received_complete_cycle_val,
            )
            requests.append(request_info)

        return requests

    def calculate_working_intervals(self, requests: List[RequestInfo], min_gap_threshold: int = 200) -> List[WorkingInterval]:
        """è®¡ç®—å·¥ä½œåŒºé—´ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not requests:
            return []

        # åˆ›å»ºæ—¶é—´ç‚¹äº‹ä»¶åˆ—è¡¨
        events = []
        for req in requests:
            events.append((req.start_time, "start", req.packet_id))
            events.append((req.end_time, "end", req.packet_id))

        # æŒ‰æ—¶é—´æ’åº
        events.sort()

        # è¯†åˆ«å·¥ä½œåŒºé—´
        raw_intervals = []
        active_requests = set()
        current_start = None

        for time_point, event_type, packet_id in events:
            if event_type == "start":
                if not active_requests:
                    current_start = time_point
                active_requests.add(packet_id)
            else:  # 'end'
                active_requests.discard(packet_id)
                if not active_requests and current_start is not None:
                    # å·¥ä½œåŒºé—´ç»“æŸ
                    raw_intervals.append((current_start, time_point))
                    current_start = None

        # å¤„ç†æœ€åæœªç»“æŸçš„åŒºé—´
        if active_requests and current_start is not None:
            last_end = max(req.end_time for req in requests)
            raw_intervals.append((current_start, last_end))

        # åˆå¹¶ç›¸è¿‘åŒºé—´
        merged_intervals = self._merge_close_intervals(raw_intervals, min_gap_threshold)

        # æ„å»ºWorkingIntervalå¯¹è±¡
        working_intervals = []
        for start, end in merged_intervals:
            # æ‰¾åˆ°è¯¥åŒºé—´å†…çš„æ‰€æœ‰è¯·æ±‚
            interval_requests = [req for req in requests if req.start_time < end and req.end_time > start]

            if not interval_requests:
                continue

            # è®¡ç®—åŒºé—´ç»Ÿè®¡
            total_bytes = sum(req.total_bytes for req in interval_requests)
            flit_count = sum(req.burst_length for req in interval_requests)

            interval = WorkingInterval(start_time=start, end_time=end, duration=end - start, flit_count=flit_count, total_bytes=total_bytes, request_count=len(interval_requests))
            working_intervals.append(interval)

        return working_intervals

    def _merge_close_intervals(self, intervals: List[tuple], min_gap_threshold: int) -> List[tuple]:
        """åˆå¹¶ç›¸è¿‘çš„æ—¶é—´åŒºé—´ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not intervals:
            return []

        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        sorted_intervals = sorted(intervals)
        merged = [sorted_intervals[0]]

        for current_start, current_end in sorted_intervals[1:]:
            last_start, last_end = merged[-1]

            # å¦‚æœé—´éš™å°äºé˜ˆå€¼ï¼Œåˆ™åˆå¹¶
            if current_start - last_end <= min_gap_threshold:
                merged[-1] = (last_start, max(last_end, current_end))
            else:
                merged.append((current_start, current_end))

        return merged

    def calculate_bandwidth_metrics(self, requests: List[RequestInfo], operation_type: str = None, min_gap_threshold: int = 200, endpoint_type: str = "network") -> Dict[str, Any]:
        """è®¡ç®—å¸¦å®½æŒ‡æ ‡ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬å®Œæ•´é€»è¾‘ï¼‰

        Args:
            requests: è¯·æ±‚åˆ—è¡¨
            operation_type: æ“ä½œç±»å‹ ("read", "write", None)
            min_gap_threshold: æœ€å°é—´éš™é˜ˆå€¼
            endpoint_type: ç«¯ç‚¹ç±»å‹ ("network", "rn", "sn")
        """
        if not requests:
            return {}

        # ç­›é€‰è¯·æ±‚å¹¶åˆ›å»ºä¸´æ—¶è¯·æ±‚åˆ—è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç»“æŸæ—¶é—´ï¼‰
        filtered_requests = []
        for req in requests:
            if operation_type is not None and req.req_type != operation_type:
                continue

            # æ ¹æ®endpoint_typeé€‰æ‹©æ­£ç¡®çš„ç»“æŸæ—¶é—´
            if endpoint_type == "rn":
                end_time = req.rn_end_time
            elif endpoint_type == "sn":
                end_time = req.sn_end_time
            else:  # network
                end_time = req.end_time

            # åˆ›å»ºä¸´æ—¶è¯·æ±‚å¯¹è±¡ï¼Œä½¿ç”¨æ­£ç¡®çš„ç»“æŸæ—¶é—´
            temp_req = RequestInfo(
                packet_id=req.packet_id,
                start_time=req.start_time,
                end_time=end_time,
                rn_end_time=req.rn_end_time,
                sn_end_time=req.sn_end_time,
                req_type=req.req_type,
                source_node=req.source_node,
                dest_node=req.dest_node,
                source_type=req.source_type,
                dest_type=req.dest_type,
                burst_length=req.burst_length,
                total_bytes=req.total_bytes,
                cmd_latency=req.cmd_latency,
                data_latency=req.data_latency,
                transaction_latency=req.transaction_latency,
            )
            filtered_requests.append(temp_req)

        if not filtered_requests:
            return {}

        # è®¡ç®—å·¥ä½œåŒºé—´
        working_intervals = self.calculate_working_intervals(filtered_requests, min_gap_threshold)

        # ç½‘ç»œå·¥ä½œæ—¶é—´çª—å£
        network_start = min(req.start_time for req in filtered_requests)
        network_end = max(req.end_time for req in filtered_requests)
        total_network_time = network_end - network_start

        # æ€»å·¥ä½œæ—¶é—´å’Œæ€»å­—èŠ‚æ•°
        total_working_time = sum(interval.duration for interval in working_intervals)
        total_bytes = sum(req.total_bytes for req in filtered_requests)

        # è®¡ç®—éåŠ æƒå¸¦å®½ï¼šæ€»æ•°æ®é‡ / ç½‘ç»œæ€»æ—¶é—´
        unweighted_bandwidth = (total_bytes / total_network_time) if total_network_time > 0 else 0.0

        # è®¡ç®—åŠ æƒå¸¦å®½ï¼šå„åŒºé—´å¸¦å®½æŒ‰flitæ•°é‡åŠ æƒå¹³å‡
        if working_intervals:
            total_weighted_bandwidth = 0.0
            total_weight = 0

            for interval in working_intervals:
                weight = interval.flit_count  # æƒé‡æ˜¯å·¥ä½œæ—¶é—´æ®µçš„flitæ•°é‡
                bandwidth = interval.bandwidth  # GB/s
                total_weighted_bandwidth += bandwidth * weight
                total_weight += weight

            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
        else:
            weighted_bandwidth = 0.0

        return {
            "éåŠ æƒå¸¦å®½_GB/s": f"{unweighted_bandwidth:.2f}",
            "åŠ æƒå¸¦å®½_GB/s": f"{weighted_bandwidth:.2f}",
            "æ€»ä¼ è¾“å­—èŠ‚æ•°": total_bytes,
            "æ€»è¯·æ±‚æ•°": len(filtered_requests),
            "å·¥ä½œåŒºé—´æ•°é‡": len(working_intervals),
            "æ€»å·¥ä½œæ—¶é—´_ns": total_working_time,
            "ç½‘ç»œæ—¶é—´_ns": total_network_time,
        }

    def _print_data_statistics(self, metrics):
        """æ‰“å°è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not metrics:
            return

        # ç»Ÿè®¡è¯»å†™è¯·æ±‚å’Œflitæ•°é‡
        read_requests = [m for m in metrics if m.req_type == "read"]
        write_requests = [m for m in metrics if m.req_type == "write"]

        read_flit_count = sum(m.burst_length for m in read_requests)
        write_flit_count = sum(m.burst_length for m in write_requests)
        total_flit_count = read_flit_count + write_flit_count

        # æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•çš„æ‰“å°è¢«ç§»åŠ¨åˆ°æ¨¡å‹çš„_print_traffic_statisticsä¸­
        # é¿å…é‡å¤æ‰“å°

    # def _print_detailed_latency_analysis(self, latency_metrics, metrics):
    #     """æ‰“å°è¯¦ç»†çš„å»¶è¿Ÿåˆ†æç»“æœ"""
    #     if not latency_metrics or not metrics:
    #         return

    #     print("\nå»¶è¿Ÿç»Ÿè®¡ (å•ä½: cycle)")

    #     # æŒ‰è¯»å†™åˆ†ç±»ç»Ÿè®¡å»¶è¿Ÿ
    #     read_metrics = [m for m in metrics if m.req_type == "read"]
    #     write_metrics = [m for m in metrics if m.req_type == "write"]

    #     # CMDå»¶è¿Ÿ
    #     if read_metrics:
    #         read_cmd_avg = sum(m.cmd_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
    #         read_cmd_max = max(m.cmd_latency for m in read_metrics) if len(read_metrics) > 0 else 0
    #     else:
    #         read_cmd_avg = read_cmd_max = 0

    #     if write_metrics:
    #         write_cmd_avg = sum(m.cmd_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
    #         write_cmd_max = max(m.cmd_latency for m in write_metrics) if len(write_metrics) > 0 else 0
    #     else:
    #         write_cmd_avg = write_cmd_max = 0

    #     mixed_cmd_avg = sum(m.cmd_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
    #     mixed_cmd_max = max(m.cmd_latency for m in metrics) if len(metrics) > 0 else 0

    #     print(f"  CMD å»¶è¿Ÿ  - è¯»: avg {read_cmd_avg:.2f}, max {read_cmd_max}ï¼›å†™: avg {write_cmd_avg:.2f}, max {write_cmd_max}ï¼›æ··åˆ: avg {mixed_cmd_avg:.2f}, max {mixed_cmd_max}")

    #     # Dataå»¶è¿Ÿ
    #     if read_metrics:
    #         read_data_avg = sum(m.data_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
    #         read_data_max = max(m.data_latency for m in read_metrics) if len(read_metrics) > 0 else 0
    #     else:
    #         read_data_avg = read_data_max = 0

    #     if write_metrics:
    #         write_data_avg = sum(m.data_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
    #         write_data_max = max(m.data_latency for m in write_metrics) if len(write_metrics) > 0 else 0
    #     else:
    #         write_data_avg = write_data_max = 0

    #     mixed_data_avg = sum(m.data_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
    #     mixed_data_max = max(m.data_latency for m in metrics) if len(metrics) > 0 else 0

    #     print(f"  Data å»¶è¿Ÿ  - è¯»: avg {read_data_avg:.2f}, max {read_data_max}ï¼›å†™: avg {write_data_avg:.2f}, max {write_data_max}ï¼›æ··åˆ: avg {mixed_data_avg:.2f}, max {mixed_data_max}")

    #     # Transå»¶è¿Ÿ
    #     if read_metrics:
    #         read_trans_avg = sum(m.transaction_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
    #         read_trans_max = max(m.transaction_latency for m in read_metrics) if len(read_metrics) > 0 else 0
    #     else:
    #         read_trans_avg = read_trans_max = 0

    #     if write_metrics:
    #         write_trans_avg = sum(m.transaction_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
    #         write_trans_max = max(m.transaction_latency for m in write_metrics) if len(write_metrics) > 0 else 0
    #     else:
    #         write_trans_avg = write_trans_max = 0

    #     mixed_trans_avg = sum(m.transaction_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
    #     mixed_trans_max = max(m.transaction_latency for m in metrics) if len(metrics) > 0 else 0

    #     print(f"  Trans å»¶è¿Ÿ  - è¯»: avg {read_trans_avg:.2f}, max {read_trans_max}ï¼›å†™: avg {write_trans_avg:.2f}, max {write_trans_max}ï¼›æ··åˆ: avg {mixed_trans_avg:.2f}, max {mixed_trans_max}")

    #     # æ€»å¸¦å®½æ˜¾ç¤ºï¼ˆä½¿ç”¨åŠ æƒå¸¦å®½ï¼‰
    #     if "latency_metrics" in locals() and "æ€»ä½“å¸¦å®½" in latency_metrics:
    #         total_bw = latency_metrics["æ€»ä½“å¸¦å®½"].get("æ€»å¸¦å®½", {}).get("åŠ æƒå¸¦å®½_GB/s", 0)
    #         print(f"Total Bandwidth: {total_bw:.2f} GB/s")
    #     else:
    #         # ä»å¸¦å®½æŒ‡æ ‡ä¸­è·å–æ€»å¸¦å®½
    #         total_bw = 0
    #         print(f"Total Bandwidth: {total_bw:.2f} GB/s")

    #     print("=" * 60)

    def analyze_bandwidth(self, requests: List[RequestInfo], verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æå¸¦å®½æŒ‡æ ‡ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not requests:
            return {}

        # æ€»ä½“å¸¦å®½åˆ†æ
        overall_metrics = self.calculate_bandwidth_metrics(requests, operation_type=None)

        # è¯»æ“ä½œå¸¦å®½åˆ†æ
        read_metrics = self.calculate_bandwidth_metrics(requests, operation_type="read")

        # å†™æ“ä½œå¸¦å®½åˆ†æ
        write_metrics = self.calculate_bandwidth_metrics(requests, operation_type="write")

        # æ‰“å°å¸¦å®½åˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç½‘ç»œå¸¦å®½åˆ†æç»“æœæ‘˜è¦")
            print("=" * 60)
            print("ç½‘ç»œæ•´ä½“å¸¦å®½:")

        # æ˜¾ç¤ºå„ç±»å‹å¸¦å®½ï¼ˆæ€»å¸¦å®½å’ŒRN IPå¹³å‡å¸¦å®½ï¼‰
        if verbose:
            # åªè®¡ç®—RNï¼ˆDMAï¼‰IPçš„å¹³å‡å¸¦å®½
            rn_requests = [r for r in requests if hasattr(r, "source_type") and "dma" in r.source_type.lower()]
            rn_read_requests = [r for r in rn_requests if r.req_type == "read"]
            rn_write_requests = [r for r in rn_requests if r.req_type == "write"]

            # ç»Ÿè®¡RN IPæ•°é‡ï¼šä½¿ç”¨ï¼ˆèŠ‚ç‚¹IDï¼ŒIPç±»å‹ï¼‰ç»„åˆæ¥åŒºåˆ†ä¸åŒçš„RN
            rn_ips = set()
            for r in rn_requests:
                # æ¯ä¸ªèŠ‚ç‚¹çš„åŒåIPéƒ½æ˜¯ä¸åŒçš„å®ä¾‹
                rn_key = (r.source_node, r.source_type.lower())
                rn_ips.add(rn_key)
            rn_ip_count = len(rn_ips) if rn_ips else 1

            for label, metrics_data in [("è¯»å¸¦å®½", read_metrics), ("å†™å¸¦å®½", write_metrics), ("æ··åˆå¸¦å®½", overall_metrics), ("æ€»å¸¦å®½", overall_metrics)]:
                if metrics_data and isinstance(metrics_data, dict) and "åŠ æƒå¸¦å®½_GB/s" in metrics_data:
                    weighted_bw = metrics_data["åŠ æƒå¸¦å®½_GB/s"]
                    try:
                        total_bw = float(weighted_bw)
                        rn_avg_bw = total_bw / rn_ip_count if rn_ip_count > 0 else 0
                        print(f"  {label}: {total_bw:.2f} GB/s (æ€»), {rn_avg_bw:.2f} GB/s (å¹³å‡)")
                    except (ValueError, TypeError):
                        print(f"  {label}: {weighted_bw} GB/s")

        return {"æ€»ä½“å¸¦å®½": overall_metrics, "è¯»æ“ä½œå¸¦å®½": read_metrics, "å†™æ“ä½œå¸¦å®½": write_metrics}

    def analyze_latency(self, metrics, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æå»¶è¿ŸæŒ‡æ ‡"""
        if not metrics:
            return {}

        latencies = [m.transaction_latency for m in metrics]
        read_latencies = [m.transaction_latency for m in metrics if m.req_type == "read"]
        write_latencies = [m.transaction_latency for m in metrics if m.req_type == "write"]

        # CMDã€Dataã€Transactionå»¶è¿Ÿç»Ÿè®¡
        cmd_latencies = [m.cmd_latency for m in metrics]
        data_latencies = [m.data_latency for m in metrics]

        result = {
            "æ€»ä½“å»¶è¿Ÿ": {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(latencies):.2f}",
                "P95å»¶è¿Ÿ_ns": f"{np.percentile(latencies, 95):.2f}",
            }
        }

        if read_latencies:
            result["è¯»æ“ä½œå»¶è¿Ÿ"] = {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(read_latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(read_latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(read_latencies):.2f}",
            }

        if write_latencies:
            result["å†™æ“ä½œå»¶è¿Ÿ"] = {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(write_latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(write_latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(write_latencies):.2f}",
            }

        # æ‰“å°å»¶è¿Ÿåˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç½‘ç»œå»¶è¿Ÿåˆ†æç»“æœæ‘˜è¦")
            print("=" * 60)

            # æ€»ä½“å»¶è¿Ÿç»Ÿè®¡ï¼ˆåˆ†CMDã€Dataã€Transactionï¼‰
            print("æ€»ä½“å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(cmd_latencies):.2f} ns, æœ€å° {np.min(cmd_latencies):.2f} ns, æœ€å¤§ {np.max(cmd_latencies):.2f} ns")
            print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(data_latencies):.2f} ns, æœ€å° {np.min(data_latencies):.2f} ns, æœ€å¤§ {np.max(data_latencies):.2f} ns")
            print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(latencies):.2f} ns, æœ€å° {np.min(latencies):.2f} ns, æœ€å¤§ {np.max(latencies):.2f} ns")
            # print(f"  P95 Transactionå»¶è¿Ÿ: {np.percentile(latencies, 95):.2f} ns")

            # æŒ‰ç±»å‹åˆ†ç±»å»¶è¿Ÿç»Ÿè®¡
            if read_latencies:
                read_cmd = [m.cmd_latency for m in metrics if m.req_type == "read"]
                read_data = [m.data_latency for m in metrics if m.req_type == "read"]
                print(f"\nè¯»æ“ä½œå»¶è¿Ÿ:")
                print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(read_cmd):.2f} ns, æœ€å¤§ {np.max(read_cmd):.2f} ns")
                print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(read_data):.2f} ns, æœ€å¤§ {np.max(read_data):.2f} ns")
                print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(read_latencies):.2f} ns, æœ€å¤§ {np.max(read_latencies):.2f} ns")

            if write_latencies:
                write_cmd = [m.cmd_latency for m in metrics if m.req_type == "write"]
                write_data = [m.data_latency for m in metrics if m.req_type == "write"]
                print(f"\nå†™æ“ä½œå»¶è¿Ÿ:")
                print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(write_cmd):.2f} ns, æœ€å¤§ {np.max(write_cmd):.2f} ns")
                print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(write_data):.2f} ns, æœ€å¤§ {np.max(write_data):.2f} ns")
                print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(write_latencies):.2f} ns, æœ€å¤§ {np.max(write_latencies):.2f} ns")

        return result

    def analyze_port_bandwidth(self, metrics, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æç«¯å£çº§åˆ«å¸¦å®½ï¼ˆæŒ‰IPç±»å‹åˆ†ç»„ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å·¥ä½œåŒºé—´ç®—æ³•ï¼‰"""
        ip_analysis = defaultdict(lambda: {"read": [], "write": []})

        for metric in metrics:
            # æŒ‰æºIPç±»å‹åˆ†ç»„ï¼ˆè°å‘èµ·çš„è¯·æ±‚ï¼‰
            source_ip_type = metric.source_type  # 'gdma' æˆ– 'ddr'
            ip_analysis[source_ip_type.upper()][metric.req_type].append(metric)

        ip_summary = {}
        for ip_type, data in ip_analysis.items():
            read_reqs = data["read"]
            write_reqs = data["write"]

            # ä½¿ç”¨ç»Ÿä¸€çš„å·¥ä½œåŒºé—´ç®—æ³•è®¡ç®—è¯»å†™å¸¦å®½
            read_metrics = self.calculate_bandwidth_metrics(read_reqs, operation_type="read", endpoint_type="network")
            write_metrics = self.calculate_bandwidth_metrics(write_reqs, operation_type="write", endpoint_type="network")

            # æå–å¸¦å®½æ•°å€¼ï¼ˆå»é™¤GB/såç¼€å¹¶è½¬æ¢ä¸ºfloatï¼‰
            read_bw = float(read_metrics.get("éåŠ æƒå¸¦å®½_GB/s", "0.00"))
            write_bw = float(write_metrics.get("éåŠ æƒå¸¦å®½_GB/s", "0.00"))

            ip_summary[ip_type] = {
                "è¯»å¸¦å®½_GB/s": f"{read_bw:.2f}",
                "å†™å¸¦å®½_GB/s": f"{write_bw:.2f}",
                "æ€»å¸¦å®½_GB/s": f"{read_bw + write_bw:.2f}",
                "è¯»è¯·æ±‚æ•°": len(read_reqs),
                "å†™è¯·æ±‚æ•°": len(write_reqs),
                "æ€»è¯·æ±‚æ•°": len(read_reqs) + len(write_reqs),
            }

        # ç«¯å£ç»Ÿè®¡ä¸éœ€è¦æ‘˜è¦è¾“å‡ºï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ç§»é™¤ï¼‰

        return ip_summary

    def analyze_tag_data(self, model, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æTagæœºåˆ¶æ•°æ®ï¼ˆæŒ‰ç…§CrossRingè§„æ ¼è¦æ±‚çš„æ ¼å¼ï¼‰"""
        tag_analysis = {
            "Circuitsç»Ÿè®¡": {"req_h": 0, "req_v": 0, "rsp_h": 0, "rsp_v": 0, "data_h": 0, "data_v": 0},
            "Wait_cycleç»Ÿè®¡": {"req_h": 0, "req_v": 0, "rsp_h": 0, "rsp_v": 0, "data_h": 0, "data_v": 0},
            "RB_ETagç»Ÿè®¡": {"T1": 0, "T0": 0},  # æ°´å¹³CrossPointçš„E-Tagç»Ÿè®¡
            "EQ_ETagç»Ÿè®¡": {"T1": 0, "T0": 0},  # å‚ç›´CrossPointçš„E-Tagç»Ÿè®¡
            "ITagç»Ÿè®¡": {"h": 0, "v": 0},
            "Retryç»Ÿè®¡": {"read": 0, "write": 0},
        }

        # ä»NoCèŠ‚ç‚¹ä¸­æ”¶é›†ç»Ÿè®¡æ•°æ®
        try:
            for node in model.nodes.values():
                # æ”¶é›†æ¨ªå‘ç¯ç»Ÿè®¡æ•°æ®
                if hasattr(node, "horizontal_crosspoint"):
                    hcp = node.horizontal_crosspoint
                    if hasattr(hcp, "stats"):
                        stats = hcp.stats

                        # Circuitsç»Ÿè®¡ï¼ˆç»•ç¯äº‹ä»¶ï¼‰
                        tag_analysis["Circuitsç»Ÿè®¡"]["req_h"] += stats.get("bypass_events", {}).get("req", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["rsp_h"] += stats.get("bypass_events", {}).get("rsp", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["data_h"] += stats.get("bypass_events", {}).get("data", 0)

                        # I-Tagç»Ÿè®¡
                        tag_analysis["ITagç»Ÿè®¡"]["h"] += stats.get("itag_triggers", {}).get("req", 0)
                        tag_analysis["ITagç»Ÿè®¡"]["h"] += stats.get("itag_triggers", {}).get("rsp", 0)
                        tag_analysis["ITagç»Ÿè®¡"]["h"] += stats.get("itag_triggers", {}).get("data", 0)

                # æ”¶é›†çºµå‘ç¯ç»Ÿè®¡æ•°æ®
                if hasattr(node, "vertical_crosspoint"):
                    vcp = node.vertical_crosspoint
                    if hasattr(vcp, "stats"):
                        stats = vcp.stats

                        # Circuitsç»Ÿè®¡ï¼ˆç»•ç¯äº‹ä»¶ï¼‰
                        tag_analysis["Circuitsç»Ÿè®¡"]["req_v"] += stats.get("bypass_events", {}).get("req", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["rsp_v"] += stats.get("bypass_events", {}).get("rsp", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["data_v"] += stats.get("bypass_events", {}).get("data", 0)

                        # I-Tagç»Ÿè®¡
                        tag_analysis["ITagç»Ÿè®¡"]["v"] += stats.get("itag_triggers", {}).get("req", 0)
                        tag_analysis["ITagç»Ÿè®¡"]["v"] += stats.get("itag_triggers", {}).get("rsp", 0)
                        tag_analysis["ITagç»Ÿè®¡"]["v"] += stats.get("itag_triggers", {}).get("data", 0)

                # æ”¶é›†CrossPoint E-Tagç»Ÿè®¡ï¼ˆçœŸæ­£çš„E-Tagå®ç°ä½ç½®ï¼‰
                if hasattr(node, "horizontal_crosspoint") and hasattr(node.horizontal_crosspoint, "stats"):
                    h_stats = node.horizontal_crosspoint.stats
                    if "etag_upgrades" in h_stats:
                        for channel in ["req", "rsp", "data"]:
                            tag_analysis["RB_ETagç»Ÿè®¡"]["T1"] += h_stats["etag_upgrades"].get(channel, {}).get("T1", 0)
                            tag_analysis["RB_ETagç»Ÿè®¡"]["T0"] += h_stats["etag_upgrades"].get(channel, {}).get("T0", 0)

                if hasattr(node, "vertical_crosspoint") and hasattr(node.vertical_crosspoint, "stats"):
                    v_stats = node.vertical_crosspoint.stats
                    if "etag_upgrades" in v_stats:
                        for channel in ["req", "rsp", "data"]:
                            tag_analysis["EQ_ETagç»Ÿè®¡"]["T1"] += v_stats["etag_upgrades"].get(channel, {}).get("T1", 0)
                            tag_analysis["EQ_ETagç»Ÿè®¡"]["T0"] += v_stats["etag_upgrades"].get(channel, {}).get("T0", 0)

            # æ”¶é›†Retryç»Ÿè®¡å’Œç­‰å¾…å‘¨æœŸç»Ÿè®¡ï¼ˆä»æ¨¡å‹çš„IPæ¥å£ä¸­ï¼‰
            if hasattr(model, "ip_interfaces"):
                # å¤„ç†å¤šç»´IPæ¥å£ç»“æ„: ip_interfaces[node_id][ip_type]
                for node_interfaces in model.ip_interfaces.values():
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—å­—å…¸ç»“æ„
                    if isinstance(node_interfaces, dict):
                        # å¤šç»´ç»“æ„ï¼šéå†æ¯ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰IPæ¥å£
                        for ip_interface in node_interfaces.values():
                            # Retryç»Ÿè®¡
                            tag_analysis["Retryç»Ÿè®¡"]["read"] += getattr(ip_interface, "read_retry_num_stat", 0)
                            tag_analysis["Retryç»Ÿè®¡"]["write"] += getattr(ip_interface, "write_retry_num_stat", 0)

                            # ç­‰å¾…å‘¨æœŸç»Ÿè®¡
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["req_h"] += getattr(ip_interface, "req_wait_cycles_h", 0)
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["req_v"] += getattr(ip_interface, "req_wait_cycles_v", 0)
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_h"] += getattr(ip_interface, "rsp_wait_cycles_h", 0)
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_v"] += getattr(ip_interface, "rsp_wait_cycles_v", 0)
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["data_h"] += getattr(ip_interface, "data_wait_cycles_h", 0)
                            tag_analysis["Wait_cycleç»Ÿè®¡"]["data_v"] += getattr(ip_interface, "data_wait_cycles_v", 0)

                            # Circuitsç»Ÿè®¡ï¼ˆç»•ç¯å®Œæˆäº‹ä»¶ï¼‰
                            tag_analysis["Circuitsç»Ÿè®¡"]["req_h"] += getattr(ip_interface, "req_cir_h_num", 0)
                            tag_analysis["Circuitsç»Ÿè®¡"]["req_v"] += getattr(ip_interface, "req_cir_v_num", 0)
                            tag_analysis["Circuitsç»Ÿè®¡"]["rsp_h"] += getattr(ip_interface, "rsp_cir_h_num", 0)
                            tag_analysis["Circuitsç»Ÿè®¡"]["rsp_v"] += getattr(ip_interface, "rsp_cir_v_num", 0)
                            tag_analysis["Circuitsç»Ÿè®¡"]["data_h"] += getattr(ip_interface, "data_cir_h_num", 0)
                            tag_analysis["Circuitsç»Ÿè®¡"]["data_v"] += getattr(ip_interface, "data_cir_v_num", 0)
                    else:
                        # ç®€å•ç»“æ„ï¼šç›´æ¥æ˜¯IPæ¥å£å¯¹è±¡
                        ip_interface = node_interfaces
                        # Retryç»Ÿè®¡
                        tag_analysis["Retryç»Ÿè®¡"]["read"] += getattr(ip_interface, "read_retry_num_stat", 0)
                        tag_analysis["Retryç»Ÿè®¡"]["write"] += getattr(ip_interface, "write_retry_num_stat", 0)

                        # ç­‰å¾…å‘¨æœŸç»Ÿè®¡
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["req_h"] += getattr(ip_interface, "req_wait_cycles_h", 0)
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["req_v"] += getattr(ip_interface, "req_wait_cycles_v", 0)
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_h"] += getattr(ip_interface, "rsp_wait_cycles_h", 0)
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_v"] += getattr(ip_interface, "rsp_wait_cycles_v", 0)
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["data_h"] += getattr(ip_interface, "data_wait_cycles_h", 0)
                        tag_analysis["Wait_cycleç»Ÿè®¡"]["data_v"] += getattr(ip_interface, "data_wait_cycles_v", 0)

                        # Circuitsç»Ÿè®¡ï¼ˆç»•ç¯å®Œæˆäº‹ä»¶ï¼‰
                        tag_analysis["Circuitsç»Ÿè®¡"]["req_h"] += getattr(ip_interface, "req_cir_h_num", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["req_v"] += getattr(ip_interface, "req_cir_v_num", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["rsp_h"] += getattr(ip_interface, "rsp_cir_h_num", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["rsp_v"] += getattr(ip_interface, "rsp_cir_v_num", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["data_h"] += getattr(ip_interface, "data_cir_h_num", 0)
                        tag_analysis["Circuitsç»Ÿè®¡"]["data_v"] += getattr(ip_interface, "data_cir_v_num", 0)

        except Exception as e:
            print(f"æ”¶é›†Tagå’Œç»•ç¯æ•°æ®æ—¶å‡ºé”™: {e}")
            import traceback

            traceback.print_exc()

        # æ‰“å°Tagåˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç»•ç¯ä¸Tagç»Ÿè®¡")
            print("=" * 60)

            circuits = tag_analysis["Circuitsç»Ÿè®¡"]
            print(f"  è¯·æ±‚ç»•ç¯  - æ¨ªå‘: {circuits['req_h']}, çºµå‘: {circuits['req_v']}")
            print(f"  å“åº”ç»•ç¯  - æ¨ªå‘: {circuits['rsp_h']}, çºµå‘: {circuits['rsp_v']}")
            print(f"  æ•°æ®ç»•ç¯  - æ¨ªå‘: {circuits['data_h']}, çºµå‘: {circuits['data_v']}")

            wait_cycles = tag_analysis["Wait_cycleç»Ÿè®¡"]
            print(f"  è¯·æ±‚ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['req_h']}, çºµå‘: {wait_cycles['req_v']}")
            print(f"  å“åº”ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['rsp_h']}, çºµå‘: {wait_cycles['rsp_v']}")
            print(f"  æ•°æ®ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['data_h']}, çºµå‘: {wait_cycles['data_v']}")

            rb_etag = tag_analysis["RB_ETagç»Ÿè®¡"]
            print(f"  RB ETagç»Ÿè®¡ - T1: {rb_etag['T1']}, T0: {rb_etag['T0']}")

            eq_etag = tag_analysis["EQ_ETagç»Ÿè®¡"]
            print(f"  EQ ETagç»Ÿè®¡ - T1: {eq_etag['T1']}, T0: {eq_etag['T0']}")

            itag = tag_analysis["ITagç»Ÿè®¡"]
            print(f"  ITagç»Ÿè®¡ - æ¨ªå‘: {itag['h']}, çºµå‘: {itag['v']}")

            retry = tag_analysis["Retryç»Ÿè®¡"]
            print(f"  Retryæ•°é‡ - è¯»: {retry['read']}, å†™: {retry['write']}")

        return tag_analysis

    def _collect_bandwidth_time_series_data(self, metrics):
        """æ”¶é›†å¸¦å®½æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä»¿ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        self.bandwidth_time_series.clear()

        # æŒ‰ç«¯å£ç±»å‹åˆ†ç»„è¯·æ±‚
        for req in metrics:
            # ç”Ÿæˆç«¯å£é”®åï¼ˆç±»ä¼¼è€ç‰ˆæœ¬çš„æ ¼å¼ï¼‰
            if hasattr(req, "source_type") and hasattr(req, "dest_type"):
                if req.req_type == "read":
                    port_key = f"{req.source_type} read {req.dest_type}"
                else:
                    port_key = f"{req.source_type} write {req.dest_type}"
            else:
                # å¦‚æœæ²¡æœ‰ç«¯å£ç±»å‹ä¿¡æ¯ï¼Œä½¿ç”¨è¯»å†™ç±»å‹
                port_key = f"{req.req_type}"

            # æ·»åŠ åˆ°æ—¶é—´åºåˆ—æ•°æ®
            self.bandwidth_time_series[port_key]["time"].append(req.end_time)
            self.bandwidth_time_series[port_key]["start_times"].append(req.start_time)
            self.bandwidth_time_series[port_key]["bytes"].append(req.total_bytes)

    def plot_bandwidth_curves(self, metrics, save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”Ÿæˆå¸¦å®½æ—¶é—´æ›²çº¿å›¾ï¼ˆä½¿ç”¨ç´¯ç§¯å¸¦å®½ç®—æ³•ï¼Œä»¿ç…§è€ç‰ˆæœ¬ï¼‰"""
        if not metrics:
            return ""

        try:
            # æŒ‰IPç±»å‹ç»„åˆåˆ†ç»„æ•°æ®ï¼Œåˆå¹¶ç›¸åŒç±»å‹çš„æ“ä½œ
            # ä¾‹å¦‚ï¼šæ‰€æœ‰èŠ‚ç‚¹çš„GDMAè¯»DDRæ“ä½œåˆå¹¶ä¸ºä¸€æ¡æ›²çº¿
            port_time_series = defaultdict(lambda: {"time": [], "start_times": [], "bytes": []})

            for metric in metrics:
                # æå–å¹¶æ¸…ç†IPç±»å‹åç§°ï¼Œå»æ‰ç¼–å·åç¼€
                def clean_ip_type(ip_type_str):
                    """æ¸…ç†IPç±»å‹åç§°ï¼Œå»æ‰ç¼–å·åç¼€ (å¦‚ GDMA_0 -> GDMA, DDR_3 -> DDR)"""
                    # ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†å‰²ï¼Œå–ç¬¬ä¸€éƒ¨åˆ†
                    return ip_type_str.split("_")[0].upper()

                clean_source = clean_ip_type(metric.source_type)
                clean_dest = clean_ip_type(metric.dest_type)

                # æ„é€ åˆå¹¶é”®ï¼šæ ¼å¼ä¸º "SOURCE_TYPE REQUEST_TYPE DEST_TYPE"ï¼Œä¾‹å¦‚ "GDMA READ DDR"
                # æ‰€æœ‰GDMA_xè¯»DDR_yçš„æ“ä½œéƒ½ä¼šåˆå¹¶åˆ°"GDMA READ DDR"
                port_key = f"{clean_source} {metric.req_type.upper()} {clean_dest}"

                port_time_series[port_key]["time"].append(metric.end_time)
                port_time_series[port_key]["start_times"].append(metric.start_time)
                port_time_series[port_key]["bytes"].append(metric.total_bytes)

            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(12, 8))

            # ç»˜åˆ¶ç´¯ç§¯å¸¦å®½æ›²çº¿
            total_final_bw = 0

            for port_key, data_dict in port_time_series.items():
                if not data_dict["time"]:
                    continue

                # æ’åºæ—¶é—´æˆ³å¹¶å»é™¤nanå€¼ï¼ˆä»¿ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰
                raw_end = np.array(data_dict["time"])
                raw_start = np.array(data_dict["start_times"])
                raw_bytes = np.array(data_dict["bytes"])

                # å»é™¤nanå€¼å’Œæ— æ•ˆæ•°æ®
                mask = ~np.isnan(raw_end) & (raw_end > 0)
                end_clean = raw_end[mask]
                start_clean = raw_start[mask]
                bytes_clean = raw_bytes[mask]

                if len(end_clean) == 0:
                    continue

                # åŒæ­¥æ’åº
                sort_idx = np.argsort(end_clean)
                times = end_clean[sort_idx]
                start_times = start_clean[sort_idx]
                bytes_data = bytes_clean[sort_idx]

                # ä»¿ç…§è€ç‰ˆæœ¬ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„å¼€å§‹æ—¶é—´ä½œä¸ºåŸºå‡†
                if len(start_times) > 0:
                    base_start = start_times[0]
                    rel_times = times - base_start

                    # é˜²æ­¢é™¤ä»¥0
                    rel_times[rel_times <= 0] = 1e-9

                    # è®¡ç®—ç´¯ç§¯è¯·æ±‚æ•°å’Œç´¯ç§¯å¸¦å®½
                    cum_counts = np.arange(1, len(rel_times) + 1)

                    # ä½¿ç”¨ç»Ÿä¸€å…¬å¼ï¼šç´¯ç§¯å­—èŠ‚æ•° / æ—¶é—´ = GB/sï¼ˆç›´æ¥ç»“æœï¼‰
                    cum_bytes = np.cumsum(bytes_data)
                    bandwidth_gbps = cum_bytes / rel_times  # ç›´æ¥å¾—åˆ°GB/s

                    # ç»˜åˆ¶æ›²çº¿ï¼ˆä½¿ç”¨ç»å¯¹æ—¶é—´è½´ï¼‰
                    time_us = times / 1000  # è½¬æ¢ä¸ºå¾®ç§’
                    (line,) = ax.plot(time_us, bandwidth_gbps, drawstyle="default", label=port_key, linewidth=2)

                    # åœ¨æ›²çº¿æœ«å°¾æ·»åŠ æ•°å€¼æ ‡æ³¨
                    if len(bandwidth_gbps) > 0:
                        final_bw = bandwidth_gbps[-1]
                        ax.text(
                            time_us[-1],
                            final_bw,
                            f"{final_bw:.2f}",
                            va="center",
                            color=line.get_color(),
                            fontsize=12,
                            bbox=dict(boxstyle="round,pad=0.2", facecolor="none", alpha=0),
                        )
                        total_final_bw += final_bw

            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_xlabel("æ—¶é—´ (Î¼s)", fontsize=12)
            ax.set_ylabel("å¸¦å®½ (GB/s)", fontsize=12)
            ax.set_title("å¸¦å®½æ›²çº¿å›¾", fontsize=14)
            ax.legend(fontsize=10, prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 10})
            ax.grid(True, alpha=0.3)
            ax.set_ylim(bottom=0)

            # æ·»åŠ æ€»å¸¦å®½ä¿¡æ¯
            # if total_final_bw > 0:
            #     ax.text(
            #         0.02,
            #         0.98,
            #         f"æ€»å¸¦å®½: {total_final_bw:.2f} GB/s",
            #         transform=ax.transAxes,
            #         fontsize=12,
            #         va="top",
            #         ha="left",
            #         bbox=dict(boxstyle="round,pad=0.3", facecolor="none", alpha=0),
            #     )

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures and save_dir:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_bandwidth_curve_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=100)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ ç´¯ç§¯å¸¦å®½æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºç´¯ç§¯å¸¦å®½æ›²çº¿å›¾")
                try:
                    plt.show()  # ä½¿ç”¨é»˜è®¤çš„block=Trueï¼Œä¿æŒçª—å£æ‰“å¼€
                except Exception as e:
                    if verbose:
                        print(f"   æ— æ³•æ˜¾ç¤ºå›¾è¡¨: {e}")
                        print(f"   å»ºè®®åœ¨æœ‰GUIçš„ç¯å¢ƒä¸­è¿è¡Œæˆ–è®¾ç½®save_figures=Trueä¿å­˜åˆ°æ–‡ä»¶")
                return ""

        except Exception as e:
            import traceback

            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return ""

    def save_detailed_requests_csv(self, metrics, save_dir: str = "output") -> Dict[str, str]:
        """ä¿å­˜è¯¦ç»†è¯·æ±‚CSVæ–‡ä»¶ï¼ˆä»¿ç…§è€ç‰ˆæœ¬æ ¼å¼ï¼‰

        Returns:
            åŒ…å«ä¿å­˜æ–‡ä»¶è·¯å¾„çš„å­—å…¸: {"read_requests_csv": path, "write_requests_csv": path}
        """
        if not metrics:
            return {}

        # å¦‚æœsave_dirä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜æ“ä½œ
        if not save_dir:
            return {}

        try:
            import csv

            os.makedirs(save_dir, exist_ok=True)

            # CSVæ–‡ä»¶å¤´ï¼ˆåŒ…å«æ‰€æœ‰cycleå­—æ®µï¼‰
            csv_header = [
                "packet_id",
                "start_time_ns",
                "end_time_ns",
                "source_node",
                "source_type",
                "dest_node",
                "dest_type",
                "burst_length",
                "cmd_latency_ns",
                "data_latency_ns",
                "transaction_latency_ns",
                # æ·»åŠ è¯¦ç»†çš„cycleæ—¶é—´æˆ³å­—æ®µ
                "cmd_entry_cake0_cycle",
                "cmd_entry_noc_from_cake0_cycle",
                "cmd_entry_noc_from_cake1_cycle",
                "cmd_received_by_cake0_cycle",
                "cmd_received_by_cake1_cycle",
                "data_entry_noc_from_cake0_cycle",
                "data_entry_noc_from_cake1_cycle",
                "data_received_complete_cycle",
            ]

            # åˆ†ç¦»è¯»å†™è¯·æ±‚
            read_requests = [req for req in metrics if req.req_type == "read"]
            write_requests = [req for req in metrics if req.req_type == "write"]

            saved_files = {}

            # ä¿å­˜è¯»è¯·æ±‚CSV
            if read_requests:
                timestamp = int(time.time())
                read_csv_path = f"{save_dir}/read_requests_{timestamp}.csv"

                with open(read_csv_path, "w", newline="", encoding="utf-8") as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(csv_header)

                    for req in read_requests:
                        row = [
                            req.packet_id,
                            req.start_time,
                            req.end_time,
                            req.source_node,
                            getattr(req, "source_type", "unknown"),
                            req.dest_node,
                            getattr(req, "dest_type", "unknown"),
                            req.burst_length,
                            req.cmd_latency,
                            req.data_latency,
                            req.transaction_latency,
                            # æ·»åŠ cycleæ—¶é—´æˆ³æ•°æ®
                            req.cmd_entry_cake0_cycle,
                            req.cmd_entry_noc_from_cake0_cycle,
                            req.cmd_entry_noc_from_cake1_cycle,
                            req.cmd_received_by_cake0_cycle,
                            req.cmd_received_by_cake1_cycle,
                            req.data_entry_noc_from_cake0_cycle,
                            req.data_entry_noc_from_cake1_cycle,
                            req.data_received_complete_cycle,
                        ]
                        writer.writerow(row)

                saved_files["read_requests_csv"] = read_csv_path

            # ä¿å­˜å†™è¯·æ±‚CSV
            if write_requests:
                timestamp = int(time.time())
                write_csv_path = f"{save_dir}/write_requests_{timestamp}.csv"

                with open(write_csv_path, "w", newline="", encoding="utf-8") as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(csv_header)

                    for req in write_requests:
                        row = [
                            req.packet_id,
                            req.start_time,
                            req.end_time,
                            req.source_node,
                            getattr(req, "source_type", "unknown"),
                            req.dest_node,
                            getattr(req, "dest_type", "unknown"),
                            req.burst_length,
                            req.cmd_latency,
                            req.data_latency,
                            req.transaction_latency,
                            # æ·»åŠ cycleæ—¶é—´æˆ³æ•°æ®
                            req.cmd_entry_cake0_cycle,
                            req.cmd_entry_noc_from_cake0_cycle,
                            req.cmd_entry_noc_from_cake1_cycle,
                            req.cmd_received_by_cake0_cycle,
                            req.cmd_received_by_cake1_cycle,
                            req.data_entry_noc_from_cake0_cycle,
                            req.data_entry_noc_from_cake1_cycle,
                            req.data_received_complete_cycle,
                        ]
                        writer.writerow(row)

                saved_files["write_requests_csv"] = write_csv_path

            return saved_files

        except Exception as e:
            import traceback

            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {}

    def save_ports_bandwidth_csv(self, metrics, save_dir: str = "output", config=None) -> str:
        """ä¿å­˜ç«¯å£å¸¦å®½CSVæ–‡ä»¶ï¼ˆä»¿ç…§è€ç‰ˆæœ¬æ ¼å¼ï¼‰

        Returns:
            ä¿å­˜çš„CSVæ–‡ä»¶è·¯å¾„
        """
        if not metrics:
            return ""

        # å¦‚æœsave_dirä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜æ“ä½œ
        if not save_dir:
            return ""

        try:
            import csv

            os.makedirs(save_dir, exist_ok=True)

            # CSVæ–‡ä»¶å¤´ï¼ˆä¸è€ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰
            csv_header = [
                "port_id",
                "coordinate",
                "read_unweighted_bandwidth_gbps",
                "read_weighted_bandwidth_gbps",
                "write_unweighted_bandwidth_gbps",
                "write_weighted_bandwidth_gbps",
                "mixed_unweighted_bandwidth_gbps",
                "mixed_weighted_bandwidth_gbps",
                "read_requests_count",
                "write_requests_count",
                "total_requests_count",
                "read_flits_count",
                "write_flits_count",
                "total_flits_count",
                "read_working_intervals_count",
                "write_working_intervals_count",
                "mixed_working_intervals_count",
                "read_total_working_time_ns",
                "write_total_working_time_ns",
                "mixed_total_working_time_ns",
                "read_network_start_time_ns",
                "read_network_end_time_ns",
                "write_network_start_time_ns",
                "write_network_end_time_ns",
                "mixed_network_start_time_ns",
                "mixed_network_end_time_ns",
            ]

            # æŒ‰ç«¯å£åˆ†ç»„ç»Ÿè®¡ - ä½¿ç”¨å…·ä½“IPåç§°å’Œåæ ‡
            port_stats = {}

            # ä»configè·å–ç½‘æ ¼å°ºå¯¸
            num_cols = getattr(config, "NUM_COL", 3) if config else 3  # é»˜è®¤3åˆ—

            for req in metrics:
                # ç»Ÿè®¡æ‰€æœ‰æ¶‰åŠçš„ç«¯å£ï¼šRNç«¯å£ï¼ˆè¯»è¯·æ±‚æºï¼‰å’ŒSNç«¯å£ï¼ˆå†™è¯·æ±‚ç›®æ ‡ï¼‰
                ports_to_update = []

                # å¯¹äºæ¯ä¸ªè¯·æ±‚ï¼Œéƒ½è¦ç»Ÿè®¡RNå’ŒSNä¸¤ä¸ªç«¯å£
                # RNç«¯å£ï¼šè¯·æ±‚å‘èµ·è€…ï¼ˆè¯»/å†™è¯·æ±‚çš„æºï¼‰
                source_port_id = req.source_type  # å¦‚ "gdma_0"
                source_node_id = req.source_node
                source_row = source_node_id // num_cols
                source_col = source_node_id % num_cols
                source_coordinate = f"x_{source_col}_y_{source_row}"
                ports_to_update.append((source_port_id, source_node_id, source_coordinate))

                # SNç«¯å£ï¼šè¯·æ±‚æ¥æ”¶è€…ï¼ˆè¯»/å†™è¯·æ±‚çš„ç›®æ ‡ï¼‰
                dest_port_id = req.dest_type  # å¦‚ "ddr_0"
                dest_node_id = req.dest_node
                dest_row = dest_node_id // num_cols
                dest_col = dest_node_id % num_cols
                dest_coordinate = f"x_{dest_col}_y_{dest_row}"
                ports_to_update.append((dest_port_id, dest_node_id, dest_coordinate))

                # æ›´æ–°æ‰€æœ‰ç›¸å…³ç«¯å£çš„ç»Ÿè®¡
                for port_id, node_id, coordinate in ports_to_update:
                    # ä½¿ç”¨èŠ‚ç‚¹IDå’Œç«¯å£IDçš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé¿å…ä¸åŒèŠ‚ç‚¹ç›¸åŒIPè¦†ç›–
                    unique_port_key = f"{port_id}_node_{node_id}"
                    if unique_port_key not in port_stats:
                        port_stats[unique_port_key] = {
                            "port_id": port_id,
                            "coordinate": coordinate,
                            "node_id": node_id,
                            "read_requests": [],
                            "write_requests": [],
                            "all_requests": [],
                        }

                    port_stats[unique_port_key]["all_requests"].append(req)
                    if req.req_type == "read":
                        port_stats[unique_port_key]["read_requests"].append(req)
                    else:
                        port_stats[unique_port_key]["write_requests"].append(req)

            # ç”ŸæˆCSVæ–‡ä»¶
            timestamp = int(time.time())
            csv_path = f"{save_dir}/ports_bandwidth_{timestamp}.csv"

            with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(csv_header)

                for unique_port_key, stats in port_stats.items():
                    # è®¡ç®—å„ç§å¸¦å®½æŒ‡æ ‡
                    read_reqs = stats["read_requests"]
                    write_reqs = stats["write_requests"]
                    all_reqs = stats["all_requests"]
                    port_id = stats["port_id"]  # ä½¿ç”¨åŸå§‹ç«¯å£ID

                    # å¸¦å®½è®¡ç®— - ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—ï¼Œä¸calculate_bandwidth_metricså®Œå…¨ä¸€è‡´
                    def calc_bandwidth_metrics(requests):
                        if not requests:
                            return {"unweighted_bw": 0.0, "weighted_bw": 0.0, "start_time": 0, "end_time": 0, "total_time": 0, "working_intervals": 0, "flits_count": 0}

                        # è®¡ç®—å·¥ä½œåŒºé—´ï¼ˆä¸calculate_bandwidth_metricsç›¸åŒçš„é€»è¾‘ï¼‰
                        working_intervals = self.calculate_working_intervals(requests, min_gap_threshold=200)

                        # ç½‘ç»œå·¥ä½œæ—¶é—´çª—å£
                        network_start = min(r.start_time for r in requests)
                        network_end = max(r.end_time for r in requests)
                        total_network_time = network_end - network_start

                        # æ€»å·¥ä½œæ—¶é—´å’Œæ€»å­—èŠ‚æ•°
                        total_working_time = sum(interval.duration for interval in working_intervals)
                        total_bytes = sum(r.total_bytes for r in requests)

                        # è®¡ç®—éåŠ æƒå¸¦å®½ï¼šæ€»æ•°æ®é‡ / ç½‘ç»œæ€»æ—¶é—´
                        unweighted_bandwidth = (total_bytes / total_network_time) if total_network_time > 0 else 0.0

                        # è®¡ç®—åŠ æƒå¸¦å®½ï¼šå„åŒºé—´å¸¦å®½æŒ‰flitæ•°é‡åŠ æƒå¹³å‡
                        if working_intervals:
                            total_weighted_bandwidth = 0.0
                            total_weight = 0

                            for interval in working_intervals:
                                weight = interval.flit_count  # æƒé‡æ˜¯å·¥ä½œæ—¶é—´æ®µçš„flitæ•°é‡
                                bandwidth = interval.bandwidth  # GB/s
                                total_weighted_bandwidth += bandwidth * weight
                                total_weight += weight

                            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                        else:
                            weighted_bandwidth = 0.0

                        return {
                            "unweighted_bw": unweighted_bandwidth,
                            "weighted_bw": weighted_bandwidth,
                            "start_time": network_start,
                            "end_time": network_end,
                            "total_time": total_network_time,
                            "working_intervals": len(working_intervals),
                            "flits_count": sum(r.burst_length for r in requests),
                        }

                    read_metrics = calc_bandwidth_metrics(read_reqs)
                    write_metrics = calc_bandwidth_metrics(write_reqs)
                    mixed_metrics = calc_bandwidth_metrics(all_reqs)

                    # ç”Ÿæˆæ ¼å¼åŒ–çš„ç«¯å£IDï¼šnode_X_porttype_Y
                    formatted_port_id = f"node_{stats['node_id']}_{port_id}"

                    row_data = [
                        formatted_port_id,
                        stats["coordinate"],
                        read_metrics["unweighted_bw"],
                        read_metrics["weighted_bw"],
                        write_metrics["unweighted_bw"],
                        write_metrics["weighted_bw"],
                        mixed_metrics["unweighted_bw"],
                        mixed_metrics["weighted_bw"],
                        len(read_reqs),
                        len(write_reqs),
                        len(all_reqs),
                        read_metrics["flits_count"],
                        write_metrics["flits_count"],
                        mixed_metrics["flits_count"],
                        read_metrics["working_intervals"],
                        write_metrics["working_intervals"],
                        mixed_metrics["working_intervals"],
                        read_metrics["total_time"],
                        write_metrics["total_time"],
                        mixed_metrics["total_time"],
                        read_metrics["start_time"],
                        read_metrics["end_time"],
                        write_metrics["start_time"],
                        write_metrics["end_time"],
                        mixed_metrics["start_time"],
                        mixed_metrics["end_time"],
                    ]
                    writer.writerow(row_data)

                # è®¡ç®—IPç±»å‹æ±‡æ€»ç»Ÿè®¡
                ip_type_aggregates = {}
                for unique_port_key, stats in port_stats.items():
                    # æå–IPç±»å‹ï¼ˆå»æ‰æ•°å­—åç¼€ï¼‰
                    port_id = stats["port_id"]
                    ip_type = port_id.split("_")[0]  # "gdma_0" -> "gdma"

                    if ip_type not in ip_type_aggregates:
                        ip_type_aggregates[ip_type] = {
                            "ports": [],
                            "total_read_requests": 0,
                            "total_write_requests": 0,
                            "total_requests": 0,
                            "total_read_flits": 0,
                            "total_write_flits": 0,
                            "total_flits": 0,
                            "read_bandwidth_sum": 0,
                            "write_bandwidth_sum": 0,
                            "mixed_bandwidth_sum": 0,
                        }

                    # è®¡ç®—è¯¥ç«¯å£çš„æŒ‡æ ‡
                    read_reqs = stats["read_requests"]
                    write_reqs = stats["write_requests"]
                    all_reqs = stats["all_requests"]

                    read_metrics = calc_bandwidth_metrics(read_reqs)
                    write_metrics = calc_bandwidth_metrics(write_reqs)
                    mixed_metrics = calc_bandwidth_metrics(all_reqs)

                    # ç´¯åŠ åˆ°IPç±»å‹ç»Ÿè®¡
                    agg = ip_type_aggregates[ip_type]
                    agg["ports"].append(port_id)
                    agg["total_read_requests"] += len(read_reqs)
                    agg["total_write_requests"] += len(write_reqs)
                    agg["total_requests"] += len(all_reqs)
                    agg["total_read_flits"] += read_metrics["flits_count"]
                    agg["total_write_flits"] += write_metrics["flits_count"]
                    agg["total_flits"] += mixed_metrics["flits_count"]
                    agg["read_bandwidth_sum"] += read_metrics["unweighted_bw"]
                    agg["write_bandwidth_sum"] += write_metrics["unweighted_bw"]
                    agg["mixed_bandwidth_sum"] += mixed_metrics["unweighted_bw"]

                # æ·»åŠ IPç±»å‹æ±‡æ€»è¡Œ
                writer.writerow([])  # ç©ºè¡Œåˆ†éš”
                writer.writerow(["=== IPç±»å‹æ±‡æ€»ç»Ÿè®¡ ==="])

                for ip_type, agg in sorted(ip_type_aggregates.items()):
                    port_count = len(agg["ports"])
                    avg_read_bw = agg["read_bandwidth_sum"] / port_count if port_count > 0 else 0
                    avg_write_bw = agg["write_bandwidth_sum"] / port_count if port_count > 0 else 0
                    avg_mixed_bw = agg["mixed_bandwidth_sum"] / port_count if port_count > 0 else 0

                    summary_row = [
                        f"{ip_type}_AVG",  # port_idæ ¼å¼ï¼šgdma_AVG, ddr_AVG
                        f"avg_of_{port_count}_ports",  # coordinateæ˜¾ç¤ºç«¯å£æ•°
                        avg_read_bw,  # å¹³å‡è¯»å¸¦å®½
                        avg_read_bw,  # å¹³å‡è¯»å¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        avg_write_bw,  # å¹³å‡å†™å¸¦å®½
                        avg_write_bw,  # å¹³å‡å†™å¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        avg_mixed_bw,  # å¹³å‡æ··åˆå¸¦å®½
                        avg_mixed_bw,  # å¹³å‡æ··åˆå¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        agg["total_read_requests"],  # æ€»è¯»è¯·æ±‚æ•°
                        agg["total_write_requests"],  # æ€»å†™è¯·æ±‚æ•°
                        agg["total_requests"],  # æ€»è¯·æ±‚æ•°
                        agg["total_read_flits"],  # æ€»è¯»flitæ•°
                        agg["total_write_flits"],  # æ€»å†™flitæ•°
                        agg["total_flits"],  # æ€»flitæ•°
                        port_count,  # å·¥ä½œåŒºé—´æ•°ç”¨ç«¯å£æ•°è¡¨ç¤º
                        port_count,
                        port_count,
                        0,
                        0,
                        0,  # æ—¶é—´ç›¸å…³å­—æ®µä¸º0ï¼ˆæ±‡æ€»æ•°æ®ï¼‰
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                    ]
                    writer.writerow(summary_row)

            return csv_path

        except Exception as e:
            import traceback

            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return ""

    def plot_latency_distribution(self, metrics, save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”Ÿæˆå»¶è¿Ÿåˆ†å¸ƒå›¾"""
        if not metrics:
            return ""

        try:
            # ä¸‰ç§å»¶è¿Ÿç±»å‹æ•°æ®
            cmd_latencies = [m.cmd_latency for m in metrics]
            data_latencies = [m.data_latency for m in metrics]
            transaction_latencies = [m.transaction_latency for m in metrics]

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # 1. ä¸‰ç§å»¶è¿Ÿç±»å‹å¯¹æ¯”ç›´æ–¹å›¾ï¼ˆä½¿ç”¨å¯¹æ•°åæ ‡è½´ï¼‰
            # è®¡ç®—ç»Ÿä¸€çš„binè¾¹ç•Œï¼Œç¡®ä¿æŸ±çŠ¶å›¾å®½åº¦ä¸€è‡´
            all_latencies = cmd_latencies + data_latencies + transaction_latencies
            if all_latencies:
                # ä½¿ç”¨å¯¹æ•°ç©ºé—´åˆ›å»ºç»Ÿä¸€çš„binè¾¹ç•Œ
                min_val = min([x for x in all_latencies if x > 0])  # æ’é™¤0å€¼
                max_val = max(all_latencies)
                bins = np.logspace(np.log10(min_val), np.log10(max_val), 31)  # 30ä¸ªåŒºé—´
            else:
                bins = 30

            # ä½¿ç”¨ç»Ÿä¸€çš„binè¾¹ç•Œå’Œæ ·å¼
            ax1.hist(cmd_latencies, bins=bins, alpha=0.6, label="CMD", color="blue", linewidth=1.5, rwidth=0.8)
            ax1.hist(data_latencies, bins=bins, alpha=0.6, label="DATA", color="green", linewidth=1.5, rwidth=0.8)
            ax1.hist(transaction_latencies, bins=bins, alpha=0.6, label="TRANSACTION", color="red", linewidth=1.5, rwidth=0.8)
            ax1.set_xlabel("å»¶è¿Ÿ (ns)")
            ax1.set_ylabel("é¢‘æ¬¡")
            ax1.set_title("å»¶è¿Ÿåˆ†å¸ƒç›´æ–¹å›¾")
            ax1.legend(prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 9})
            ax1.grid(True, alpha=0.3)
            # è®¾ç½®Xè½´ä¸ºå¯¹æ•°åæ ‡ï¼Œå¹¶è°ƒæ•´åˆ»åº¦
            ax1.set_xscale("log")
            # è®¾ç½®æ›´å¯†é›†çš„ä¸»è¦åˆ»åº¦å’Œæ¬¡è¦åˆ»åº¦
            from matplotlib.ticker import LogLocator, LogFormatter

            ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=8))
            ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=np.arange(2, 10) * 0.1, numticks=100))
            ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))

            # 2. å»¶è¿Ÿç±»å‹ç®±çº¿å›¾ï¼ˆä½¿ç”¨å¯¹æ•°åæ ‡è½´ï¼‰
            latency_data = [cmd_latencies, data_latencies, transaction_latencies]
            latency_labels = ["CMDå»¶è¿Ÿ", "DATAå»¶è¿Ÿ", "TRANSACTIONå»¶è¿Ÿ"]

            # åˆ›å»ºç®±çº¿å›¾ï¼Œéšè—å¼‚å¸¸å€¼
            box_plot = ax2.boxplot(latency_data, labels=latency_labels, patch_artist=True, showfliers=False)  # å…è®¸å¡«å……é¢œè‰²  # éšè—å¼‚å¸¸å€¼

            # ä¸ºæ¯ä¸ªç®±å­è®¾ç½®ä¸åŒé¢œè‰²
            colors = ["lightblue", "lightgreen", "lightcoral"]
            for patch, color in zip(box_plot["boxes"], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)

            # ä¸ºæ¯ä¸ªç®±çº¿å›¾æ·»åŠ å¯¹åº”çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¾åœ¨å„è‡ªå³è¾¹ï¼‰
            for i, (data, label) in enumerate(zip(latency_data, latency_labels)):
                if len(data) > 0:
                    mean_val = np.mean(data)
                    median_val = np.median(data)
                    std_val = np.std(data)
                    max_val = np.max(data)

                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ä½ç½®ï¼ˆåœ¨æ¯ä¸ªç®±å­å³ä¾§ï¼‰
                    x_pos = i + 1.3  # ç®±å­ä½ç½®æ˜¯1,2,3ï¼Œå³ä¾§åç§»0.3

                    # ä½¿ç”¨ç®±å­çš„ä¸­ä½æ•°é«˜åº¦ä½œä¸ºæ–‡æœ¬å‚ç›´ä½ç½®
                    median_y = np.median(data)

                    stats_text = f"å‡å€¼: {mean_val:.1f}ns\nä¸­ä½æ•°: {median_val:.1f}ns\næœ€å¤§å€¼: {max_val:.1f}ns\næ ‡å‡†å·®: {std_val:.1f}ns"
                    ax2.text(x_pos, median_y, stats_text, ha="left", va="center", fontsize=8, bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8, edgecolor="gray"))

            ax2.set_ylabel("å»¶è¿Ÿ (ns)")
            ax2.set_title("å»¶è¿Ÿç®±çº¿å›¾")
            ax2.grid(True, alpha=0.3)
            # è®¾ç½®Yè½´ä¸ºå¯¹æ•°åæ ‡ï¼Œå¹¶è°ƒæ•´åˆ»åº¦
            ax2.set_yscale("log")
            # è®¾ç½®æ›´å¯†é›†çš„ä¸»è¦åˆ»åº¦å’Œæ¬¡è¦åˆ»åº¦
            ax2.yaxis.set_major_locator(LogLocator(base=10, numticks=8))
            ax2.yaxis.set_minor_locator(LogLocator(base=10, subs=np.arange(2, 10) * 0.1, numticks=100))
            ax2.yaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))

            # åˆ é™¤è¯»å†™åˆ†ç±»çš„å›¾è¡¨ï¼Œåªä¿ç•™ä¸¤ä¸ªæ€»ä½“ç»Ÿè®¡å›¾

            plt.tight_layout()

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures and save_dir:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_latency_distribution_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=300)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ å»¶è¿Ÿåˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºå»¶è¿Ÿåˆ†å¸ƒå›¾")
                plt.show()
                return ""

        except Exception as e:
            print(f"ç”Ÿæˆå»¶è¿Ÿåˆ†å¸ƒå›¾å¤±è´¥: {e}")
            return ""

    def plot_port_bandwidth_comparison(self, ip_analysis: Dict[str, Any], save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”ŸæˆIPç±»å‹å¸¦å®½å¯¹æ¯”å›¾"""
        if not ip_analysis:
            return ""

        try:
            ip_types = list(ip_analysis.keys())

            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))

            x = np.arange(len(ip_types))
            width = 0.35

            # æå–è¯»å†™å¸¦å®½æ•°æ®
            read_bw = [float(ip_analysis[ip_type]["è¯»å¸¦å®½_GB/s"]) for ip_type in ip_types]
            write_bw = [float(ip_analysis[ip_type]["å†™å¸¦å®½_GB/s"]) for ip_type in ip_types]

            bars1 = ax.bar(x - width / 2, read_bw, width, label="è¯»å¸¦å®½", color="green", alpha=0.7)
            bars2 = ax.bar(x + width / 2, write_bw, width, label="å†™å¸¦å®½", color="red", alpha=0.7)

            ax.set_xlabel("IPç±»å‹", fontsize=12)
            ax.set_ylabel("å¸¦å®½ (GB/s)", fontsize=12)
            ax.set_title("å„IPç±»å‹å¸¦å®½å¯¹æ¯”", fontsize=14)
            ax.set_xticks(x)
            ax.set_xticklabels(ip_types)
            ax.legend(prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 9})
            ax.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars1:
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f"{height:.2f}", ha="center", va="bottom", fontsize=10)

            for bar in bars2:
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f"{height:.2f}", ha="center", va="bottom", fontsize=10)

            # æ·»åŠ è¯·æ±‚æ•°é‡ä¿¡æ¯
            for i, ip_type in enumerate(ip_types):
                total_requests = ip_analysis[ip_type]["æ€»è¯·æ±‚æ•°"]
                read_requests = ip_analysis[ip_type]["è¯»è¯·æ±‚æ•°"]
                write_requests = ip_analysis[ip_type]["å†™è¯·æ±‚æ•°"]

                # åœ¨Xè½´æ ‡ç­¾ä¸‹æ–¹æ·»åŠ è¯·æ±‚æ•°ä¿¡æ¯
                ax.text(
                    i, -max(max(read_bw), max(write_bw)) * 0.1, f"æ€»è¯·æ±‚: {total_requests}\n(è¯»:{read_requests}, å†™:{write_requests})", ha="center", va="top", fontsize=8, alpha=0.7
                )

            plt.tight_layout()

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures and save_dir:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_ip_bandwidth_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=150)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ IPå¸¦å®½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºIPå¸¦å®½å¯¹æ¯”å›¾")
                plt.show()
                return ""

        except Exception as e:
            print(f"ç”ŸæˆIPå¸¦å®½å¯¹æ¯”å›¾å¤±è´¥: {e}")
            return ""

    def save_results(self, analysis: Dict[str, Any], save_dir: str = "output") -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        # å¦‚æœsave_dirä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜æ“ä½œ
        if not save_dir:
            return ""

        try:
            timestamp = int(time.time())
            results_file = f"{save_dir}/crossring_analysis_{timestamp}.json"

            os.makedirs(save_dir, exist_ok=True)
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)

            return results_file
        except Exception as e:
            print(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return ""

    def plot_traffic_distribution(self, model, metrics, save_dir: str = "output", mode: str = "total", save_figures: bool = True, verbose: bool = True) -> str:
        """
        ç»˜åˆ¶æµé‡åˆ†å¸ƒå›¾ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹IPå¸¦å®½å’Œé“¾è·¯å¸¦å®½

        Args:
            model: CrossRingModelå®ä¾‹
            metrics: è¯·æ±‚åº¦é‡æ•°æ®
            save_dir: ä¿å­˜ç›®å½•
            mode: æ˜¾ç¤ºæ¨¡å¼ ("read", "write", "total")

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not metrics:
            return ""

        try:
            # è·å–èŠ‚ç‚¹æ•°é‡å’Œæ‹“æ‰‘ä¿¡æ¯
            num_nodes = len(model.nodes)
            num_rows = model.config.NUM_ROW
            num_cols = model.config.NUM_COL

            # æ”¶é›†èŠ‚ç‚¹çº§IPå¸¦å®½æ•°æ®å’Œé“¾è·¯å¸¦å®½æ•°æ®
            # æ”¯æŒçš„IPç±»å‹ï¼šGDMA, SDMA, CDMA, DDR, L2Mç­‰ï¼Œç”¨å°å†™å­˜å‚¨
            node_ip_bandwidth = defaultdict(lambda: defaultdict(float))
            link_bandwidth = defaultdict(float)

            # é¦–å…ˆè®¡ç®—æ•´ä½“æ—¶é—´çª—å£
            if not metrics:
                return ""

            overall_start_time = min(metric.start_time for metric in metrics)
            overall_end_time = max(metric.end_time for metric in metrics)
            overall_time_window = overall_end_time - overall_start_time if overall_end_time > overall_start_time else 1

            # æŒ‰IPç±»å‹åˆ†ç»„æ”¶é›†å­—èŠ‚æ•°
            ip_type_bytes = defaultdict(int)
            node_ip_bytes = defaultdict(lambda: defaultdict(int))
            link_bytes = defaultdict(int)

            # åˆ†ææ¯ä¸ªè¯·æ±‚çš„å­—èŠ‚æ•°è´¡çŒ®
            for metric in metrics:
                # æå–IPç±»å‹ï¼šgdma_0 -> gdma, ddr_0 -> ddr
                source_ip_type = metric.source_type.split("_")[0].lower() if "_" in metric.source_type else metric.source_type.lower()
                dest_ip_type = metric.dest_type.split("_")[0].lower() if "_" in metric.dest_type else metric.dest_type.lower()

                # ç´¯è®¡å­—èŠ‚æ•°ï¼ˆæŒ‰èŠ‚ç‚¹å’ŒIPç±»å‹èšåˆï¼‰
                # æºèŠ‚ç‚¹ï¼šå‘é€å­—èŠ‚æ•°
                node_ip_bytes[metric.source_node][source_ip_type] += metric.total_bytes
                # ç›®æ ‡èŠ‚ç‚¹ï¼šæ¥æ”¶å­—èŠ‚æ•°
                node_ip_bytes[metric.dest_node][dest_ip_type] += metric.total_bytes

                # è®¡ç®—é“¾è·¯å­—èŠ‚æ•°ï¼ˆåªå¤„ç†è·¨èŠ‚ç‚¹é€šä¿¡ï¼‰
                if metric.source_node != metric.dest_node:
                    src_row = metric.source_node // num_cols
                    src_col = metric.source_node % num_cols
                    dst_row = metric.dest_node // num_cols
                    dst_col = metric.dest_node % num_cols

                    # æ°´å¹³è·¯ç”±
                    if src_row == dst_row:
                        step = 1 if dst_col > src_col else -1
                        for col in range(src_col, dst_col, step):
                            curr_node = src_row * num_cols + col
                            next_node = src_row * num_cols + col + step
                            if mode == "total" or mode == metric.req_type:
                                link_bytes[(curr_node, next_node)] += metric.total_bytes

                    # å‚ç›´è·¯ç”±
                    elif src_col == dst_col:
                        step = 1 if dst_row > src_row else -1
                        for row in range(src_row, dst_row, step):
                            curr_node = row * num_cols + src_col
                            next_node = (row + step) * num_cols + src_col
                            if mode == "total" or mode == metric.req_type:
                                link_bytes[(curr_node, next_node)] += metric.total_bytes

            # è®¡ç®—æœ€ç»ˆå¸¦å®½ï¼šä½¿ç”¨å·¥ä½œåŒºé—´æ–¹æ³•è®¡ç®—åŠ æƒå¸¦å®½
            # æŒ‰èŠ‚ç‚¹å’ŒIPç±»å‹åˆ†ç»„è¯·æ±‚ï¼Œè®¡ç®—å„è‡ªçš„å·¥ä½œåŒºé—´å¸¦å®½
            for node_id, ip_data in node_ip_bytes.items():
                for ip_type, total_bytes in ip_data.items():
                    # æ‰¾åˆ°è¯¥èŠ‚ç‚¹è¯¥IPç±»å‹çš„æ‰€æœ‰è¯·æ±‚ï¼ˆåŒ…æ‹¬æ‰€æœ‰å®ä¾‹ï¼‰
                    node_ip_requests = []
                    for metric in metrics:
                        # æå–è¯·æ±‚çš„IPç±»å‹
                        source_type = metric.source_type.split("_")[0].lower() if "_" in metric.source_type else metric.source_type.lower()
                        dest_type = metric.dest_type.split("_")[0].lower() if "_" in metric.dest_type else metric.dest_type.lower()

                        if (metric.source_node == node_id and source_type == ip_type) or (metric.dest_node == node_id and dest_type == ip_type):
                            node_ip_requests.append(metric)

                    if node_ip_requests:
                        # ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—è¯¥èŠ‚ç‚¹è¯¥IPçš„åŠ æƒå¸¦å®½
                        working_intervals = self.calculate_working_intervals(node_ip_requests, min_gap_threshold=200)

                        # è®¡ç®—åŠ æƒå¸¦å®½
                        if working_intervals:
                            total_weighted_bandwidth = 0.0
                            total_weight = 0

                            for interval in working_intervals:
                                weight = interval.flit_count
                                bandwidth = interval.bandwidth  # GB/s
                                total_weighted_bandwidth += bandwidth * weight
                                total_weight += weight

                            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                            bandwidth_gbps = weighted_bandwidth  # ç›´æ¥ä½¿ç”¨ï¼Œå·²ç»æ˜¯GB/s
                        else:
                            bandwidth_gbps = 0.0
                    else:
                        bandwidth_gbps = 0.0

                    node_ip_bandwidth[node_id][ip_type] = bandwidth_gbps

            # è®¡ç®—é“¾è·¯å¸¦å®½ï¼šä½¿ç”¨æ¨¡å‹ä¸­é“¾è·¯å¯¹è±¡çš„å®é™…ç»Ÿè®¡æ•°æ®
            link_bandwidth = {}
            self_loop_bandwidth = {}  # ä¸“é—¨å­˜å‚¨è‡ªç¯é“¾è·¯å¸¦å®½

            # è·å–æ¨¡å‹ä¸­çš„é“¾è·¯ç»Ÿè®¡æ•°æ®
            if hasattr(model, "links") and model.links:
                for link_id, link in model.links.items():
                    # è·å–é“¾è·¯çš„æ€§èƒ½æŒ‡æ ‡
                    try:
                        performance_metrics = link.get_link_performance_metrics()

                        # è®¡ç®—è¯¥é“¾è·¯çš„æ€»å¸¦å®½ï¼ˆæ‰€æœ‰é€šé“çš„å¸¦å®½ä¹‹å’Œï¼‰
                        total_bandwidth = 0.0
                        for channel in ["req", "rsp", "data"]:
                            if channel in performance_metrics:
                                total_bandwidth += performance_metrics[channel].get("bandwidth_gbps", 0.0)

                        # è§£æé“¾è·¯IDè·å–æºå’Œç›®æ ‡èŠ‚ç‚¹
                        # å¤„ç†ä¸åŒçš„é“¾è·¯IDæ ¼å¼
                        if link_id.startswith("link_"):
                            parts = link_id.split("_")
                            if len(parts) >= 4:
                                try:
                                    if len(parts) == 4:  # æ™®é€šé“¾è·¯ï¼šlink_0_TR_1
                                        source_node = int(parts[1])
                                        dest_node = int(parts[3])
                                        link_key = (source_node, dest_node)
                                        link_bandwidth[link_key] = total_bandwidth
                                    elif len(parts) == 5:  # è‡ªç¯é“¾è·¯ï¼šlink_0_TU_TD_0 æˆ– link_0_TL_TR_0
                                        source_node = int(parts[1])
                                        dest_node = int(parts[4])
                                        if source_node == dest_node and total_bandwidth > 0:
                                            # å­˜å‚¨è‡ªç¯é“¾è·¯å¸¦å®½ï¼ŒæŒ‰æ–¹å‘åˆ†ç±»
                                            direction1 = parts[2]  # TU, TD, TL, TR
                                            direction2 = parts[3]  # TD, TU, TR, TL
                                            if source_node not in self_loop_bandwidth:
                                                self_loop_bandwidth[source_node] = {}
                                            # ä½¿ç”¨æ–¹å‘ç»„åˆä½œä¸ºkey
                                            direction_key = f"{direction1}_{direction2}"
                                            self_loop_bandwidth[source_node][direction_key] = total_bandwidth
                                except (ValueError, IndexError):
                                    # è§£æå¤±è´¥æ—¶è·³è¿‡è¯¥é“¾è·¯
                                    if verbose:
                                        print(f"è­¦å‘Šï¼šæ— æ³•è§£æé“¾è·¯ID {link_id}")
                                    continue

                    except Exception as e:
                        if verbose:
                            print(f"è­¦å‘Šï¼šè·å–é“¾è·¯ {link_id} ç»Ÿè®¡æ•°æ®æ—¶å‡ºé”™: {e}")
                        continue

            # å¦‚æœæ²¡æœ‰ä»æ¨¡å‹è·å¾—é“¾è·¯æ•°æ®ï¼Œå›é€€åˆ°åŸºäºè¯·æ±‚çš„è®¡ç®—
            if not link_bandwidth:
                if verbose:
                    print("è­¦å‘Šï¼šæœªèƒ½ä»æ¨¡å‹è·å–é“¾è·¯ç»Ÿè®¡æ•°æ®ï¼Œä½¿ç”¨è¯·æ±‚æ•°æ®è®¡ç®—")

                for link_key, total_bytes in link_bytes.items():
                    # æ‰¾åˆ°é€šè¿‡è¯¥é“¾è·¯çš„æ‰€æœ‰è¯·æ±‚
                    link_requests = []
                    curr_node, next_node = link_key

                    for metric in metrics:
                        if metric.source_node != metric.dest_node:
                            # æ£€æŸ¥è¯¥è¯·æ±‚æ˜¯å¦é€šè¿‡è¿™æ¡é“¾è·¯
                            src_row = metric.source_node // num_cols
                            src_col = metric.source_node % num_cols
                            dst_row = metric.dest_node // num_cols
                            dst_col = metric.dest_node % num_cols

                            passes_through_link = False

                            # æ°´å¹³è·¯ç”±æ£€æŸ¥
                            if src_row == dst_row:
                                step = 1 if dst_col > src_col else -1
                                for col in range(src_col, dst_col, step):
                                    check_curr = src_row * num_cols + col
                                    check_next = src_row * num_cols + col + step
                                    if (check_curr, check_next) == link_key:
                                        passes_through_link = True
                                        break

                            # å‚ç›´è·¯ç”±æ£€æŸ¥
                            elif src_col == dst_col:
                                step = 1 if dst_row > src_row else -1
                                for row in range(src_row, dst_row, step):
                                    check_curr = row * num_cols + src_col
                                    check_next = (row + step) * num_cols + src_col
                                    if (check_curr, check_next) == link_key:
                                        passes_through_link = True
                                        break

                            if passes_through_link:
                                link_requests.append(metric)

                    if link_requests:
                        # ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—é“¾è·¯åŠ æƒå¸¦å®½
                        working_intervals = self.calculate_working_intervals(link_requests, min_gap_threshold=200)

                        if working_intervals:
                            total_weighted_bandwidth = 0.0
                            total_weight = 0

                            for interval in working_intervals:
                                weight = interval.flit_count
                                bandwidth = interval.bandwidth  # GB/s
                                total_weighted_bandwidth += bandwidth * weight
                                total_weight += weight

                            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                            bandwidth_gbps = weighted_bandwidth  # ç›´æ¥ä½¿ç”¨ï¼Œå·²ç»æ˜¯GB/s
                        else:
                            bandwidth_gbps = 0.0
                    else:
                        bandwidth_gbps = 0.0

                    link_bandwidth[link_key] = bandwidth_gbps

            # è®¡ç®—èŠ‚ç‚¹ä½ç½®ï¼ˆç½‘æ ¼å¯¹é½ï¼Œä¸äº¤é”™ï¼‰
            pos = {}
            for node_id in range(num_nodes):
                row = node_id // num_cols
                col = node_id % num_cols
                pos[node_id] = (col * 3, -row * 2)  # è§„æ•´ç½‘æ ¼ï¼Œä¸åç§»

            # æ ¹æ®ç½‘ç»œè§„æ¨¡è°ƒæ•´å›¾å½¢å¤§å°ï¼Œç¡®ä¿ä¸è¶…å‡ºå±å¹•
            if num_nodes <= 9:  # 3x3åŠä»¥ä¸‹
                figsize = (10, 8)
            elif num_nodes <= 25:  # 5x5
                figsize = (12, 9)
            elif num_nodes <= 64:  # 8x8
                figsize = (14, 10)
            else:  # æ›´å¤§çš„ç½‘ç»œ
                figsize = (16, 12)

            # åˆ›å»ºå›¾å½¢
            fig, ax = plt.subplots(figsize=figsize)
            ax.set_aspect("equal")

            # æ ¹æ®ç½‘ç»œè§„æ¨¡åŠ¨æ€è®¡ç®—æ‰€æœ‰å°ºå¯¸å‚æ•°
            # å­—ä½“å¤§å°ï¼šéšèŠ‚ç‚¹æ•°é‡åŠ¨æ€ç¼©æ”¾ï¼Œå°ç½‘ç»œç”¨å¤§å­—ä½“ï¼Œå¤§ç½‘ç»œç”¨å°å­—ä½“
            base_font = 12
            if num_nodes <= 9:  # 3x3åŠä»¥ä¸‹
                dynamic_font = 14
                arrow_scale = 15  # å‡å°ç®­å¤´å¤§å°
                node_size_factor = 1.2
                link_label_font_factor = 0.9
                node_label_font_factor = 1.0
            elif num_nodes <= 25:  # 5x5
                dynamic_font = 11
                arrow_scale = 12  # å‡å°ç®­å¤´å¤§å°
                node_size_factor = 1.0
                link_label_font_factor = 0.8
                node_label_font_factor = 0.9
            elif num_nodes <= 64:  # 8x8
                dynamic_font = 9
                arrow_scale = 10  # å‡å°ç®­å¤´å¤§å°
                node_size_factor = 0.8
                link_label_font_factor = 0.7
                node_label_font_factor = 0.8
            else:  # æ›´å¤§çš„ç½‘ç»œ
                dynamic_font = 7
                arrow_scale = 8  # å‡å°ç®­å¤´å¤§å°
                node_size_factor = 0.6
                link_label_font_factor = 0.6
                node_label_font_factor = 0.7

            # èŠ‚ç‚¹å¤§å°ï¼šæ ¹æ®ç½‘ç»œè§„æ¨¡è°ƒæ•´
            base_node_size = 4000
            node_size = base_node_size * node_size_factor
            square_size = np.sqrt(node_size) / 60

            # æ·»åŠ æ‰€æœ‰å¯èƒ½çš„é“¾è·¯ï¼ˆåŒ…æ‹¬æ²¡æœ‰æµé‡çš„ï¼‰
            all_links = set()
            for node_id in range(num_nodes):
                row = node_id // num_cols
                col = node_id % num_cols

                # æ°´å¹³è¿æ¥
                if col < num_cols - 1:
                    all_links.add((node_id, node_id + 1))
                    all_links.add((node_id + 1, node_id))

                # å‚ç›´è¿æ¥
                if row < num_rows - 1:
                    all_links.add((node_id, node_id + num_cols))
                    all_links.add((node_id + num_cols, node_id))

            # ç»˜åˆ¶æ‰€æœ‰é“¾è·¯ï¼ˆåŒ…æ‹¬æ— æµé‡çš„ï¼‰ï¼Œä½¿ç”¨åŒå‘ç®­å¤´æ˜¾ç¤º
            max_link_bw = max(link_bandwidth.values()) if link_bandwidth else 1.0

            # ä¸ºäº†é¿å…åŒå‘ç®­å¤´é‡å ï¼Œéœ€è¦ä¸ºæ¯ä¸ªæ–¹å‘è®¡ç®—åç§»
            for src, dst in all_links:
                x1, y1 = pos[src]
                x2, y2 = pos[dst]

                bandwidth = link_bandwidth.get((src, dst), 0.0)

                # è®¡ç®—é“¾è·¯é¢œè‰²å’Œçº¿å®½
                if bandwidth > 0:
                    intensity = min(1.0, bandwidth / max_link_bw)
                    color = (intensity, 0, 0)
                    linewidth = 1 + intensity * 2  # å‡å°çº¿å®½
                    alpha = 0.9
                else:
                    color = (0.7, 0.7, 0.7)  # ç°è‰²è¡¨ç¤ºæ— æµé‡
                    linewidth = 0.8  # å‡å°æ— æµé‡é“¾è·¯çº¿å®½
                    alpha = 0.5

                # è®¡ç®—åŸºæœ¬æ–¹å‘å‘é‡
                dx, dy = x2 - x1, y2 - y1
                dist = np.hypot(dx, dy)
                if dist > 0:
                    dx, dy = dx / dist, dy / dist

                    # è®¡ç®—å‚ç›´åç§»å‘é‡ï¼ˆç”¨äºåˆ†ç¦»åŒå‘ç®­å¤´ï¼‰
                    perp_dx, perp_dy = -dy, dx  # å‚ç›´æ–¹å‘
                    offset = 0.08  # å‡å°åç§»è·ç¦»ï¼Œè®©åŒå‘ç®­å¤´æ›´è¿‘

                    # ä¸ºè¯¥æ–¹å‘çš„ç®­å¤´æ·»åŠ åç§»
                    offset_x1 = x1 + perp_dx * offset
                    offset_y1 = y1 + perp_dy * offset
                    offset_x2 = x2 + perp_dx * offset
                    offset_y2 = y2 + perp_dy * offset

                    # è®¡ç®—ä»åç§»åèŠ‚ç‚¹è¾¹ç¼˜çš„èµ·æ­¢ç‚¹
                    start_x = offset_x1 + dx * square_size / 2
                    start_y = offset_y1 + dy * square_size / 2
                    end_x = offset_x2 - dx * square_size / 2
                    end_y = offset_y2 - dy * square_size / 2

                    # ç»˜åˆ¶å¸¦ç®­å¤´çš„è¿æ¥çº¿ï¼Œç®­å¤´å¤§å°æ ¹æ®ç½‘ç»œè§„æ¨¡è°ƒæ•´
                    arrow = FancyArrowPatch(
                        (start_x, start_y),
                        (end_x, end_y),
                        arrowstyle="-|>",
                        mutation_scale=arrow_scale,
                        color=color,
                        linewidth=linewidth,
                        alpha=alpha,
                        zorder=1,
                    )
                    ax.add_patch(arrow)

                    # å¦‚æœæœ‰å¸¦å®½ï¼Œåœ¨ç®­å¤´æ—è¾¹æ·»åŠ æ ‡ç­¾
                    if bandwidth > 0:
                        # åˆ¤æ–­é“¾è·¯æ–¹å‘å¹¶å†³å®šæ ‡ç­¾ä½ç½®ï¼Œå¢åŠ ä¸ç®­å¤´çš„è·ç¦»
                        if abs(dx) > abs(dy):  # æ°´å¹³é“¾è·¯
                            # æ°´å¹³é“¾è·¯æ ‡ç­¾æ”¾åœ¨ä¸Šä¸‹ï¼Œå¢åŠ è·ç¦»
                            label_offset_x = 0
                            label_offset_y = 0.35 if src < dst else -0.35
                        else:  # å‚ç›´é“¾è·¯
                            # å‚ç›´é“¾è·¯æ ‡ç­¾æ”¾åœ¨å·¦å³ï¼Œå¢åŠ è·ç¦»
                            label_offset_x = 0.35 if src < dst else -0.35
                            label_offset_y = 0

                        mid_x = (start_x + end_x) / 2 + label_offset_x
                        mid_y = (start_y + end_y) / 2 + label_offset_y

                        ax.text(
                            mid_x,
                            mid_y,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * link_label_font_factor,
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="none", alpha=0),
                            color=color,
                            fontweight="bold",
                        )

            # ç»˜åˆ¶èŠ‚ç‚¹å’ŒIPä¿¡æ¯
            for node_id in range(num_nodes):
                x, y = pos[node_id]

                # ç»˜åˆ¶ä¸»èŠ‚ç‚¹æ–¹æ¡†
                rect = Rectangle((x - square_size / 2, y - square_size / 2), width=square_size, height=square_size, color="lightblue", ec="black", linewidth=2, zorder=2)
                ax.add_patch(rect)

                # èŠ‚ç‚¹ç¼–å·å’ŒIPå¸¦å®½ä¿¡æ¯å†™åœ¨æ–¹æ¡†å†…
                # è·å–è¯¥èŠ‚ç‚¹çš„å®é™…IPå¸¦å®½ï¼ˆåŠ¨æ€æ”¯æŒæ‰€æœ‰IPç±»å‹ï¼‰
                node_ip_data = node_ip_bandwidth[node_id]

                # IPç±»å‹é¦–å­—æ¯æ˜ å°„å‡½æ•°
                def get_ip_abbreviation(ip_type):
                    """è·å–IPç±»å‹çš„é¦–å­—æ¯ç¼©å†™"""
                    ip_map = {"gdma": "G", "sdma": "S", "cdma": "C", "ddr": "D", "l2m": "L", "pcie": "P", "ethernet": "E"}
                    return ip_map.get(ip_type.lower(), ip_type.upper()[0] if ip_type else "")

                # å®šä¹‰IPç±»å‹çš„å›ºå®šæ˜¾ç¤ºé¡ºåº
                ip_type_order = ["gdma", "sdma", "cdma", "ddr", "l2m", "pcie", "ethernet"]

                # æ‰¾å‡ºè¯¥èŠ‚ç‚¹æœ‰å¸¦å®½çš„IPç±»å‹å¹¶æŒ‰å›ºå®šé¡ºåºæ’åº
                active_ips = [(ip_type, bw) for ip_type, bw in node_ip_data.items() if bw > 0]

                # æŒ‰é¢„å®šä¹‰é¡ºåºæ’åºï¼ŒæœªçŸ¥ç±»å‹æ’åœ¨æœ€å
                def get_sort_key(item):
                    ip_type, _ = item
                    try:
                        return ip_type_order.index(ip_type.lower())
                    except ValueError:
                        return len(ip_type_order)  # æœªçŸ¥ç±»å‹æ’åœ¨æœ€å

                active_ips.sort(key=get_sort_key)

                if len(active_ips) == 0:
                    ip_text = ""  # æ— æµé‡æ—¶ä¸æ˜¾ç¤ºä»»ä½•æ–‡å­—
                elif len(active_ips) == 1:
                    ip_type, bw = active_ips[0]
                    ip_abbrev = get_ip_abbreviation(ip_type)
                    ip_text = f"{ip_abbrev}: {bw:.1f}"
                else:
                    # å¤šä¸ªIPç±»å‹ï¼Œæ¯ä¸ªIPç±»å‹åˆ†è¡Œæ˜¾ç¤ºï¼ŒæŒ‰å›ºå®šé¡ºåº
                    ip_lines = []
                    for ip_type, bw in active_ips:
                        ip_abbrev = get_ip_abbreviation(ip_type)
                        ip_lines.append(f"{ip_abbrev}: {bw:.1f}")
                    ip_text = "\n".join(ip_lines)

                # åœ¨èŠ‚ç‚¹æ–¹æ¡†å†…æ˜¾ç¤ºä¿¡æ¯
                if ip_text:
                    node_text = f"{node_id}\n{ip_text}"
                else:
                    node_text = f"{node_id}"
                ax.text(x, y, node_text, ha="center", va="center", fontsize=dynamic_font * node_label_font_factor, fontweight="bold")

                # æ·»åŠ è‡ªç¯é“¾è·¯å¸¦å®½æ ‡æ³¨
                if node_id in self_loop_bandwidth:
                    loop_data = self_loop_bandwidth[node_id]

                    # TL_TR (æ°´å¹³è‡ªç¯) - æ ‡åœ¨èŠ‚ç‚¹å·¦å³ä¸¤è¾¹ï¼Œç«–ç€å†™
                    if "TL_TR" in loop_data:
                        bandwidth = loop_data["TL_TR"]
                        # å·¦ä¾§æ ‡æ³¨
                        ax.text(
                            x - square_size / 2 - 0.3,
                            y,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * link_label_font_factor,
                            rotation=90,
                            fontweight="bold",
                            color="red",
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="none", alpha=0),
                        )

                    if "TR_TL" in loop_data:
                        bandwidth = loop_data["TR_TL"]
                        # å³ä¾§æ ‡æ³¨
                        ax.text(
                            x + square_size / 2 + 0.3,
                            y,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * link_label_font_factor,
                            rotation=90,
                            fontweight="bold",
                            color="red",
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="none", alpha=0),
                        )

                    # TU_TD (å‚ç›´è‡ªç¯) - æ ‡åœ¨èŠ‚ç‚¹ä¸Šä¸‹ä¸¤è¾¹ï¼Œæ­£å¸¸å†™
                    if "TU_TD" in loop_data:
                        bandwidth = loop_data["TU_TD"]
                        # ä¸Šä¾§æ ‡æ³¨
                        ax.text(
                            x,
                            y + square_size / 2 + 0.3,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * link_label_font_factor,
                            fontweight="bold",
                            color="red",
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="none", alpha=0),
                        )

                    if "TD_TU" in loop_data:
                        bandwidth = loop_data["TD_TU"]
                        # ä¸‹ä¾§æ ‡æ³¨
                        ax.text(
                            x,
                            y - square_size / 2 - 0.3,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * link_label_font_factor,
                            fontweight="bold",
                            color="red",
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="none", alpha=0),
                        )

            # è®¾ç½®æ ‡é¢˜å’Œå¸ƒå±€
            title = f"æµé‡åˆ†å¸ƒå›¾"
            ax.set_title(title, fontsize=16, fontweight="bold")

            # è®¾ç½®åæ ‡è½´èŒƒå›´ï¼Œä¸ºå›¾ä¾‹é¢„ç•™æ›´å¤šç©ºé—´
            all_x = [x for x, y in pos.values()]
            all_y = [y for x, y in pos.values()]
            margin = 1.5
            # å³ä¾§é¢„ç•™æ›´å¤šç©ºé—´ç»™å›¾ä¾‹
            ax.set_xlim(min(all_x) - margin, max(all_x) + margin + 1.5)
            ax.set_ylim(min(all_y) - margin, max(all_y) + margin)
            ax.axis("off")

            # ç§»é™¤å›¾ä¾‹æ˜¾ç¤º

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures and save_dir:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_traffic_distribution_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=150)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ æµé‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºæµé‡åˆ†å¸ƒå›¾")
                plt.show()
                return ""

        except Exception as e:
            print(f"ç”Ÿæˆæµé‡åˆ†å¸ƒå›¾å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return ""

    def analyze_noc_results(
        self,
        request_tracker,
        config,
        model,
        results: Dict[str, Any],
        enable_visualization: bool = True,
        save_results: bool = True,
        save_dir: str = "output",
        save_figures: bool = True,
        verbose: bool = True,
        viz_config: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        NoCä»¿çœŸç»“æœå®Œæ•´åˆ†æ

        Args:
            request_tracker: RequestTrackerå®ä¾‹
            config: NoCé…ç½®
            model: NoCæ¨¡å‹å®ä¾‹
            results: ä»¿çœŸåŸºç¡€ç»“æœ
            enable_visualization: æ˜¯å¦ç”Ÿæˆå›¾è¡¨
            save_results: æ˜¯å¦ä¿å­˜ç»“æœæ–‡ä»¶

        Returns:
            è¯¦ç»†çš„åˆ†æç»“æœï¼ˆä¸­æ–‡æ ¼å¼ï¼‰
        """
        analysis = {}

        # åŸºç¡€æŒ‡æ ‡
        sim_info = results.get("simulation_info", {})
        total_requests = len(request_tracker.completed_requests) + len(request_tracker.active_requests)
        completed_requests = len(request_tracker.completed_requests)
        active_requests = len(request_tracker.active_requests)

        analysis["åŸºç¡€æŒ‡æ ‡"] = {
            "æ€»å‘¨æœŸæ•°": sim_info.get("total_cycles", 0),
            "æ€»è¯·æ±‚æ•°": total_requests,
            "å·²å®Œæˆè¯·æ±‚æ•°": completed_requests,
            "æ´»è·ƒè¯·æ±‚æ•°": active_requests,
            "å®Œæˆç‡": f"{(completed_requests / total_requests * 100):.2f}%" if total_requests > 0 else "0.00%",
        }

        # è½¬æ¢æ•°æ®æ ¼å¼
        metrics = self.convert_tracker_to_request_info(request_tracker, config)

        if not metrics:
            return analysis

        # æ·»åŠ è¯¦ç»†æ•°æ®ç»Ÿè®¡è¾“å‡º
        self._print_data_statistics(metrics)

        # å¸¦å®½åˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["å¸¦å®½æŒ‡æ ‡"] = self.analyze_bandwidth(metrics, verbose=verbose)

        # å»¶è¿Ÿåˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["å»¶è¿ŸæŒ‡æ ‡"] = self.analyze_latency(metrics, verbose=verbose)

        # ç«¯å£å¸¦å®½åˆ†æ
        analysis["ç«¯å£å¸¦å®½åˆ†æ"] = self.analyze_port_bandwidth(metrics, verbose=verbose)

        # Tagå’Œç»•ç¯æ•°æ®åˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["Tagå’Œç»•ç¯åˆ†æ"] = self.analyze_tag_data(model, verbose=verbose)

        # ç”Ÿæˆå›¾è¡¨
        if enable_visualization:
            chart_paths = []

            # ä½¿ç”¨viz_configæ§åˆ¶ç‹¬ç«‹çš„å›¾è¡¨ç”Ÿæˆ
            viz_config = viz_config or {}

            # å¸¦å®½åˆ†æå›¾ï¼ˆåŒ…æ‹¬å¸¦å®½æ›²çº¿å›¾ï¼‰
            if viz_config.get("bandwidth_analysis", False):
                bw_path = self.plot_bandwidth_curves(metrics, save_dir=save_dir, save_figures=save_figures, verbose=verbose)
                if bw_path:
                    chart_paths.append(bw_path)

            # å»¶è¿Ÿåˆ†æå›¾
            if viz_config.get("latency_analysis", False):
                lat_path = self.plot_latency_distribution(metrics, save_dir=save_dir, save_figures=save_figures, verbose=verbose)
                if lat_path:
                    chart_paths.append(lat_path)

            # æµé‡åˆ†å¸ƒå›¾
            if viz_config.get("flow_distribution", False):
                traffic_path = self.plot_traffic_distribution(model, metrics, save_dir=save_dir, mode="total", save_figures=save_figures, verbose=verbose)
                if traffic_path:
                    chart_paths.append(traffic_path)

            analysis["å¯è§†åŒ–æ–‡ä»¶"] = {"ç”Ÿæˆçš„å›¾è¡¨": chart_paths, "å›¾è¡¨æ•°é‡": len(chart_paths)}

        # ä¿å­˜ç»“æœæ–‡ä»¶
        if save_results:
            # ä¿å­˜åˆ†æç»“æœJSON
            results_file = self.save_results(analysis, save_dir=save_dir)

            # ä¿å­˜è¯¦ç»†è¯·æ±‚CSVæ–‡ä»¶
            csv_files = self.save_detailed_requests_csv(metrics, save_dir=save_dir)

            # ä¿å­˜ç«¯å£å¸¦å®½CSVæ–‡ä»¶
            ports_csv = self.save_ports_bandwidth_csv(metrics, save_dir=save_dir, config=config)

            output_files = {}
            if results_file:
                output_files["åˆ†æç»“æœæ–‡ä»¶"] = results_file

            # æ·»åŠ CSVæ–‡ä»¶ä¿¡æ¯
            if csv_files:
                if "read_requests_csv" in csv_files:
                    output_files["è¯»è¯·æ±‚CSV"] = csv_files["read_requests_csv"]
                if "write_requests_csv" in csv_files:
                    output_files["å†™è¯·æ±‚CSV"] = csv_files["write_requests_csv"]

            if ports_csv:
                output_files["ç«¯å£å¸¦å®½CSV"] = ports_csv

            if output_files:
                analysis["è¾“å‡ºæ–‡ä»¶"] = {
                    **output_files,
                    "ä¿å­˜æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                }

                # è¾“å‡ºæ–‡ä»¶ä¿å­˜è·¯å¾„æ€»ç»“
                print("\n" + "=" * 60)
                print("ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜æ€»ç»“")
                print("=" * 60)

                # ç»Ÿè®¡è¯·æ±‚æ•°é‡
                read_count = len([req for req in metrics if getattr(req, "req_type", None) == "read"])
                write_count = len([req for req in metrics if getattr(req, "req_type", None) == "write"])

                print("è¯¦ç»†è¯·æ±‚è®°å½•ç»Ÿè®¡:")
                if "è¯»è¯·æ±‚CSV" in output_files:
                    print(f"  è¯»è¯·æ±‚CSV, {read_count} æ¡è®°å½•:  {output_files['è¯»è¯·æ±‚CSV']}")
                if "å†™è¯·æ±‚CSV" in output_files:
                    print(f"  å†™è¯·æ±‚CSV, {write_count} æ¡è®°å½•:  {output_files['å†™è¯·æ±‚CSV']}")

                if "åˆ†æç»“æœæ–‡ä»¶" in output_files:
                    print(f"åˆ†æé…ç½®å·²ä¿å­˜: {output_files['åˆ†æç»“æœæ–‡ä»¶']}")

                if "ç«¯å£å¸¦å®½CSV" in output_files:
                    print(f"å…·ä½“ç«¯å£çš„ç»Ÿè®¡CSVï¼š {output_files['ç«¯å£å¸¦å®½CSV']}")

                print("=" * 60)

        return analysis
