"""
NoCç»“æœåˆ†æå™¨
é€šç”¨çš„NoCæ€§èƒ½åˆ†æå·¥å…·ï¼ŒåŒ…å«å¸¦å®½ã€å»¶è¿Ÿã€æµé‡åˆ†æç­‰åŠŸèƒ½
æ”¯æŒå¤šç§NoCæ‹“æ‰‘ï¼ˆCrossRingã€Meshç­‰ï¼‰
"""

from typing import Dict, List, Optional, Any
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Rectangle, FancyArrowPatch
from matplotlib import font_manager
import sys
import matplotlib
import logging

# è®¾ç½®matplotlibå­—ä½“ç®¡ç†å™¨çš„æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

if sys.platform == "darwin":  # macOS çš„ç³»ç»Ÿæ ‡è¯†æ˜¯ 'darwin'
    matplotlib.use("macosx")  # ä»…åœ¨ macOS ä¸Šä½¿ç”¨è¯¥åç«¯
# è®¾ç½®ä¸­è‹±æ–‡å­—ä½“
plt.rcParams["font.sans-serif"] = ["Microsoft YaHei", "SimHei", "DejaVu Sans"]  # ä¸­æ–‡å­—ä½“ä¼˜å…ˆä½¿ç”¨å¾®è½¯é›…é»‘
plt.rcParams["font.serif"] = ["Times New Roman", "Times", "DejaVu Serif"]  # è‹±æ–‡serifå­—ä½“ä½¿ç”¨Times
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams["font.size"] = 10  # é»˜è®¤å­—ä½“å¤§å°
plt.rcParams["axes.titlesize"] = 12  # æ ‡é¢˜å­—ä½“å¤§å°
plt.rcParams["axes.labelsize"] = 10  # è½´æ ‡ç­¾å­—ä½“å¤§å°
plt.rcParams["legend.fontsize"] = 9  # å›¾ä¾‹å­—ä½“å¤§å°
import networkx as nx
import logging
import time
import os
import json
from dataclasses import dataclass
from enum import Enum


class RequestType(Enum):
    """è¯·æ±‚ç±»å‹"""

    READ = "read"
    WRITE = "write"
    ALL = "all"


@dataclass
class RequestInfo:
    """è¯·æ±‚ä¿¡æ¯æ•°æ®ç»“æ„ï¼ˆä¸è€ç‰ˆæœ¬å…¼å®¹ï¼‰"""

    packet_id: str
    start_time: int  # ns
    end_time: int  # ns
    rn_end_time: int  # ns (RNç«¯å£ç»“æŸæ—¶é—´)
    sn_end_time: int  # ns (SNç«¯å£ç»“æŸæ—¶é—´)
    req_type: str  # 'read' or 'write'
    source_node: int
    dest_node: int
    source_type: str
    dest_type: str
    burst_length: int
    total_bytes: int
    cmd_latency: int
    data_latency: int
    transaction_latency: int


@dataclass
class WorkingInterval:
    """å·¥ä½œåŒºé—´æ•°æ®ç»“æ„ï¼ˆä¸è€ç‰ˆæœ¬å…¼å®¹ï¼‰"""

    start_time: int
    end_time: int
    duration: int
    flit_count: int
    total_bytes: int
    request_count: int

    @property
    def bandwidth(self) -> float:
        """åŒºé—´å†…å¹³å‡å¸¦å®½ (GB/s)"""
        return self.total_bytes / self.duration if self.duration > 0 else 0.0


class ResultAnalyzer:
    """é€šç”¨NoCç»“æœåˆ†æå™¨"""

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        plt.rcParams["font.sans-serif"] = [
            "SimHei",  # é»‘ä½“
            "Microsoft YaHei",  # å¾®è½¯é›…é»‘
            "WenQuanYi Zen Hei",  # æ–‡æ³‰é©¿æ­£é»‘
            "Noto Sans CJK SC",  # æ€æºé»‘ä½“
            "PingFang SC",  # è‹¹æ–¹
            "Heiti SC",  # é»‘ä½“-ç®€
            "Arial Unicode MS",  # ä¸€ç§åŒ…å«å¤šç§å­—ç¬¦çš„å­—ä½“
            "DejaVu Sans",
        ]
        plt.rcParams["axes.unicode_minus"] = False

        # ç”¨äºå­˜å‚¨å¸¦å®½æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä»¿ç…§æ—§ç‰ˆæœ¬ï¼‰
        self.bandwidth_time_series = defaultdict(lambda: {"time": [], "start_times": [], "bytes": []})

    def convert_tracker_to_request_info(self, request_tracker, config) -> List[RequestInfo]:
        """è½¬æ¢RequestTrackeræ•°æ®ä¸ºRequestInfoæ ¼å¼ï¼ˆä½¿ç”¨æ­£ç¡®çš„å»¶è¿Ÿè®¡ç®—ï¼‰"""
        requests = []

        # è·å–é…ç½®å‚æ•°
        network_frequency = getattr(config.basic_config, "NETWORK_FREQUENCY", 2.0) if hasattr(config, "basic_config") else 2.0
        # cycleæ—¶é—´ï¼š1000 / frequency (ns per cycle) - ä¾‹å¦‚2GHz = 0.5ns per cycle
        cycle_time_ns = 1000.0 / (network_frequency * 1000)  # frequencyæ˜¯GHzï¼Œè½¬æ¢ä¸ºns

        for req_id, lifecycle in request_tracker.completed_requests.items():
            # ä½¿ç”¨å®é™…çš„æ—¶é—´æˆ³
            # æ—¶é—´è½¬æ¢ï¼šcycle -> ns
            start_time = int(lifecycle.created_cycle * cycle_time_ns)
            end_time = int(lifecycle.completed_cycle * cycle_time_ns)

            # æå–source_typeå’Œdest_type
            source_type = "unknown"
            dest_type = "unknown"

            # ç›´æ¥ä»æ‰€æœ‰flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            cmd_entry_cake0_cycle = np.inf
            cmd_entry_noc_from_cake0_cycle = np.inf
            cmd_entry_noc_from_cake1_cycle = np.inf
            cmd_received_by_cake0_cycle = np.inf
            cmd_received_by_cake1_cycle = np.inf
            data_entry_noc_from_cake0_cycle = np.inf
            data_entry_noc_from_cake1_cycle = np.inf
            data_received_complete_cycle = np.inf

            # ä»request flitsä¸­æ”¶é›†æ—¶é—´æˆ³å’ŒIPç±»å‹
            for flit in lifecycle.request_flits:
                # æå–IPç±»å‹ä¿¡æ¯
                if hasattr(flit, "source_type") and source_type == "unknown":
                    source_type = flit.source_type
                if hasattr(flit, "destination_type") and dest_type == "unknown":
                    dest_type = flit.destination_type

                # æ”¶é›†æ—¶é—´æˆ³
                if hasattr(flit, "cmd_entry_cake0_cycle") and flit.cmd_entry_cake0_cycle < np.inf:
                    cmd_entry_cake0_cycle = min(cmd_entry_cake0_cycle, flit.cmd_entry_cake0_cycle)
                if hasattr(flit, "cmd_entry_noc_from_cake0_cycle") and flit.cmd_entry_noc_from_cake0_cycle < np.inf:
                    cmd_entry_noc_from_cake0_cycle = min(cmd_entry_noc_from_cake0_cycle, flit.cmd_entry_noc_from_cake0_cycle)
                if hasattr(flit, "cmd_entry_noc_from_cake1_cycle") and flit.cmd_entry_noc_from_cake1_cycle < np.inf:
                    cmd_entry_noc_from_cake1_cycle = min(cmd_entry_noc_from_cake1_cycle, flit.cmd_entry_noc_from_cake1_cycle)
                if hasattr(flit, "cmd_received_by_cake0_cycle") and flit.cmd_received_by_cake0_cycle < np.inf:
                    cmd_received_by_cake0_cycle = min(cmd_received_by_cake0_cycle, flit.cmd_received_by_cake0_cycle)
                if hasattr(flit, "cmd_received_by_cake1_cycle") and flit.cmd_received_by_cake1_cycle < np.inf:
                    cmd_received_by_cake1_cycle = min(cmd_received_by_cake1_cycle, flit.cmd_received_by_cake1_cycle)

            # ä»response flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            for flit in lifecycle.response_flits:
                if hasattr(flit, "cmd_received_by_cake0_cycle") and flit.cmd_received_by_cake0_cycle < np.inf:
                    cmd_received_by_cake0_cycle = min(cmd_received_by_cake0_cycle, flit.cmd_received_by_cake0_cycle)
                if hasattr(flit, "cmd_received_by_cake1_cycle") and flit.cmd_received_by_cake1_cycle < np.inf:
                    cmd_received_by_cake1_cycle = min(cmd_received_by_cake1_cycle, flit.cmd_received_by_cake1_cycle)

            # ä»data flitsä¸­æ”¶é›†æ—¶é—´æˆ³
            for flit in lifecycle.data_flits:
                if hasattr(flit, "data_entry_noc_from_cake0_cycle") and flit.data_entry_noc_from_cake0_cycle < np.inf:
                    data_entry_noc_from_cake0_cycle = min(data_entry_noc_from_cake0_cycle, flit.data_entry_noc_from_cake0_cycle)
                if hasattr(flit, "data_entry_noc_from_cake1_cycle") and flit.data_entry_noc_from_cake1_cycle < np.inf:
                    data_entry_noc_from_cake1_cycle = min(data_entry_noc_from_cake1_cycle, flit.data_entry_noc_from_cake1_cycle)
                if hasattr(flit, "data_received_complete_cycle") and flit.data_received_complete_cycle < np.inf:
                    data_received_complete_cycle = min(data_received_complete_cycle, flit.data_received_complete_cycle)

            # æŒ‰ç…§BaseFlitçš„calculate_latenciesæ–¹æ³•è®¡ç®—å»¶è¿Ÿ
            cmd_latency = np.inf
            data_latency = np.inf
            transaction_latency = np.inf

            # è°ƒè¯•ï¼šæ‰“å°æ”¶é›†åˆ°çš„æ—¶é—´æˆ³
            if req_id == list(request_tracker.completed_requests.keys())[0]:  # åªæ‰“å°ç¬¬ä¸€ä¸ªè¯·æ±‚
                self.logger.debug(f"è¯·æ±‚ {req_id} æ—¶é—´æˆ³:")
                self.logger.debug(f"  cmd_entry_cake0_cycle: {cmd_entry_cake0_cycle}")
                self.logger.debug(f"  cmd_entry_noc_from_cake0_cycle: {cmd_entry_noc_from_cake0_cycle}")
                self.logger.debug(f"  cmd_received_by_cake1_cycle: {cmd_received_by_cake1_cycle}")
                self.logger.debug(f"  data_entry_noc_from_cake0_cycle: {data_entry_noc_from_cake0_cycle}")
                self.logger.debug(f"  data_entry_noc_from_cake1_cycle: {data_entry_noc_from_cake1_cycle}")
                self.logger.debug(f"  data_received_complete_cycle: {data_received_complete_cycle}")
                self.logger.debug(f"  lifecycleä¸­çš„flitæ•°é‡: req={len(lifecycle.request_flits)}, rsp={len(lifecycle.response_flits)}, data={len(lifecycle.data_flits)}")

            # å‘½ä»¤å»¶è¿Ÿï¼šcmd_received_by_cake1_cycle - cmd_entry_noc_from_cake0_cycle
            if cmd_entry_noc_from_cake0_cycle < np.inf and cmd_received_by_cake1_cycle < np.inf:
                cmd_latency = cmd_received_by_cake1_cycle - cmd_entry_noc_from_cake0_cycle

            # æ•°æ®å»¶è¿Ÿï¼šæ ¹æ®è¯»å†™ç±»å‹ä¸åŒ
            if lifecycle.op_type == "read":
                # è¯»æ“ä½œï¼šdata_received_complete_cycle - data_entry_noc_from_cake1_cycle
                if data_entry_noc_from_cake1_cycle < np.inf and data_received_complete_cycle < np.inf:
                    data_latency = data_received_complete_cycle - data_entry_noc_from_cake1_cycle
            elif lifecycle.op_type == "write":
                # å†™æ“ä½œï¼šdata_received_complete_cycle - data_entry_noc_from_cake0_cycle
                if data_entry_noc_from_cake0_cycle < np.inf and data_received_complete_cycle < np.inf:
                    data_latency = data_received_complete_cycle - data_entry_noc_from_cake0_cycle

            # äº‹åŠ¡å»¶è¿Ÿï¼šdata_received_complete_cycle - cmd_entry_cake0_cycle
            if cmd_entry_cake0_cycle < np.inf and data_received_complete_cycle < np.inf:
                transaction_latency = data_received_complete_cycle - cmd_entry_cake0_cycle

            # å°†cycleå»¶è¿Ÿè½¬æ¢ä¸ºns
            cmd_latency_ns = int(cmd_latency * cycle_time_ns) if cmd_latency < np.inf else 0
            data_latency_ns = int(data_latency * cycle_time_ns) if data_latency < np.inf else 0
            transaction_latency_ns = int(transaction_latency * cycle_time_ns) if transaction_latency < np.inf else 0

            # è®¡ç®—RNå’ŒSNç«¯å£ç»“æŸæ—¶é—´ï¼ˆæŒ‰ç…§æ—§ç‰ˆæœ¬é€»è¾‘åŒºåˆ†è¯»å†™æ“ä½œï¼‰
            if lifecycle.op_type == "read":
                # è¯»è¯·æ±‚ï¼šRNæ”¶åˆ°æ•°æ®æ—¶ç»“æŸï¼ŒSNå‘å‡ºæ•°æ®æ—¶ç»“æŸ
                rn_end_time = int(data_received_complete_cycle * cycle_time_ns) if data_received_complete_cycle < np.inf else end_time
                sn_end_time = int(data_entry_noc_from_cake1_cycle * cycle_time_ns) if data_entry_noc_from_cake1_cycle < np.inf else end_time
            else:  # write
                # å†™è¯·æ±‚ï¼šRNå‘å‡ºæ•°æ®æ—¶ç»“æŸï¼ŒSNæ”¶åˆ°æ•°æ®æ—¶ç»“æŸ
                rn_end_time = int(data_entry_noc_from_cake0_cycle * cycle_time_ns) if data_entry_noc_from_cake0_cycle < np.inf else end_time
                sn_end_time = int(data_received_complete_cycle * cycle_time_ns) if data_received_complete_cycle < np.inf else end_time

            # è®¡ç®—å­—èŠ‚æ•°
            burst_length = lifecycle.burst_size
            total_bytes = burst_length * 128  # 128å­—èŠ‚/flit

            request_info = RequestInfo(
                packet_id=str(req_id),
                start_time=start_time,
                end_time=end_time,
                rn_end_time=rn_end_time,
                sn_end_time=sn_end_time,
                req_type=lifecycle.op_type,
                source_node=lifecycle.source,
                dest_node=lifecycle.destination,
                source_type=source_type,
                dest_type=dest_type,
                burst_length=burst_length,
                total_bytes=total_bytes,
                cmd_latency=cmd_latency_ns,
                data_latency=data_latency_ns,
                transaction_latency=transaction_latency_ns,
            )
            requests.append(request_info)

        return requests

    def calculate_working_intervals(self, requests: List[RequestInfo], min_gap_threshold: int = 200) -> List[WorkingInterval]:
        """è®¡ç®—å·¥ä½œåŒºé—´ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not requests:
            return []

        # åˆ›å»ºæ—¶é—´ç‚¹äº‹ä»¶åˆ—è¡¨
        events = []
        for req in requests:
            events.append((req.start_time, "start", req.packet_id))
            events.append((req.end_time, "end", req.packet_id))

        # æŒ‰æ—¶é—´æ’åº
        events.sort()

        # è¯†åˆ«å·¥ä½œåŒºé—´
        raw_intervals = []
        active_requests = set()
        current_start = None

        for time_point, event_type, packet_id in events:
            if event_type == "start":
                if not active_requests:
                    current_start = time_point
                active_requests.add(packet_id)
            else:  # 'end'
                active_requests.discard(packet_id)
                if not active_requests and current_start is not None:
                    # å·¥ä½œåŒºé—´ç»“æŸ
                    raw_intervals.append((current_start, time_point))
                    current_start = None

        # å¤„ç†æœ€åæœªç»“æŸçš„åŒºé—´
        if active_requests and current_start is not None:
            last_end = max(req.end_time for req in requests)
            raw_intervals.append((current_start, last_end))

        # åˆå¹¶ç›¸è¿‘åŒºé—´
        merged_intervals = self._merge_close_intervals(raw_intervals, min_gap_threshold)

        # æ„å»ºWorkingIntervalå¯¹è±¡
        working_intervals = []
        for start, end in merged_intervals:
            # æ‰¾åˆ°è¯¥åŒºé—´å†…çš„æ‰€æœ‰è¯·æ±‚
            interval_requests = [req for req in requests if req.start_time < end and req.end_time > start]

            if not interval_requests:
                continue

            # è®¡ç®—åŒºé—´ç»Ÿè®¡
            total_bytes = sum(req.total_bytes for req in interval_requests)
            flit_count = sum(req.burst_length for req in interval_requests)

            interval = WorkingInterval(start_time=start, end_time=end, duration=end - start, flit_count=flit_count, total_bytes=total_bytes, request_count=len(interval_requests))
            working_intervals.append(interval)

        return working_intervals

    def _merge_close_intervals(self, intervals: List[tuple], min_gap_threshold: int) -> List[tuple]:
        """åˆå¹¶ç›¸è¿‘çš„æ—¶é—´åŒºé—´ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not intervals:
            return []

        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        sorted_intervals = sorted(intervals)
        merged = [sorted_intervals[0]]

        for current_start, current_end in sorted_intervals[1:]:
            last_start, last_end = merged[-1]

            # å¦‚æœé—´éš™å°äºé˜ˆå€¼ï¼Œåˆ™åˆå¹¶
            if current_start - last_end <= min_gap_threshold:
                merged[-1] = (last_start, max(last_end, current_end))
            else:
                merged.append((current_start, current_end))

        return merged

    def calculate_bandwidth_metrics(self, requests: List[RequestInfo], operation_type: str = None, min_gap_threshold: int = 200, endpoint_type: str = "network") -> Dict[str, Any]:
        """è®¡ç®—å¸¦å®½æŒ‡æ ‡ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬å®Œæ•´é€»è¾‘ï¼‰

        Args:
            requests: è¯·æ±‚åˆ—è¡¨
            operation_type: æ“ä½œç±»å‹ ("read", "write", None)
            min_gap_threshold: æœ€å°é—´éš™é˜ˆå€¼
            endpoint_type: ç«¯ç‚¹ç±»å‹ ("network", "rn", "sn")
        """
        if not requests:
            return {}

        # ç­›é€‰è¯·æ±‚å¹¶åˆ›å»ºä¸´æ—¶è¯·æ±‚åˆ—è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç»“æŸæ—¶é—´ï¼‰
        filtered_requests = []
        for req in requests:
            if operation_type is not None and req.req_type != operation_type:
                continue

            # æ ¹æ®endpoint_typeé€‰æ‹©æ­£ç¡®çš„ç»“æŸæ—¶é—´
            if endpoint_type == "rn":
                end_time = req.rn_end_time
            elif endpoint_type == "sn":
                end_time = req.sn_end_time
            else:  # network
                end_time = req.end_time

            # åˆ›å»ºä¸´æ—¶è¯·æ±‚å¯¹è±¡ï¼Œä½¿ç”¨æ­£ç¡®çš„ç»“æŸæ—¶é—´
            temp_req = RequestInfo(
                packet_id=req.packet_id,
                start_time=req.start_time,
                end_time=end_time,
                rn_end_time=req.rn_end_time,
                sn_end_time=req.sn_end_time,
                req_type=req.req_type,
                source_node=req.source_node,
                dest_node=req.dest_node,
                source_type=req.source_type,
                dest_type=req.dest_type,
                burst_length=req.burst_length,
                total_bytes=req.total_bytes,
                cmd_latency=req.cmd_latency,
                data_latency=req.data_latency,
                transaction_latency=req.transaction_latency,
            )
            filtered_requests.append(temp_req)

        if not filtered_requests:
            return {}

        # è®¡ç®—å·¥ä½œåŒºé—´
        working_intervals = self.calculate_working_intervals(filtered_requests, min_gap_threshold)

        # ç½‘ç»œå·¥ä½œæ—¶é—´çª—å£
        network_start = min(req.start_time for req in filtered_requests)
        network_end = max(req.end_time for req in filtered_requests)
        total_network_time = network_end - network_start

        # æ€»å·¥ä½œæ—¶é—´å’Œæ€»å­—èŠ‚æ•°
        total_working_time = sum(interval.duration for interval in working_intervals)
        total_bytes = sum(req.total_bytes for req in filtered_requests)

        # è®¡ç®—éåŠ æƒå¸¦å®½ï¼šæ€»æ•°æ®é‡ / ç½‘ç»œæ€»æ—¶é—´
        unweighted_bandwidth = (total_bytes / total_network_time) if total_network_time > 0 else 0.0

        # è®¡ç®—åŠ æƒå¸¦å®½ï¼šå„åŒºé—´å¸¦å®½æŒ‰flitæ•°é‡åŠ æƒå¹³å‡
        if working_intervals:
            total_weighted_bandwidth = 0.0
            total_weight = 0

            for interval in working_intervals:
                weight = interval.flit_count  # æƒé‡æ˜¯å·¥ä½œæ—¶é—´æ®µçš„flitæ•°é‡
                bandwidth = interval.bandwidth  # GB/s
                total_weighted_bandwidth += bandwidth * weight
                total_weight += weight

            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
        else:
            weighted_bandwidth = 0.0

        return {
            "éåŠ æƒå¸¦å®½_GB/s": f"{unweighted_bandwidth:.2f}",
            "åŠ æƒå¸¦å®½_GB/s": f"{weighted_bandwidth:.2f}",
            "æ€»ä¼ è¾“å­—èŠ‚æ•°": total_bytes,
            "æ€»è¯·æ±‚æ•°": len(filtered_requests),
            "å·¥ä½œåŒºé—´æ•°é‡": len(working_intervals),
            "æ€»å·¥ä½œæ—¶é—´_ns": total_working_time,
            "ç½‘ç»œæ—¶é—´_ns": total_network_time,
        }

    def _print_data_statistics(self, metrics):
        """æ‰“å°è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not metrics:
            return

        # ç»Ÿè®¡è¯»å†™è¯·æ±‚å’Œflitæ•°é‡
        read_requests = [m for m in metrics if m.req_type == "read"]
        write_requests = [m for m in metrics if m.req_type == "write"]

        read_flit_count = sum(m.burst_length for m in read_requests)
        write_flit_count = sum(m.burst_length for m in write_requests)
        total_flit_count = read_flit_count + write_flit_count

        # æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•çš„æ‰“å°è¢«ç§»åŠ¨åˆ°æ¨¡å‹çš„_print_traffic_statisticsä¸­
        # é¿å…é‡å¤æ‰“å°

    def _print_detailed_bandwidth_analysis(self, bandwidth_metrics):
        """æ‰“å°è¯¦ç»†çš„å¸¦å®½åˆ†æç»“æœ"""
        if not bandwidth_metrics:
            return

        print("\n" + "=" * 60)
        print("ç½‘ç»œå¸¦å®½åˆ†æç»“æœæ‘˜è¦")
        print("=" * 60)

        # ç½‘ç»œæ•´ä½“å¸¦å®½
        if "æ€»ä½“å¸¦å®½" in bandwidth_metrics:
            overall = bandwidth_metrics["æ€»ä½“å¸¦å®½"]
            print("ç½‘ç»œæ•´ä½“å¸¦å®½:")

            # æŒ‰æ“ä½œç±»å‹åˆ†ç±»æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºåŠ æƒå¸¦å®½ï¼‰
            for op_type in ["è¯»", "å†™", "æ··åˆ", "æ€»"]:
                if f"{op_type}å¸¦å®½" in overall:
                    bw_data = overall[f"{op_type}å¸¦å®½"]
                    weighted = bw_data.get("åŠ æƒå¸¦å®½_GB/s", 0)
                    print(f"  {op_type}å¸¦å®½: {weighted:.3f} GB/s")

    def _print_detailed_latency_analysis(self, latency_metrics, metrics):
        """æ‰“å°è¯¦ç»†çš„å»¶è¿Ÿåˆ†æç»“æœ"""
        if not latency_metrics or not metrics:
            return

        print("\nå»¶è¿Ÿç»Ÿè®¡ (å•ä½: cycle)")

        # æŒ‰è¯»å†™åˆ†ç±»ç»Ÿè®¡å»¶è¿Ÿ
        read_metrics = [m for m in metrics if m.req_type == "read"]
        write_metrics = [m for m in metrics if m.req_type == "write"]

        # CMDå»¶è¿Ÿ
        if read_metrics:
            read_cmd_avg = sum(m.cmd_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
            read_cmd_max = max(m.cmd_latency for m in read_metrics) if len(read_metrics) > 0 else 0
        else:
            read_cmd_avg = read_cmd_max = 0

        if write_metrics:
            write_cmd_avg = sum(m.cmd_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
            write_cmd_max = max(m.cmd_latency for m in write_metrics) if len(write_metrics) > 0 else 0
        else:
            write_cmd_avg = write_cmd_max = 0

        mixed_cmd_avg = sum(m.cmd_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
        mixed_cmd_max = max(m.cmd_latency for m in metrics) if len(metrics) > 0 else 0

        print(f"  CMD å»¶è¿Ÿ  - è¯»: avg {read_cmd_avg:.2f}, max {read_cmd_max}ï¼›å†™: avg {write_cmd_avg:.2f}, max {write_cmd_max}ï¼›æ··åˆ: avg {mixed_cmd_avg:.2f}, max {mixed_cmd_max}")

        # Dataå»¶è¿Ÿ
        if read_metrics:
            read_data_avg = sum(m.data_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
            read_data_max = max(m.data_latency for m in read_metrics) if len(read_metrics) > 0 else 0
        else:
            read_data_avg = read_data_max = 0

        if write_metrics:
            write_data_avg = sum(m.data_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
            write_data_max = max(m.data_latency for m in write_metrics) if len(write_metrics) > 0 else 0
        else:
            write_data_avg = write_data_max = 0

        mixed_data_avg = sum(m.data_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
        mixed_data_max = max(m.data_latency for m in metrics) if len(metrics) > 0 else 0

        print(
            f"  Data å»¶è¿Ÿ  - è¯»: avg {read_data_avg:.2f}, max {read_data_max}ï¼›å†™: avg {write_data_avg:.2f}, max {write_data_max}ï¼›æ··åˆ: avg {mixed_data_avg:.2f}, max {mixed_data_max}"
        )

        # Transå»¶è¿Ÿ
        if read_metrics:
            read_trans_avg = sum(m.transaction_latency for m in read_metrics) / len(read_metrics) if len(read_metrics) > 0 else 0
            read_trans_max = max(m.transaction_latency for m in read_metrics) if len(read_metrics) > 0 else 0
        else:
            read_trans_avg = read_trans_max = 0

        if write_metrics:
            write_trans_avg = sum(m.transaction_latency for m in write_metrics) / len(write_metrics) if len(write_metrics) > 0 else 0
            write_trans_max = max(m.transaction_latency for m in write_metrics) if len(write_metrics) > 0 else 0
        else:
            write_trans_avg = write_trans_max = 0

        mixed_trans_avg = sum(m.transaction_latency for m in metrics) / len(metrics) if len(metrics) > 0 else 0
        mixed_trans_max = max(m.transaction_latency for m in metrics) if len(metrics) > 0 else 0

        print(
            f"  Trans å»¶è¿Ÿ  - è¯»: avg {read_trans_avg:.2f}, max {read_trans_max}ï¼›å†™: avg {write_trans_avg:.2f}, max {write_trans_max}ï¼›æ··åˆ: avg {mixed_trans_avg:.2f}, max {mixed_trans_max}"
        )

        # æ€»å¸¦å®½æ˜¾ç¤ºï¼ˆä½¿ç”¨åŠ æƒå¸¦å®½ï¼‰
        if "latency_metrics" in locals() and "æ€»ä½“å¸¦å®½" in latency_metrics:
            total_bw = latency_metrics["æ€»ä½“å¸¦å®½"].get("æ€»å¸¦å®½", {}).get("åŠ æƒå¸¦å®½_GB/s", 0)
            print(f"Total Bandwidth: {total_bw:.2f} GB/s")
        else:
            # ä»å¸¦å®½æŒ‡æ ‡ä¸­è·å–æ€»å¸¦å®½
            total_bw = 0
            print(f"Total Bandwidth: {total_bw:.2f} GB/s")

        print("=" * 60)

    def analyze_bandwidth(self, requests: List[RequestInfo], verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æå¸¦å®½æŒ‡æ ‡ï¼ˆæŒ‰ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        if not requests:
            return {}

        # æ€»ä½“å¸¦å®½åˆ†æ
        overall_metrics = self.calculate_bandwidth_metrics(requests, operation_type=None)

        # è¯»æ“ä½œå¸¦å®½åˆ†æ
        read_metrics = self.calculate_bandwidth_metrics(requests, operation_type="read")

        # å†™æ“ä½œå¸¦å®½åˆ†æ
        write_metrics = self.calculate_bandwidth_metrics(requests, operation_type="write")

        # æ‰“å°å¸¦å®½åˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç½‘ç»œå¸¦å®½åˆ†æç»“æœæ‘˜è¦")
            print("=" * 60)
            print("ç½‘ç»œæ•´ä½“å¸¦å®½:")

        # æ˜¾ç¤ºå„ç±»å‹å¸¦å®½ï¼ˆæ€»å¸¦å®½å’ŒRN IPå¹³å‡å¸¦å®½ï¼‰
        if verbose:
            # åªè®¡ç®—RNï¼ˆDMAï¼‰IPçš„å¹³å‡å¸¦å®½
            rn_requests = [r for r in requests if hasattr(r, "source_type") and r.source_type.lower() in ["gdma", "dma"]]
            rn_read_requests = [r for r in rn_requests if r.req_type == "read"]
            rn_write_requests = [r for r in rn_requests if r.req_type == "write"]

            # ç»Ÿè®¡RN IPæ•°é‡ï¼ˆå»é‡ï¼‰
            rn_ips = set()
            for r in rn_requests:
                if hasattr(r, "source_ip"):
                    rn_ips.add(r.source_ip)
            rn_ip_count = len(rn_ips) if rn_ips else 1

            for label, metrics_data in [("è¯»å¸¦å®½", read_metrics), ("å†™å¸¦å®½", write_metrics), ("æ··åˆå¸¦å®½", overall_metrics), ("æ€»å¸¦å®½", overall_metrics)]:
                if metrics_data and isinstance(metrics_data, dict) and "åŠ æƒå¸¦å®½_GB/s" in metrics_data:
                    weighted_bw = metrics_data["åŠ æƒå¸¦å®½_GB/s"]
                    try:
                        total_bw = float(weighted_bw)
                        rn_avg_bw = total_bw / rn_ip_count if rn_ip_count > 0 else 0
                        print(f"  {label}: {total_bw:.3f} GB/s (æ€»), {rn_avg_bw:.6f} GB/s (RNå¹³å‡)")
                    except (ValueError, TypeError):
                        print(f"  {label}: {weighted_bw} GB/s")

        return {"æ€»ä½“å¸¦å®½": overall_metrics, "è¯»æ“ä½œå¸¦å®½": read_metrics, "å†™æ“ä½œå¸¦å®½": write_metrics}

    def analyze_latency(self, metrics, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æå»¶è¿ŸæŒ‡æ ‡"""
        if not metrics:
            return {}

        latencies = [m.transaction_latency for m in metrics]
        read_latencies = [m.transaction_latency for m in metrics if m.req_type == "read"]
        write_latencies = [m.transaction_latency for m in metrics if m.req_type == "write"]

        # CMDã€Dataã€Transactionå»¶è¿Ÿç»Ÿè®¡
        cmd_latencies = [m.cmd_latency for m in metrics]
        data_latencies = [m.data_latency for m in metrics]

        result = {
            "æ€»ä½“å»¶è¿Ÿ": {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(latencies):.2f}",
                "P95å»¶è¿Ÿ_ns": f"{np.percentile(latencies, 95):.2f}",
            }
        }

        if read_latencies:
            result["è¯»æ“ä½œå»¶è¿Ÿ"] = {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(read_latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(read_latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(read_latencies):.2f}",
            }

        if write_latencies:
            result["å†™æ“ä½œå»¶è¿Ÿ"] = {
                "å¹³å‡å»¶è¿Ÿ_ns": f"{np.mean(write_latencies):.2f}",
                "æœ€å°å»¶è¿Ÿ_ns": f"{np.min(write_latencies):.2f}",
                "æœ€å¤§å»¶è¿Ÿ_ns": f"{np.max(write_latencies):.2f}",
            }

        # æ‰“å°å»¶è¿Ÿåˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç½‘ç»œå»¶è¿Ÿåˆ†æç»“æœæ‘˜è¦")
            print("=" * 60)

            # æ€»ä½“å»¶è¿Ÿç»Ÿè®¡ï¼ˆåˆ†CMDã€Dataã€Transactionï¼‰
            print("æ€»ä½“å»¶è¿Ÿç»Ÿè®¡:")
            print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(cmd_latencies):.2f} ns, æœ€å° {np.min(cmd_latencies):.2f} ns, æœ€å¤§ {np.max(cmd_latencies):.2f} ns")
            print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(data_latencies):.2f} ns, æœ€å° {np.min(data_latencies):.2f} ns, æœ€å¤§ {np.max(data_latencies):.2f} ns")
            print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(latencies):.2f} ns, æœ€å° {np.min(latencies):.2f} ns, æœ€å¤§ {np.max(latencies):.2f} ns")
            print(f"  P95 Transactionå»¶è¿Ÿ: {np.percentile(latencies, 95):.2f} ns")

            # æŒ‰ç±»å‹åˆ†ç±»å»¶è¿Ÿç»Ÿè®¡
            if read_latencies:
                read_cmd = [m.cmd_latency for m in metrics if m.req_type == "read"]
                read_data = [m.data_latency for m in metrics if m.req_type == "read"]
                print(f"\nè¯»æ“ä½œå»¶è¿Ÿ:")
                print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(read_cmd):.2f} ns, æœ€å¤§ {np.max(read_cmd):.2f} ns")
                print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(read_data):.2f} ns, æœ€å¤§ {np.max(read_data):.2f} ns")
                print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(read_latencies):.2f} ns, æœ€å¤§ {np.max(read_latencies):.2f} ns")

            if write_latencies:
                write_cmd = [m.cmd_latency for m in metrics if m.req_type == "write"]
                write_data = [m.data_latency for m in metrics if m.req_type == "write"]
                print(f"\nå†™æ“ä½œå»¶è¿Ÿ:")
                print(f"  CMDå»¶è¿Ÿ: å¹³å‡ {np.mean(write_cmd):.2f} ns, æœ€å¤§ {np.max(write_cmd):.2f} ns")
                print(f"  Dataå»¶è¿Ÿ: å¹³å‡ {np.mean(write_data):.2f} ns, æœ€å¤§ {np.max(write_data):.2f} ns")
                print(f"  Transactionå»¶è¿Ÿ: å¹³å‡ {np.mean(write_latencies):.2f} ns, æœ€å¤§ {np.max(write_latencies):.2f} ns")

        return result

    def analyze_port_bandwidth(self, metrics, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æç«¯å£çº§åˆ«å¸¦å®½ï¼ˆæŒ‰IPç±»å‹åˆ†ç»„ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å·¥ä½œåŒºé—´ç®—æ³•ï¼‰"""
        ip_analysis = defaultdict(lambda: {"read": [], "write": []})

        for metric in metrics:
            # æŒ‰æºIPç±»å‹åˆ†ç»„ï¼ˆè°å‘èµ·çš„è¯·æ±‚ï¼‰
            source_ip_type = metric.source_type  # 'gdma' æˆ– 'ddr'
            ip_analysis[source_ip_type.upper()][metric.req_type].append(metric)

        ip_summary = {}
        for ip_type, data in ip_analysis.items():
            read_reqs = data["read"]
            write_reqs = data["write"]

            # ä½¿ç”¨ç»Ÿä¸€çš„å·¥ä½œåŒºé—´ç®—æ³•è®¡ç®—è¯»å†™å¸¦å®½
            read_metrics = self.calculate_bandwidth_metrics(read_reqs, operation_type="read", endpoint_type="network")
            write_metrics = self.calculate_bandwidth_metrics(write_reqs, operation_type="write", endpoint_type="network")

            # æå–å¸¦å®½æ•°å€¼ï¼ˆå»é™¤GB/såç¼€å¹¶è½¬æ¢ä¸ºfloatï¼‰
            read_bw = float(read_metrics.get("éåŠ æƒå¸¦å®½_GB/s", "0.00"))
            write_bw = float(write_metrics.get("éåŠ æƒå¸¦å®½_GB/s", "0.00"))

            ip_summary[ip_type] = {
                "è¯»å¸¦å®½_GB/s": f"{read_bw:.2f}",
                "å†™å¸¦å®½_GB/s": f"{write_bw:.2f}",
                "æ€»å¸¦å®½_GB/s": f"{read_bw + write_bw:.2f}",
                "è¯»è¯·æ±‚æ•°": len(read_reqs),
                "å†™è¯·æ±‚æ•°": len(write_reqs),
                "æ€»è¯·æ±‚æ•°": len(read_reqs) + len(write_reqs),
            }

        # ç«¯å£ç»Ÿè®¡ä¸éœ€è¦æ‘˜è¦è¾“å‡ºï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ç§»é™¤ï¼‰

        return ip_summary

    def analyze_tag_data(self, model, verbose: bool = True) -> Dict[str, Any]:
        """åˆ†æTagæœºåˆ¶æ•°æ®ï¼ˆæŒ‰ç…§CrossRingè§„æ ¼è¦æ±‚çš„æ ¼å¼ï¼‰"""
        tag_analysis = {
            "Circuitsç»Ÿè®¡": {"req_h": 0, "req_v": 0, "rsp_h": 0, "rsp_v": 0, "data_h": 0, "data_v": 0},
            "Wait_cycleç»Ÿè®¡": {"req_h": 0, "req_v": 0, "rsp_h": 0, "rsp_v": 0, "data_h": 0, "data_v": 0},
            "RB_ETagç»Ÿè®¡": {"T1": 0, "T0": 0},
            "EQ_ETagç»Ÿè®¡": {"T1": 0, "T0": 0},
            "ITagç»Ÿè®¡": {"h": 0, "v": 0},
            "Retryç»Ÿè®¡": {"read": 0, "write": 0},
        }

        # ä»NoCèŠ‚ç‚¹ä¸­æ”¶é›†ç»Ÿè®¡æ•°æ®
        try:
            for node in model.nodes.values():
                # æ”¶é›†æ¨ªå‘ç¯ç»Ÿè®¡æ•°æ®
                if hasattr(node, "horizontal_crosspoint"):
                    hcp = node.horizontal_crosspoint

                    # Circuitsç»Ÿè®¡
                    tag_analysis["Circuitsç»Ÿè®¡"]["req_h"] += getattr(hcp, "circuit_req_count", 0)
                    tag_analysis["Circuitsç»Ÿè®¡"]["rsp_h"] += getattr(hcp, "circuit_rsp_count", 0)
                    tag_analysis["Circuitsç»Ÿè®¡"]["data_h"] += getattr(hcp, "circuit_data_count", 0)

                    # Wait cycleç»Ÿè®¡
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["req_h"] += getattr(hcp, "wait_req_cycles", 0)
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_h"] += getattr(hcp, "wait_rsp_cycles", 0)
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["data_h"] += getattr(hcp, "wait_data_cycles", 0)

                    # I-Tagç»Ÿè®¡
                    tag_analysis["ITagç»Ÿè®¡"]["h"] += getattr(hcp, "itag_trigger_count", 0)

                # æ”¶é›†çºµå‘ç¯ç»Ÿè®¡æ•°æ®
                if hasattr(node, "vertical_crosspoint"):
                    vcp = node.vertical_crosspoint

                    # Circuitsç»Ÿè®¡
                    tag_analysis["Circuitsç»Ÿè®¡"]["req_v"] += getattr(vcp, "circuit_req_count", 0)
                    tag_analysis["Circuitsç»Ÿè®¡"]["rsp_v"] += getattr(vcp, "circuit_rsp_count", 0)
                    tag_analysis["Circuitsç»Ÿè®¡"]["data_v"] += getattr(vcp, "circuit_data_count", 0)

                    # Wait cycleç»Ÿè®¡
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["req_v"] += getattr(vcp, "wait_req_cycles", 0)
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["rsp_v"] += getattr(vcp, "wait_rsp_cycles", 0)
                    tag_analysis["Wait_cycleç»Ÿè®¡"]["data_v"] += getattr(vcp, "wait_data_cycles", 0)

                    # I-Tagç»Ÿè®¡
                    tag_analysis["ITagç»Ÿè®¡"]["v"] += getattr(vcp, "itag_trigger_count", 0)

                # æ”¶é›†Ring Bridge E-Tagç»Ÿè®¡
                if hasattr(node, "ring_bridge"):
                    rb = node.ring_bridge
                    tag_analysis["RB_ETagç»Ÿè®¡"]["T1"] += getattr(rb, "etag_t1_count", 0)
                    tag_analysis["RB_ETagç»Ÿè®¡"]["T0"] += getattr(rb, "etag_t0_count", 0)

                # æ”¶é›†Eject Queue E-Tagç»Ÿè®¡
                if hasattr(node, "eject_queue"):
                    eq = node.eject_queue
                    tag_analysis["EQ_ETagç»Ÿè®¡"]["T1"] += getattr(eq, "etag_t1_count", 0)
                    tag_analysis["EQ_ETagç»Ÿè®¡"]["T0"] += getattr(eq, "etag_t0_count", 0)

                # æ”¶é›†Retryç»Ÿè®¡
                if hasattr(node, "ip_interfaces"):
                    for ip_interface in node.ip_interfaces.values():
                        tag_analysis["Retryç»Ÿè®¡"]["read"] += getattr(ip_interface, "retry_read_count", 0)
                        tag_analysis["Retryç»Ÿè®¡"]["write"] += getattr(ip_interface, "retry_write_count", 0)

        except Exception as e:
            self.logger.warning(f"æ”¶é›†Tagå’Œç»•ç¯æ•°æ®æ—¶å‡ºé”™: {e}")

        # æ‰“å°Tagåˆ†æç»“æœï¼ˆä»…åœ¨verbose=Trueæ—¶ï¼‰
        if verbose:
            print("\n" + "=" * 60)
            print("ç»•ç¯ä¸Tagç»Ÿè®¡")
            print("=" * 60)

            circuits = tag_analysis["Circuitsç»Ÿè®¡"]
            print(f"  è¯·æ±‚ç»•ç¯  - æ¨ªå‘: {circuits['req_h']}, çºµå‘: {circuits['req_v']}")
            print(f"  å“åº”ç»•ç¯  - æ¨ªå‘: {circuits['rsp_h']}, çºµå‘: {circuits['rsp_v']}")
            print(f"  æ•°æ®ç»•ç¯  - æ¨ªå‘: {circuits['data_h']}, çºµå‘: {circuits['data_v']}")

            wait_cycles = tag_analysis["Wait_cycleç»Ÿè®¡"]
            print(f"  è¯·æ±‚ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['req_h']}, çºµå‘: {wait_cycles['req_v']}")
            print(f"  å“åº”ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['rsp_h']}, çºµå‘: {wait_cycles['rsp_v']}")
            print(f"  æ•°æ®ç­‰å¾…æ—¶é—´  - æ¨ªå‘: {wait_cycles['data_h']}, çºµå‘: {wait_cycles['data_v']}")

            rb_etag = tag_analysis["RB_ETagç»Ÿè®¡"]
            print(f"  RB ETagç»Ÿè®¡ - T1: {rb_etag['T1']}, T0: {rb_etag['T0']}")

            eq_etag = tag_analysis["EQ_ETagç»Ÿè®¡"]
            print(f"  EQ ETagç»Ÿè®¡ - T1: {eq_etag['T1']}, T0: {eq_etag['T0']}")

            itag = tag_analysis["ITagç»Ÿè®¡"]
            print(f"  æ³¨å…¥æ ‡ç­¾ - æ¨ªå‘: {itag['h']}, çºµå‘: {itag['v']}")

            retry = tag_analysis["Retryç»Ÿè®¡"]
            print(f"  Retryæ•°é‡ - è¯»: {retry['read']}, å†™: {retry['write']}")

        return tag_analysis

    def _collect_bandwidth_time_series_data(self, metrics):
        """æ”¶é›†å¸¦å®½æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä»¿ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰"""
        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        self.bandwidth_time_series.clear()

        # æŒ‰ç«¯å£ç±»å‹åˆ†ç»„è¯·æ±‚
        for req in metrics:
            # ç”Ÿæˆç«¯å£é”®åï¼ˆç±»ä¼¼è€ç‰ˆæœ¬çš„æ ¼å¼ï¼‰
            if hasattr(req, "source_type") and hasattr(req, "dest_type"):
                if req.req_type == "read":
                    port_key = f"{req.source_type} read {req.dest_type}"
                else:
                    port_key = f"{req.source_type} write {req.dest_type}"
            else:
                # å¦‚æœæ²¡æœ‰ç«¯å£ç±»å‹ä¿¡æ¯ï¼Œä½¿ç”¨è¯»å†™ç±»å‹
                port_key = f"{req.req_type}"

            # æ·»åŠ åˆ°æ—¶é—´åºåˆ—æ•°æ®
            self.bandwidth_time_series[port_key]["time"].append(req.end_time)
            self.bandwidth_time_series[port_key]["start_times"].append(req.start_time)
            self.bandwidth_time_series[port_key]["bytes"].append(req.total_bytes)

    def plot_bandwidth_curves(self, metrics, save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”Ÿæˆå¸¦å®½æ—¶é—´æ›²çº¿å›¾ï¼ˆä½¿ç”¨ç´¯ç§¯å¸¦å®½ç®—æ³•ï¼Œä»¿ç…§è€ç‰ˆæœ¬ï¼‰"""
        if not metrics:
            return ""

        try:
            # æŒ‰ç«¯å£ç±»å‹åˆ†ç»„æ•°æ®ï¼ˆä»¿ç…§è€ç‰ˆæœ¬çš„rn_bandwidth_time_seriesï¼‰
            port_time_series = defaultdict(lambda: {"time": [], "start_times": [], "bytes": []})

            for metric in metrics:
                # æ„é€ ç«¯å£æ ‡è¯†ï¼šæ ¼å¼ä¸º "SOURCE_TYPE REQUEST_TYPE DEST_TYPE"ï¼Œä¾‹å¦‚ "GDMA READ DDR"
                port_key = f"{metric.source_type.upper()} {metric.req_type.upper()} {metric.dest_type.upper()}"

                port_time_series[port_key]["time"].append(metric.end_time)
                port_time_series[port_key]["start_times"].append(metric.start_time)
                port_time_series[port_key]["bytes"].append(metric.total_bytes)

            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(12, 8))

            # ç»˜åˆ¶ç´¯ç§¯å¸¦å®½æ›²çº¿
            total_final_bw = 0

            for port_key, data_dict in port_time_series.items():
                if not data_dict["time"]:
                    continue

                # æ’åºæ—¶é—´æˆ³å¹¶å»é™¤nanå€¼ï¼ˆä»¿ç…§è€ç‰ˆæœ¬é€»è¾‘ï¼‰
                raw_end = np.array(data_dict["time"])
                raw_start = np.array(data_dict["start_times"])
                raw_bytes = np.array(data_dict["bytes"])

                # å»é™¤nanå€¼å’Œæ— æ•ˆæ•°æ®
                mask = ~np.isnan(raw_end) & (raw_end > 0)
                end_clean = raw_end[mask]
                start_clean = raw_start[mask]
                bytes_clean = raw_bytes[mask]

                if len(end_clean) == 0:
                    continue

                # åŒæ­¥æ’åº
                sort_idx = np.argsort(end_clean)
                times = end_clean[sort_idx]
                start_times = start_clean[sort_idx]
                bytes_data = bytes_clean[sort_idx]

                # ä»¿ç…§è€ç‰ˆæœ¬ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªè¯·æ±‚çš„å¼€å§‹æ—¶é—´ä½œä¸ºåŸºå‡†
                if len(start_times) > 0:
                    base_start = start_times[0]
                    rel_times = times - base_start

                    # é˜²æ­¢é™¤ä»¥0
                    rel_times[rel_times <= 0] = 1e-9

                    # è®¡ç®—ç´¯ç§¯è¯·æ±‚æ•°å’Œç´¯ç§¯å¸¦å®½
                    cum_counts = np.arange(1, len(rel_times) + 1)

                    # ä½¿ç”¨ç»Ÿä¸€å…¬å¼ï¼šç´¯ç§¯å­—èŠ‚æ•° / æ—¶é—´ = GB/sï¼ˆç›´æ¥ç»“æœï¼‰
                    cum_bytes = np.cumsum(bytes_data)
                    bandwidth_gbps = cum_bytes / rel_times  # ç›´æ¥å¾—åˆ°GB/s

                    # ç»˜åˆ¶æ›²çº¿ï¼ˆä½¿ç”¨ç»å¯¹æ—¶é—´è½´ï¼‰
                    time_us = times / 1000  # è½¬æ¢ä¸ºå¾®ç§’
                    (line,) = ax.plot(time_us, bandwidth_gbps, drawstyle="default", label=port_key, linewidth=2)

                    # åœ¨æ›²çº¿æœ«å°¾æ·»åŠ æ•°å€¼æ ‡æ³¨
                    if len(bandwidth_gbps) > 0:
                        final_bw = bandwidth_gbps[-1]
                        ax.text(
                            time_us[-1],
                            final_bw,
                            f"{final_bw:.2f}",
                            va="center",
                            color=line.get_color(),
                            fontsize=10,
                            bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.8),
                        )
                        total_final_bw += final_bw

            # è®¾ç½®å›¾è¡¨å±æ€§
            ax.set_xlabel("æ—¶é—´ (Î¼s)", fontsize=12)
            ax.set_ylabel("å¸¦å®½ (GB/s)", fontsize=12)
            ax.set_title("CrossRing NoC ç´¯ç§¯å¸¦å®½æ—¶é—´æ›²çº¿", fontsize=14)
            ax.legend(fontsize=10, prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 10})
            ax.grid(True, alpha=0.3)
            ax.set_ylim(bottom=0)

            # æ·»åŠ æ€»å¸¦å®½ä¿¡æ¯
            if total_final_bw > 0:
                ax.text(
                    0.02,
                    0.98,
                    f"æ€»å¸¦å®½: {total_final_bw:.2f} GB/s",
                    transform=ax.transAxes,
                    fontsize=12,
                    va="top",
                    ha="left",
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8),
                )

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_bandwidth_curve_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=100)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ ç´¯ç§¯å¸¦å®½æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
                self.logger.info(f"ç´¯ç§¯å¸¦å®½æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {save_path}")
                self.logger.info(f"æ€»å¸¦å®½: {total_final_bw:.2f} GB/s")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºç´¯ç§¯å¸¦å®½æ›²çº¿å›¾")
                try:
                    plt.show()  # ä½¿ç”¨é»˜è®¤çš„block=Trueï¼Œä¿æŒçª—å£æ‰“å¼€
                except Exception as e:
                    if verbose:
                        print(f"   æ— æ³•æ˜¾ç¤ºå›¾è¡¨: {e}")
                        print(f"   å»ºè®®åœ¨æœ‰GUIçš„ç¯å¢ƒä¸­è¿è¡Œæˆ–è®¾ç½®save_figures=Trueä¿å­˜åˆ°æ–‡ä»¶")
                self.logger.info(f"æ˜¾ç¤ºç´¯ç§¯å¸¦å®½æ›²çº¿å›¾")
                self.logger.info(f"æ€»å¸¦å®½: {total_final_bw:.2f} GB/s")
                return ""

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¸¦å®½æ›²çº¿å›¾å¤±è´¥: {e}")
            import traceback

            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return ""

    def save_detailed_requests_csv(self, metrics, save_dir: str = "output") -> Dict[str, str]:
        """ä¿å­˜è¯¦ç»†è¯·æ±‚CSVæ–‡ä»¶ï¼ˆä»¿ç…§è€ç‰ˆæœ¬æ ¼å¼ï¼‰

        Returns:
            åŒ…å«ä¿å­˜æ–‡ä»¶è·¯å¾„çš„å­—å…¸: {"read_requests_csv": path, "write_requests_csv": path}
        """
        if not metrics:
            return {}

        try:
            import csv

            os.makedirs(save_dir, exist_ok=True)

            # CSVæ–‡ä»¶å¤´ï¼ˆä¸è€ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰
            csv_header = [
                "packet_id",
                "start_time_ns",
                "end_time_ns",
                "source_node",
                "source_type",
                "dest_node",
                "dest_type",
                "burst_length",
                "cmd_latency_ns",
                "data_latency_ns",
                "transaction_latency_ns",
            ]

            # åˆ†ç¦»è¯»å†™è¯·æ±‚
            read_requests = [req for req in metrics if req.req_type == "read"]
            write_requests = [req for req in metrics if req.req_type == "write"]

            saved_files = {}

            # ä¿å­˜è¯»è¯·æ±‚CSV
            if read_requests:
                timestamp = int(time.time())
                read_csv_path = f"{save_dir}/read_requests_{timestamp}.csv"

                with open(read_csv_path, "w", newline="", encoding="utf-8") as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(csv_header)

                    for req in read_requests:
                        row = [
                            req.packet_id,
                            req.start_time,
                            req.end_time,
                            req.source_node,
                            getattr(req, "source_type", "unknown"),
                            req.dest_node,
                            getattr(req, "dest_type", "unknown"),
                            req.burst_length,
                            req.cmd_latency,
                            req.data_latency,
                            req.transaction_latency,
                        ]
                        writer.writerow(row)

                saved_files["read_requests_csv"] = read_csv_path
                self.logger.info(f"è¯»è¯·æ±‚CSVå·²ä¿å­˜: {read_csv_path} ({len(read_requests)} æ¡è®°å½•)")

            # ä¿å­˜å†™è¯·æ±‚CSV
            if write_requests:
                timestamp = int(time.time())
                write_csv_path = f"{save_dir}/write_requests_{timestamp}.csv"

                with open(write_csv_path, "w", newline="", encoding="utf-8") as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(csv_header)

                    for req in write_requests:
                        row = [
                            req.packet_id,
                            req.start_time,
                            req.end_time,
                            req.source_node,
                            getattr(req, "source_type", "unknown"),
                            req.dest_node,
                            getattr(req, "dest_type", "unknown"),
                            req.burst_length,
                            req.cmd_latency,
                            req.data_latency,
                            req.transaction_latency,
                        ]
                        writer.writerow(row)

                saved_files["write_requests_csv"] = write_csv_path
                self.logger.info(f"å†™è¯·æ±‚CSVå·²ä¿å­˜: {write_csv_path} ({len(write_requests)} æ¡è®°å½•)")

            return saved_files

        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯¦ç»†è¯·æ±‚CSVå¤±è´¥: {e}")
            import traceback

            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {}

    def save_ports_bandwidth_csv(self, metrics, save_dir: str = "output", config=None) -> str:
        """ä¿å­˜ç«¯å£å¸¦å®½CSVæ–‡ä»¶ï¼ˆä»¿ç…§è€ç‰ˆæœ¬æ ¼å¼ï¼‰

        Returns:
            ä¿å­˜çš„CSVæ–‡ä»¶è·¯å¾„
        """
        if not metrics:
            return ""

        try:
            import csv

            os.makedirs(save_dir, exist_ok=True)

            # CSVæ–‡ä»¶å¤´ï¼ˆä¸è€ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰
            csv_header = [
                "port_id",
                "coordinate",
                "read_unweighted_bandwidth_gbps",
                "read_weighted_bandwidth_gbps",
                "write_unweighted_bandwidth_gbps",
                "write_weighted_bandwidth_gbps",
                "mixed_unweighted_bandwidth_gbps",
                "mixed_weighted_bandwidth_gbps",
                "read_requests_count",
                "write_requests_count",
                "total_requests_count",
                "read_flits_count",
                "write_flits_count",
                "total_flits_count",
                "read_working_intervals_count",
                "write_working_intervals_count",
                "mixed_working_intervals_count",
                "read_total_working_time_ns",
                "write_total_working_time_ns",
                "mixed_total_working_time_ns",
                "read_network_start_time_ns",
                "read_network_end_time_ns",
                "write_network_start_time_ns",
                "write_network_end_time_ns",
                "mixed_network_start_time_ns",
                "mixed_network_end_time_ns",
            ]

            # æŒ‰ç«¯å£åˆ†ç»„ç»Ÿè®¡ - ä½¿ç”¨å…·ä½“IPåç§°å’Œåæ ‡
            port_stats = {}

            # ä»configè·å–ç½‘æ ¼å°ºå¯¸
            num_cols = getattr(config, "NUM_COL", 3) if config else 3  # é»˜è®¤3åˆ—

            for req in metrics:
                # ç»Ÿè®¡æ‰€æœ‰æ¶‰åŠçš„ç«¯å£ï¼šRNç«¯å£ï¼ˆè¯»è¯·æ±‚æºï¼‰å’ŒSNç«¯å£ï¼ˆå†™è¯·æ±‚ç›®æ ‡ï¼‰
                ports_to_update = []

                # å¯¹äºæ¯ä¸ªè¯·æ±‚ï¼Œéƒ½è¦ç»Ÿè®¡RNå’ŒSNä¸¤ä¸ªç«¯å£
                # RNç«¯å£ï¼šè¯·æ±‚å‘èµ·è€…ï¼ˆè¯»/å†™è¯·æ±‚çš„æºï¼‰
                source_port_id = req.source_type  # å¦‚ "gdma_0"
                source_node_id = req.source_node
                source_row = source_node_id // num_cols
                source_col = source_node_id % num_cols
                source_coordinate = f"x_{source_col}_y_{source_row}"
                ports_to_update.append((source_port_id, source_node_id, source_coordinate))

                # SNç«¯å£ï¼šè¯·æ±‚æ¥æ”¶è€…ï¼ˆè¯»/å†™è¯·æ±‚çš„ç›®æ ‡ï¼‰
                dest_port_id = req.dest_type  # å¦‚ "ddr_0"
                dest_node_id = req.dest_node
                dest_row = dest_node_id // num_cols
                dest_col = dest_node_id % num_cols
                dest_coordinate = f"x_{dest_col}_y_{dest_row}"
                ports_to_update.append((dest_port_id, dest_node_id, dest_coordinate))

                # æ›´æ–°æ‰€æœ‰ç›¸å…³ç«¯å£çš„ç»Ÿè®¡
                for port_id, node_id, coordinate in ports_to_update:
                    if port_id not in port_stats:
                        port_stats[port_id] = {"coordinate": coordinate, "node_id": node_id, "read_requests": [], "write_requests": [], "all_requests": []}

                    port_stats[port_id]["all_requests"].append(req)
                    if req.req_type == "read":
                        port_stats[port_id]["read_requests"].append(req)
                    else:
                        port_stats[port_id]["write_requests"].append(req)

            # ç”ŸæˆCSVæ–‡ä»¶
            timestamp = int(time.time())
            csv_path = f"{save_dir}/ports_bandwidth_{timestamp}.csv"

            with open(csv_path, "w", newline="", encoding="utf-8") as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(csv_header)

                for port_id, stats in port_stats.items():
                    # è®¡ç®—å„ç§å¸¦å®½æŒ‡æ ‡
                    read_reqs = stats["read_requests"]
                    write_reqs = stats["write_requests"]
                    all_reqs = stats["all_requests"]

                    # å¸¦å®½è®¡ç®— - ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—ï¼Œä¸calculate_bandwidth_metricså®Œå…¨ä¸€è‡´
                    def calc_bandwidth_metrics(requests):
                        if not requests:
                            return {"unweighted_bw": 0.0, "weighted_bw": 0.0, "start_time": 0, "end_time": 0, "total_time": 0, "working_intervals": 0, "flits_count": 0}

                        # è®¡ç®—å·¥ä½œåŒºé—´ï¼ˆä¸calculate_bandwidth_metricsç›¸åŒçš„é€»è¾‘ï¼‰
                        working_intervals = self.calculate_working_intervals(requests, min_gap_threshold=200)

                        # ç½‘ç»œå·¥ä½œæ—¶é—´çª—å£
                        network_start = min(r.start_time for r in requests)
                        network_end = max(r.end_time for r in requests)
                        total_network_time = network_end - network_start

                        # æ€»å·¥ä½œæ—¶é—´å’Œæ€»å­—èŠ‚æ•°
                        total_working_time = sum(interval.duration for interval in working_intervals)
                        total_bytes = sum(r.total_bytes for r in requests)

                        # è®¡ç®—éåŠ æƒå¸¦å®½ï¼šæ€»æ•°æ®é‡ / ç½‘ç»œæ€»æ—¶é—´
                        unweighted_bandwidth = (total_bytes / total_network_time) if total_network_time > 0 else 0.0

                        # è®¡ç®—åŠ æƒå¸¦å®½ï¼šå„åŒºé—´å¸¦å®½æŒ‰flitæ•°é‡åŠ æƒå¹³å‡
                        if working_intervals:
                            total_weighted_bandwidth = 0.0
                            total_weight = 0

                            for interval in working_intervals:
                                weight = interval.flit_count  # æƒé‡æ˜¯å·¥ä½œæ—¶é—´æ®µçš„flitæ•°é‡
                                bandwidth = interval.bandwidth  # GB/s
                                total_weighted_bandwidth += bandwidth * weight
                                total_weight += weight

                            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                        else:
                            weighted_bandwidth = 0.0

                        return {
                            "unweighted_bw": unweighted_bandwidth,
                            "weighted_bw": weighted_bandwidth,
                            "start_time": network_start,
                            "end_time": network_end,
                            "total_time": total_network_time,
                            "working_intervals": len(working_intervals),
                            "flits_count": sum(r.burst_length for r in requests),
                        }

                    read_metrics = calc_bandwidth_metrics(read_reqs)
                    write_metrics = calc_bandwidth_metrics(write_reqs)
                    mixed_metrics = calc_bandwidth_metrics(all_reqs)

                    row_data = [
                        port_id,
                        stats["coordinate"],
                        read_metrics["unweighted_bw"],
                        read_metrics["weighted_bw"],
                        write_metrics["unweighted_bw"],
                        write_metrics["weighted_bw"],
                        mixed_metrics["unweighted_bw"],
                        mixed_metrics["weighted_bw"],
                        len(read_reqs),
                        len(write_reqs),
                        len(all_reqs),
                        read_metrics["flits_count"],
                        write_metrics["flits_count"],
                        mixed_metrics["flits_count"],
                        read_metrics["working_intervals"],
                        write_metrics["working_intervals"],
                        mixed_metrics["working_intervals"],
                        read_metrics["total_time"],
                        write_metrics["total_time"],
                        mixed_metrics["total_time"],
                        read_metrics["start_time"],
                        read_metrics["end_time"],
                        write_metrics["start_time"],
                        write_metrics["end_time"],
                        mixed_metrics["start_time"],
                        mixed_metrics["end_time"],
                    ]
                    writer.writerow(row_data)

                # è®¡ç®—IPç±»å‹æ±‡æ€»ç»Ÿè®¡
                ip_type_aggregates = {}
                for port_id, stats in port_stats.items():
                    # æå–IPç±»å‹ï¼ˆå»æ‰æ•°å­—åç¼€ï¼‰
                    ip_type = port_id.split("_")[0]  # "gdma_0" -> "gdma"

                    if ip_type not in ip_type_aggregates:
                        ip_type_aggregates[ip_type] = {
                            "ports": [],
                            "total_read_requests": 0,
                            "total_write_requests": 0,
                            "total_requests": 0,
                            "total_read_flits": 0,
                            "total_write_flits": 0,
                            "total_flits": 0,
                            "read_bandwidth_sum": 0,
                            "write_bandwidth_sum": 0,
                            "mixed_bandwidth_sum": 0,
                        }

                    # è®¡ç®—è¯¥ç«¯å£çš„æŒ‡æ ‡
                    read_reqs = stats["read_requests"]
                    write_reqs = stats["write_requests"]
                    all_reqs = stats["all_requests"]

                    read_metrics = calc_bandwidth_metrics(read_reqs)
                    write_metrics = calc_bandwidth_metrics(write_reqs)
                    mixed_metrics = calc_bandwidth_metrics(all_reqs)

                    # ç´¯åŠ åˆ°IPç±»å‹ç»Ÿè®¡
                    agg = ip_type_aggregates[ip_type]
                    agg["ports"].append(port_id)
                    agg["total_read_requests"] += len(read_reqs)
                    agg["total_write_requests"] += len(write_reqs)
                    agg["total_requests"] += len(all_reqs)
                    agg["total_read_flits"] += read_metrics["flits_count"]
                    agg["total_write_flits"] += write_metrics["flits_count"]
                    agg["total_flits"] += mixed_metrics["flits_count"]
                    agg["read_bandwidth_sum"] += read_metrics["unweighted_bw"]
                    agg["write_bandwidth_sum"] += write_metrics["unweighted_bw"]
                    agg["mixed_bandwidth_sum"] += mixed_metrics["unweighted_bw"]

                # æ·»åŠ IPç±»å‹æ±‡æ€»è¡Œ
                writer.writerow([])  # ç©ºè¡Œåˆ†éš”
                writer.writerow(["=== IPç±»å‹æ±‡æ€»ç»Ÿè®¡ ==="])

                for ip_type, agg in sorted(ip_type_aggregates.items()):
                    port_count = len(agg["ports"])
                    avg_read_bw = agg["read_bandwidth_sum"] / port_count if port_count > 0 else 0
                    avg_write_bw = agg["write_bandwidth_sum"] / port_count if port_count > 0 else 0
                    avg_mixed_bw = agg["mixed_bandwidth_sum"] / port_count if port_count > 0 else 0

                    summary_row = [
                        f"{ip_type}_AVG",  # port_idæ ¼å¼ï¼šgdma_AVG, ddr_AVG
                        f"avg_of_{port_count}_ports",  # coordinateæ˜¾ç¤ºç«¯å£æ•°
                        avg_read_bw,  # å¹³å‡è¯»å¸¦å®½
                        avg_read_bw,  # å¹³å‡è¯»å¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        avg_write_bw,  # å¹³å‡å†™å¸¦å®½
                        avg_write_bw,  # å¹³å‡å†™å¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        avg_mixed_bw,  # å¹³å‡æ··åˆå¸¦å®½
                        avg_mixed_bw,  # å¹³å‡æ··åˆå¸¦å®½ï¼ˆåŠ æƒï¼Œç®€åŒ–ä¸ºç›¸åŒï¼‰
                        agg["total_read_requests"],  # æ€»è¯»è¯·æ±‚æ•°
                        agg["total_write_requests"],  # æ€»å†™è¯·æ±‚æ•°
                        agg["total_requests"],  # æ€»è¯·æ±‚æ•°
                        agg["total_read_flits"],  # æ€»è¯»flitæ•°
                        agg["total_write_flits"],  # æ€»å†™flitæ•°
                        agg["total_flits"],  # æ€»flitæ•°
                        port_count,  # å·¥ä½œåŒºé—´æ•°ç”¨ç«¯å£æ•°è¡¨ç¤º
                        port_count,
                        port_count,
                        0,
                        0,
                        0,  # æ—¶é—´ç›¸å…³å­—æ®µä¸º0ï¼ˆæ±‡æ€»æ•°æ®ï¼‰
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                    ]
                    writer.writerow(summary_row)

            self.logger.info(f"ç«¯å£å¸¦å®½CSVå·²ä¿å­˜: {csv_path} ({len(port_stats)} ä¸ªç«¯å£)")
            return csv_path

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç«¯å£å¸¦å®½CSVå¤±è´¥: {e}")
            import traceback

            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return ""

    def plot_latency_distribution(self, metrics, save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”Ÿæˆå»¶è¿Ÿåˆ†å¸ƒå›¾"""
        if not metrics:
            return ""

        try:
            # ä¸‰ç§å»¶è¿Ÿç±»å‹æ•°æ®
            cmd_latencies = [m.cmd_latency for m in metrics]
            data_latencies = [m.data_latency for m in metrics]
            transaction_latencies = [m.transaction_latency for m in metrics]

            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å»¶è¿Ÿæ•°æ®çš„åˆ†å¸ƒ
            cmd_zero_count = sum(1 for x in cmd_latencies if x == 0)
            data_zero_count = sum(1 for x in data_latencies if x == 0)
            trans_zero_count = sum(1 for x in transaction_latencies if x == 0)

            if cmd_zero_count > 0:
                self.logger.warning(f"CMDå»¶è¿Ÿä¸­æœ‰{cmd_zero_count}ä¸ªå€¼ä¸º0ï¼ˆå¯èƒ½æ˜¯ç”±äºæ—¶é—´æˆ³ç¼ºå¤±ï¼‰")
            if data_zero_count > 0:
                self.logger.info(f"DATAå»¶è¿Ÿä¸­æœ‰{data_zero_count}ä¸ªå€¼ä¸º0")
            if trans_zero_count > 0:
                self.logger.info(f"TRANSACTIONå»¶è¿Ÿä¸­æœ‰{trans_zero_count}ä¸ªå€¼ä¸º0")

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # 1. ä¸‰ç§å»¶è¿Ÿç±»å‹å¯¹æ¯”ç›´æ–¹å›¾ï¼ˆä½¿ç”¨å¯¹æ•°åæ ‡è½´ï¼‰
            # ä½¿ç”¨ç»Ÿä¸€çš„çº¿å®½å’Œé€æ˜åº¦
            ax1.hist(cmd_latencies, bins=30, alpha=0.6, label="CMDå»¶è¿Ÿ", color="blue", linewidth=1.5)
            ax1.hist(data_latencies, bins=30, alpha=0.6, label="DATAå»¶è¿Ÿ", color="green", linewidth=1.5)
            ax1.hist(transaction_latencies, bins=30, alpha=0.6, label="TRANSACTIONå»¶è¿Ÿ", color="red", linewidth=1.5)
            ax1.set_xlabel("å»¶è¿Ÿ (ns)")
            ax1.set_ylabel("é¢‘æ¬¡")
            ax1.set_title("ä¸‰ç§å»¶è¿Ÿç±»å‹åˆ†å¸ƒç›´æ–¹å›¾")
            ax1.legend(prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 9})
            ax1.grid(True, alpha=0.3)
            # è®¾ç½®Xè½´ä¸ºå¯¹æ•°åæ ‡ï¼Œå¹¶è°ƒæ•´åˆ»åº¦
            ax1.set_xscale('log')
            # è®¾ç½®æ›´å¯†é›†çš„ä¸»è¦åˆ»åº¦å’Œæ¬¡è¦åˆ»åº¦
            from matplotlib.ticker import LogLocator, LogFormatter
            ax1.xaxis.set_major_locator(LogLocator(base=10, numticks=8))
            ax1.xaxis.set_minor_locator(LogLocator(base=10, subs=np.arange(2, 10) * 0.1, numticks=100))
            ax1.xaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))

            # 2. å»¶è¿Ÿç±»å‹ç®±çº¿å›¾ï¼ˆä½¿ç”¨å¯¹æ•°åæ ‡è½´ï¼‰
            latency_data = [cmd_latencies, data_latencies, transaction_latencies]
            latency_labels = ["CMDå»¶è¿Ÿ", "DATAå»¶è¿Ÿ", "TRANSACTIONå»¶è¿Ÿ"]
            ax2.boxplot(latency_data, labels=latency_labels)
            ax2.set_ylabel("å»¶è¿Ÿ (ns)")
            ax2.set_title("å»¶è¿Ÿç±»å‹ç®±çº¿å›¾")
            ax2.grid(True, alpha=0.3)
            # è®¾ç½®Yè½´ä¸ºå¯¹æ•°åæ ‡ï¼Œå¹¶è°ƒæ•´åˆ»åº¦
            ax2.set_yscale('log')
            # è®¾ç½®æ›´å¯†é›†çš„ä¸»è¦åˆ»åº¦å’Œæ¬¡è¦åˆ»åº¦
            ax2.yaxis.set_major_locator(LogLocator(base=10, numticks=8))
            ax2.yaxis.set_minor_locator(LogLocator(base=10, subs=np.arange(2, 10) * 0.1, numticks=100))
            ax2.yaxis.set_major_formatter(LogFormatter(base=10, labelOnlyBase=False))

            # åˆ é™¤è¯»å†™åˆ†ç±»çš„å›¾è¡¨ï¼Œåªä¿ç•™ä¸¤ä¸ªæ€»ä½“ç»Ÿè®¡å›¾

            plt.tight_layout()

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_latency_distribution_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=300)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ å»¶è¿Ÿåˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                self.logger.info(f"å»¶è¿Ÿåˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºå»¶è¿Ÿåˆ†å¸ƒå›¾")
                plt.show()
                self.logger.info(f"æ˜¾ç¤ºå»¶è¿Ÿåˆ†å¸ƒå›¾")
                return ""

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå»¶è¿Ÿåˆ†å¸ƒå›¾å¤±è´¥: {e}")
            return ""

    def plot_port_bandwidth_comparison(self, ip_analysis: Dict[str, Any], save_dir: str = "output", save_figures: bool = True, verbose: bool = True) -> str:
        """ç”ŸæˆIPç±»å‹å¸¦å®½å¯¹æ¯”å›¾"""
        if not ip_analysis:
            return ""

        try:
            ip_types = list(ip_analysis.keys())

            # åˆ›å»ºå›¾è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))

            x = np.arange(len(ip_types))
            width = 0.35

            # æå–è¯»å†™å¸¦å®½æ•°æ®
            read_bw = [float(ip_analysis[ip_type]["è¯»å¸¦å®½_GB/s"]) for ip_type in ip_types]
            write_bw = [float(ip_analysis[ip_type]["å†™å¸¦å®½_GB/s"]) for ip_type in ip_types]

            bars1 = ax.bar(x - width / 2, read_bw, width, label="è¯»å¸¦å®½", color="green", alpha=0.7)
            bars2 = ax.bar(x + width / 2, write_bw, width, label="å†™å¸¦å®½", color="red", alpha=0.7)

            ax.set_xlabel("IPç±»å‹", fontsize=12)
            ax.set_ylabel("å¸¦å®½ (GB/s)", fontsize=12)
            ax.set_title("å„IPç±»å‹å¸¦å®½å¯¹æ¯”", fontsize=14)
            ax.set_xticks(x)
            ax.set_xticklabels(ip_types)
            ax.legend(prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 9})
            ax.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars1:
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f"{height:.2f}", ha="center", va="bottom", fontsize=10)

            for bar in bars2:
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f"{height:.2f}", ha="center", va="bottom", fontsize=10)

            # æ·»åŠ è¯·æ±‚æ•°é‡ä¿¡æ¯
            for i, ip_type in enumerate(ip_types):
                total_requests = ip_analysis[ip_type]["æ€»è¯·æ±‚æ•°"]
                read_requests = ip_analysis[ip_type]["è¯»è¯·æ±‚æ•°"]
                write_requests = ip_analysis[ip_type]["å†™è¯·æ±‚æ•°"]

                # åœ¨Xè½´æ ‡ç­¾ä¸‹æ–¹æ·»åŠ è¯·æ±‚æ•°ä¿¡æ¯
                ax.text(
                    i, -max(max(read_bw), max(write_bw)) * 0.1, f"æ€»è¯·æ±‚: {total_requests}\n(è¯»:{read_requests}, å†™:{write_requests})", ha="center", va="top", fontsize=8, alpha=0.7
                )

            plt.tight_layout()

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_ip_bandwidth_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=150)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ IPå¸¦å®½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
                self.logger.info(f"IPå¸¦å®½å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºIPå¸¦å®½å¯¹æ¯”å›¾")
                plt.show()
                self.logger.info(f"æ˜¾ç¤ºIPå¸¦å®½å¯¹æ¯”å›¾")
                return ""

        except Exception as e:
            self.logger.error(f"ç”ŸæˆIPå¸¦å®½å¯¹æ¯”å›¾å¤±è´¥: {e}")
            return ""

    def save_results(self, analysis: Dict[str, Any], save_dir: str = "output") -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            timestamp = int(time.time())
            results_file = f"{save_dir}/crossring_analysis_{timestamp}.json"

            os.makedirs(save_dir, exist_ok=True)
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)

            self.logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            return results_file
        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return ""

    def plot_traffic_distribution(self, model, metrics, save_dir: str = "output", mode: str = "total", save_figures: bool = True, verbose: bool = True) -> str:
        """
        ç»˜åˆ¶æµé‡åˆ†å¸ƒå›¾ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹IPå¸¦å®½å’Œé“¾è·¯å¸¦å®½

        Args:
            model: CrossRingModelå®ä¾‹
            metrics: è¯·æ±‚åº¦é‡æ•°æ®
            save_dir: ä¿å­˜ç›®å½•
            mode: æ˜¾ç¤ºæ¨¡å¼ ("read", "write", "total")

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not metrics:
            return ""

        try:
            # è·å–èŠ‚ç‚¹æ•°é‡å’Œæ‹“æ‰‘ä¿¡æ¯
            num_nodes = len(model.nodes)
            num_rows = model.config.NUM_ROW
            num_cols = model.config.NUM_COL

            # æ”¶é›†èŠ‚ç‚¹çº§IPå¸¦å®½æ•°æ®å’Œé“¾è·¯å¸¦å®½æ•°æ®
            # æ”¯æŒçš„IPç±»å‹ï¼šGDMA, SDMA, CDMA, DDR, L2Mç­‰ï¼Œç”¨å°å†™å­˜å‚¨
            node_ip_bandwidth = defaultdict(lambda: defaultdict(float))
            link_bandwidth = defaultdict(float)

            # é¦–å…ˆè®¡ç®—æ•´ä½“æ—¶é—´çª—å£
            if not metrics:
                return ""

            overall_start_time = min(metric.start_time for metric in metrics)
            overall_end_time = max(metric.end_time for metric in metrics)
            overall_time_window = overall_end_time - overall_start_time if overall_end_time > overall_start_time else 1

            # æŒ‰IPç±»å‹åˆ†ç»„æ”¶é›†å­—èŠ‚æ•°
            ip_type_bytes = defaultdict(int)
            node_ip_bytes = defaultdict(lambda: defaultdict(int))
            link_bytes = defaultdict(int)

            # åˆ†ææ¯ä¸ªè¯·æ±‚çš„å­—èŠ‚æ•°è´¡çŒ®
            for metric in metrics:
                source_ip_type = metric.source_type.lower()  # gdma/ddr
                dest_ip_type = metric.dest_type.lower()  # gdma/ddr

                # ç´¯è®¡å­—èŠ‚æ•°ï¼ˆä¸æ˜¯å¸¦å®½ï¼‰
                # æºèŠ‚ç‚¹ï¼šå‘é€å­—èŠ‚æ•°
                node_ip_bytes[metric.source_node][source_ip_type] += metric.total_bytes
                # ç›®æ ‡èŠ‚ç‚¹ï¼šæ¥æ”¶å­—èŠ‚æ•°
                node_ip_bytes[metric.dest_node][dest_ip_type] += metric.total_bytes

                # è®¡ç®—é“¾è·¯å­—èŠ‚æ•°ï¼ˆåªå¤„ç†è·¨èŠ‚ç‚¹é€šä¿¡ï¼‰
                if metric.source_node != metric.dest_node:
                    src_row = metric.source_node // num_cols
                    src_col = metric.source_node % num_cols
                    dst_row = metric.dest_node // num_cols
                    dst_col = metric.dest_node % num_cols

                    # æ°´å¹³è·¯ç”±
                    if src_row == dst_row:
                        step = 1 if dst_col > src_col else -1
                        for col in range(src_col, dst_col, step):
                            curr_node = src_row * num_cols + col
                            next_node = src_row * num_cols + col + step
                            if mode == "total" or mode == metric.req_type:
                                link_bytes[(curr_node, next_node)] += metric.total_bytes

                    # å‚ç›´è·¯ç”±
                    elif src_col == dst_col:
                        step = 1 if dst_row > src_row else -1
                        for row in range(src_row, dst_row, step):
                            curr_node = row * num_cols + src_col
                            next_node = (row + step) * num_cols + src_col
                            if mode == "total" or mode == metric.req_type:
                                link_bytes[(curr_node, next_node)] += metric.total_bytes

            # è®¡ç®—æœ€ç»ˆå¸¦å®½ï¼šä½¿ç”¨å·¥ä½œåŒºé—´æ–¹æ³•è®¡ç®—åŠ æƒå¸¦å®½
            # æŒ‰èŠ‚ç‚¹å’ŒIPç±»å‹åˆ†ç»„è¯·æ±‚ï¼Œè®¡ç®—å„è‡ªçš„å·¥ä½œåŒºé—´å¸¦å®½
            for node_id, ip_data in node_ip_bytes.items():
                for ip_type, total_bytes in ip_data.items():
                    # æ‰¾åˆ°è¯¥èŠ‚ç‚¹è¯¥IPç±»å‹çš„æ‰€æœ‰è¯·æ±‚
                    node_ip_requests = []
                    for metric in metrics:
                        if (metric.source_node == node_id and metric.source_type.lower() == ip_type) or (metric.dest_node == node_id and metric.dest_type.lower() == ip_type):
                            node_ip_requests.append(metric)

                    if node_ip_requests:
                        # ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—è¯¥èŠ‚ç‚¹è¯¥IPçš„åŠ æƒå¸¦å®½
                        working_intervals = self.calculate_working_intervals(node_ip_requests, min_gap_threshold=200)

                        # è®¡ç®—åŠ æƒå¸¦å®½
                        if working_intervals:
                            total_weighted_bandwidth = 0.0
                            total_weight = 0

                            for interval in working_intervals:
                                weight = interval.flit_count
                                bandwidth = interval.bandwidth  # GB/s
                                total_weighted_bandwidth += bandwidth * weight
                                total_weight += weight

                            weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                            bandwidth_gbps = weighted_bandwidth  # ç›´æ¥ä½¿ç”¨ï¼Œå·²ç»æ˜¯GB/s
                        else:
                            bandwidth_gbps = 0.0
                    else:
                        bandwidth_gbps = 0.0

                    node_ip_bandwidth[node_id][ip_type] = bandwidth_gbps

            # è®¡ç®—é“¾è·¯å¸¦å®½ï¼šä½¿ç”¨é€šè¿‡è¯¥é“¾è·¯çš„è¯·æ±‚è®¡ç®—å·¥ä½œåŒºé—´å¸¦å®½
            link_bandwidth = {}
            for link_key, total_bytes in link_bytes.items():
                # æ‰¾åˆ°é€šè¿‡è¯¥é“¾è·¯çš„æ‰€æœ‰è¯·æ±‚
                link_requests = []
                curr_node, next_node = link_key

                for metric in metrics:
                    if metric.source_node != metric.dest_node:
                        # æ£€æŸ¥è¯¥è¯·æ±‚æ˜¯å¦é€šè¿‡è¿™æ¡é“¾è·¯
                        src_row = metric.source_node // num_cols
                        src_col = metric.source_node % num_cols
                        dst_row = metric.dest_node // num_cols
                        dst_col = metric.dest_node % num_cols

                        passes_through_link = False

                        # æ°´å¹³è·¯ç”±æ£€æŸ¥
                        if src_row == dst_row:
                            step = 1 if dst_col > src_col else -1
                            for col in range(src_col, dst_col, step):
                                check_curr = src_row * num_cols + col
                                check_next = src_row * num_cols + col + step
                                if (check_curr, check_next) == link_key:
                                    passes_through_link = True
                                    break

                        # å‚ç›´è·¯ç”±æ£€æŸ¥
                        elif src_col == dst_col:
                            step = 1 if dst_row > src_row else -1
                            for row in range(src_row, dst_row, step):
                                check_curr = row * num_cols + src_col
                                check_next = (row + step) * num_cols + src_col
                                if (check_curr, check_next) == link_key:
                                    passes_through_link = True
                                    break

                        if passes_through_link:
                            link_requests.append(metric)

                if link_requests:
                    # ä½¿ç”¨å·¥ä½œåŒºé—´è®¡ç®—é“¾è·¯åŠ æƒå¸¦å®½
                    working_intervals = self.calculate_working_intervals(link_requests, min_gap_threshold=200)

                    if working_intervals:
                        total_weighted_bandwidth = 0.0
                        total_weight = 0

                        for interval in working_intervals:
                            weight = interval.flit_count
                            bandwidth = interval.bandwidth  # GB/s
                            total_weighted_bandwidth += bandwidth * weight
                            total_weight += weight

                        weighted_bandwidth = (total_weighted_bandwidth / total_weight) if total_weight > 0 else 0.0
                        bandwidth_gbps = weighted_bandwidth  # ç›´æ¥ä½¿ç”¨ï¼Œå·²ç»æ˜¯GB/s
                    else:
                        bandwidth_gbps = 0.0
                else:
                    bandwidth_gbps = 0.0

                link_bandwidth[link_key] = bandwidth_gbps

            # è®¡ç®—æ€»IPç±»å‹å¸¦å®½ï¼ˆç”¨äºæ±‡æ€»æ˜¾ç¤ºï¼‰
            # åŠ¨æ€è®¡ç®—æ‰€æœ‰IPç±»å‹çš„æ€»å¸¦å®½
            ip_type_totals = defaultdict(float)
            for node_data in node_ip_bandwidth.values():
                for ip_type, bandwidth in node_data.items():
                    ip_type_totals[ip_type] += bandwidth

            # è®¡ç®—èŠ‚ç‚¹ä½ç½®ï¼ˆç½‘æ ¼å¯¹é½ï¼Œä¸äº¤é”™ï¼‰
            pos = {}
            for node_id in range(num_nodes):
                row = node_id // num_cols
                col = node_id % num_cols
                pos[node_id] = (col * 3, -row * 2)  # è§„æ•´ç½‘æ ¼ï¼Œä¸åç§»

            # åˆ›å»ºå›¾å½¢
            fig, ax = plt.subplots(figsize=(14, 10))
            ax.set_aspect("equal")

            # åŠ¨æ€è®¡ç®—å­—ä½“å¤§å°
            base_font = 10
            dynamic_font = min(14, max(6, base_font * (65 / num_nodes) ** 0.5))

            # èŠ‚ç‚¹å¤§å°
            node_size = 4000  # å¢å¤§èŠ‚ç‚¹
            square_size = np.sqrt(node_size) / 60  # è°ƒæ•´èŠ‚ç‚¹å¤§å°ï¼Œæ›´å¤§çš„æ–¹æ¡†

            # æ·»åŠ æ‰€æœ‰å¯èƒ½çš„é“¾è·¯ï¼ˆåŒ…æ‹¬æ²¡æœ‰æµé‡çš„ï¼‰
            all_links = set()
            for node_id in range(num_nodes):
                row = node_id // num_cols
                col = node_id % num_cols

                # æ°´å¹³è¿æ¥
                if col < num_cols - 1:
                    all_links.add((node_id, node_id + 1))
                    all_links.add((node_id + 1, node_id))

                # å‚ç›´è¿æ¥
                if row < num_rows - 1:
                    all_links.add((node_id, node_id + num_cols))
                    all_links.add((node_id + num_cols, node_id))

            # ç»˜åˆ¶æ‰€æœ‰é“¾è·¯ï¼ˆåŒ…æ‹¬æ— æµé‡çš„ï¼‰ï¼Œä½¿ç”¨åŒå‘ç®­å¤´æ˜¾ç¤º
            max_link_bw = max(link_bandwidth.values()) if link_bandwidth else 1.0

            # ä¸ºäº†é¿å…åŒå‘ç®­å¤´é‡å ï¼Œéœ€è¦ä¸ºæ¯ä¸ªæ–¹å‘è®¡ç®—åç§»
            for src, dst in all_links:
                x1, y1 = pos[src]
                x2, y2 = pos[dst]

                bandwidth = link_bandwidth.get((src, dst), 0.0)

                # è®¡ç®—é“¾è·¯é¢œè‰²å’Œçº¿å®½
                if bandwidth > 0:
                    intensity = min(1.0, bandwidth / max_link_bw)
                    color = (intensity, 0, 0)
                    linewidth = 1 + intensity * 2  # å‡å°çº¿å®½
                    alpha = 0.9
                else:
                    color = (0.7, 0.7, 0.7)  # ç°è‰²è¡¨ç¤ºæ— æµé‡
                    linewidth = 0.8  # å‡å°æ— æµé‡é“¾è·¯çº¿å®½
                    alpha = 0.5

                # è®¡ç®—åŸºæœ¬æ–¹å‘å‘é‡
                dx, dy = x2 - x1, y2 - y1
                dist = np.hypot(dx, dy)
                if dist > 0:
                    dx, dy = dx / dist, dy / dist

                    # è®¡ç®—å‚ç›´åç§»å‘é‡ï¼ˆç”¨äºåˆ†ç¦»åŒå‘ç®­å¤´ï¼‰
                    perp_dx, perp_dy = -dy, dx  # å‚ç›´æ–¹å‘
                    offset = 0.08  # å‡å°åç§»è·ç¦»ï¼Œè®©åŒå‘ç®­å¤´æ›´è¿‘

                    # ä¸ºè¯¥æ–¹å‘çš„ç®­å¤´æ·»åŠ åç§»
                    offset_x1 = x1 + perp_dx * offset
                    offset_y1 = y1 + perp_dy * offset
                    offset_x2 = x2 + perp_dx * offset
                    offset_y2 = y2 + perp_dy * offset

                    # è®¡ç®—ä»åç§»åèŠ‚ç‚¹è¾¹ç¼˜çš„èµ·æ­¢ç‚¹
                    start_x = offset_x1 + dx * square_size / 2
                    start_y = offset_y1 + dy * square_size / 2
                    end_x = offset_x2 - dx * square_size / 2
                    end_y = offset_y2 - dy * square_size / 2

                    # ç»˜åˆ¶å¸¦ç®­å¤´çš„è¿æ¥çº¿
                    arrow = FancyArrowPatch(
                        (start_x, start_y),
                        (end_x, end_y),
                        arrowstyle="-|>",
                        mutation_scale=dynamic_font * 1.2,
                        color=color,
                        linewidth=linewidth,
                        alpha=alpha,
                        zorder=1,  # å¢å¤§ç®­å¤´å¤§å°
                    )
                    ax.add_patch(arrow)

                    # å¦‚æœæœ‰å¸¦å®½ï¼Œåœ¨ç®­å¤´æ—è¾¹æ·»åŠ æ ‡ç­¾
                    if bandwidth > 0:
                        # åˆ¤æ–­é“¾è·¯æ–¹å‘å¹¶å†³å®šæ ‡ç­¾ä½ç½®ï¼ˆæ›´é è¿‘é“¾è·¯ï¼Œå­—ä½“æ›´å¤§ï¼‰
                        if abs(dx) > abs(dy):  # æ°´å¹³é“¾è·¯
                            # æ°´å¹³é“¾è·¯æ ‡ç­¾æ”¾åœ¨ä¸Šä¸‹
                            label_offset_x = 0
                            label_offset_y = 0.2 if src < dst else -0.2  # å‡å°è·ç¦»
                        else:  # å‚ç›´é“¾è·¯
                            # å‚ç›´é“¾è·¯æ ‡ç­¾æ”¾åœ¨å·¦å³
                            label_offset_x = 0.2 if src < dst else -0.2  # å‡å°è·ç¦»
                            label_offset_y = 0

                        mid_x = (start_x + end_x) / 2 + label_offset_x
                        mid_y = (start_y + end_y) / 2 + label_offset_y

                        ax.text(
                            mid_x,
                            mid_y,
                            f"{bandwidth:.1f}",
                            ha="center",
                            va="center",
                            fontsize=dynamic_font * 0.7,  # å¢å¤§å­—ä½“
                            bbox=dict(boxstyle="round,pad=0.1", facecolor="white", alpha=0.8),
                            color=color,
                            fontweight="bold",
                        )

            # ç»˜åˆ¶èŠ‚ç‚¹å’ŒIPä¿¡æ¯
            for node_id in range(num_nodes):
                x, y = pos[node_id]

                # ç»˜åˆ¶ä¸»èŠ‚ç‚¹æ–¹æ¡†
                rect = Rectangle((x - square_size / 2, y - square_size / 2), width=square_size, height=square_size, color="lightblue", ec="black", linewidth=2, zorder=2)
                ax.add_patch(rect)

                # èŠ‚ç‚¹ç¼–å·å’ŒIPå¸¦å®½ä¿¡æ¯å†™åœ¨æ–¹æ¡†å†…
                # è·å–è¯¥èŠ‚ç‚¹çš„å®é™…IPå¸¦å®½ï¼ˆåŠ¨æ€æ”¯æŒæ‰€æœ‰IPç±»å‹ï¼‰
                node_ip_data = node_ip_bandwidth[node_id]

                # IPç±»å‹é¦–å­—æ¯æ˜ å°„
                def get_ip_abbreviation(ip_type):
                    """è·å–IPç±»å‹çš„é¦–å­—æ¯ç¼©å†™"""
                    return ip_type.upper()[0] if ip_type else ""

                # æ‰¾å‡ºè¯¥èŠ‚ç‚¹æœ‰å¸¦å®½çš„IPç±»å‹
                active_ips = [(ip_type, bw) for ip_type, bw in node_ip_data.items() if bw > 0]

                if len(active_ips) == 0:
                    ip_text = ""  # æ— æµé‡æ—¶ä¸æ˜¾ç¤ºä»»ä½•æ–‡å­—
                elif len(active_ips) == 1:
                    ip_type, bw = active_ips[0]
                    ip_abbrev = get_ip_abbreviation(ip_type)
                    ip_text = f"{ip_abbrev}: {bw:.1f}"
                else:
                    # å¤šä¸ªIPç±»å‹ï¼Œæ¯ä¸ªIPç±»å‹åˆ†è¡Œæ˜¾ç¤º
                    ip_lines = []
                    for ip_type, bw in active_ips:
                        ip_abbrev = get_ip_abbreviation(ip_type)
                        ip_lines.append(f"{ip_abbrev}: {bw:.1f}")
                    ip_text = "\n".join(ip_lines)

                # åœ¨èŠ‚ç‚¹æ–¹æ¡†å†…æ˜¾ç¤ºä¿¡æ¯
                if ip_text:
                    node_text = f"{node_id}\n{ip_text}"
                else:
                    node_text = f"{node_id}"
                ax.text(x, y, node_text, ha="center", va="center", fontsize=dynamic_font * 0.8, fontweight="bold")  # å¢å¤§å­—ä½“

            # æ·»åŠ æ€»ç»“ä¿¡æ¯æ¡†ï¼ˆåŠ¨æ€æ˜¾ç¤ºæ‰€æœ‰IPç±»å‹ï¼‰
            summary_lines = ["æ€»å¸¦å®½ç»Ÿè®¡:"]
            for ip_type, total_bw in sorted(ip_type_totals.items()):
                ip_display = ip_type.upper()  # æ˜¾ç¤ºå¤§å†™
                summary_lines.append(f"{ip_display}: {total_bw:.2f} GB/s")

            summary_text = "\n".join(summary_lines)
            ax.text(0.02, 0.98, summary_text, transform=ax.transAxes, fontsize=12, verticalalignment="top", bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.9))

            # è®¾ç½®æ ‡é¢˜å’Œå¸ƒå±€
            title = f"CrossRing NoC æµé‡åˆ†å¸ƒå›¾ ({mode.upper()}æ¨¡å¼)"
            ax.set_title(title, fontsize=16, fontweight="bold")

            # è®¾ç½®åæ ‡è½´èŒƒå›´
            all_x = [x for x, y in pos.values()]
            all_y = [y for x, y in pos.values()]
            margin = 1.5
            ax.set_xlim(min(all_x) - margin, max(all_x) + margin)
            ax.set_ylim(min(all_y) - margin, max(all_y) + margin)
            ax.axis("off")

            # æ·»åŠ å›¾ä¾‹
            legend_elements = [
                mpatches.Patch(color="lightblue", label="èŠ‚ç‚¹(å«IPä¿¡æ¯)"),
                mpatches.Patch(color="red", label="é«˜å¸¦å®½é“¾è·¯"),
                mpatches.Patch(color="gray", label="æ— æµé‡é“¾è·¯"),
                mpatches.Patch(color="lightgray", label="å¸¦å®½ç»Ÿè®¡"),
            ]
            ax.legend(handles=legend_elements, loc="upper right", prop={"family": ["Times New Roman", "Microsoft YaHei", "SimHei"], "size": 10})

            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if save_figures:
                timestamp = int(time.time())
                save_path = f"{save_dir}/crossring_traffic_distribution_{timestamp}.png"
                os.makedirs(save_dir, exist_ok=True)
                fig.savefig(save_path, bbox_inches="tight", dpi=150)
                plt.close(fig)
                if verbose:
                    print(f"ğŸ“ æµé‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                self.logger.info(f"æµé‡åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: {save_path}")
                return save_path
            else:
                if verbose:
                    print(f"ğŸ“Š æ˜¾ç¤ºæµé‡åˆ†å¸ƒå›¾")
                plt.show()
                self.logger.info(f"æ˜¾ç¤ºæµé‡åˆ†å¸ƒå›¾")
                return ""

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæµé‡åˆ†å¸ƒå›¾å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return ""

    def analyze_noc_results(
        self,
        request_tracker,
        config,
        model,
        results: Dict[str, Any],
        enable_visualization: bool = True,
        save_results: bool = True,
        save_dir: str = "output",
        save_figures: bool = True,
        verbose: bool = True,
    ) -> Dict[str, Any]:
        """
        NoCä»¿çœŸç»“æœå®Œæ•´åˆ†æ

        Args:
            request_tracker: RequestTrackerå®ä¾‹
            config: NoCé…ç½®
            model: NoCæ¨¡å‹å®ä¾‹
            results: ä»¿çœŸåŸºç¡€ç»“æœ
            enable_visualization: æ˜¯å¦ç”Ÿæˆå›¾è¡¨
            save_results: æ˜¯å¦ä¿å­˜ç»“æœæ–‡ä»¶

        Returns:
            è¯¦ç»†çš„åˆ†æç»“æœï¼ˆä¸­æ–‡æ ¼å¼ï¼‰
        """
        analysis = {}

        # åŸºç¡€æŒ‡æ ‡
        sim_info = results.get("simulation_info", {})
        total_requests = len(request_tracker.completed_requests) + len(request_tracker.active_requests)
        completed_requests = len(request_tracker.completed_requests)
        active_requests = len(request_tracker.active_requests)

        analysis["åŸºç¡€æŒ‡æ ‡"] = {
            "æ€»å‘¨æœŸæ•°": sim_info.get("total_cycles", 0),
            "æ€»è¯·æ±‚æ•°": total_requests,
            "å·²å®Œæˆè¯·æ±‚æ•°": completed_requests,
            "æ´»è·ƒè¯·æ±‚æ•°": active_requests,
            "å®Œæˆç‡": f"{(completed_requests / total_requests * 100):.2f}%" if total_requests > 0 else "0.00%",
        }

        # è½¬æ¢æ•°æ®æ ¼å¼
        metrics = self.convert_tracker_to_request_info(request_tracker, config)

        if not metrics:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„è¯·æ±‚æ•°æ®")
            return analysis

        # æ·»åŠ è¯¦ç»†æ•°æ®ç»Ÿè®¡è¾“å‡º
        self._print_data_statistics(metrics)

        # å¸¦å®½åˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["å¸¦å®½æŒ‡æ ‡"] = self.analyze_bandwidth(metrics, verbose=verbose)

        # å»¶è¿Ÿåˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["å»¶è¿ŸæŒ‡æ ‡"] = self.analyze_latency(metrics, verbose=verbose)

        # ç«¯å£å¸¦å®½åˆ†æ
        analysis["ç«¯å£å¸¦å®½åˆ†æ"] = self.analyze_port_bandwidth(metrics, verbose=verbose)

        # Tagå’Œç»•ç¯æ•°æ®åˆ†æï¼ˆåœ¨åˆ†ææ—¶åŒæ—¶æ‰“å°ï¼‰
        analysis["Tagå’Œç»•ç¯åˆ†æ"] = self.analyze_tag_data(model, verbose=verbose)

        # ç”Ÿæˆå›¾è¡¨
        if enable_visualization:
            chart_paths = []

            # å¸¦å®½æ›²çº¿å›¾
            bw_path = self.plot_bandwidth_curves(metrics, save_dir=save_dir, save_figures=save_figures, verbose=verbose)
            if bw_path:
                chart_paths.append(bw_path)

            # å»¶è¿Ÿåˆ†å¸ƒå›¾
            lat_path = self.plot_latency_distribution(metrics, save_dir=save_dir, save_figures=save_figures, verbose=verbose)
            if lat_path:
                chart_paths.append(lat_path)

            # ç«¯å£å¸¦å®½å¯¹æ¯”å›¾å·²ç§»é™¤

            # æµé‡åˆ†å¸ƒå›¾
            traffic_path = self.plot_traffic_distribution(model, metrics, save_dir=save_dir, mode="total", save_figures=save_figures, verbose=verbose)
            if traffic_path:
                chart_paths.append(traffic_path)

            analysis["å¯è§†åŒ–æ–‡ä»¶"] = {"ç”Ÿæˆçš„å›¾è¡¨": chart_paths, "å›¾è¡¨æ•°é‡": len(chart_paths)}

        # ä¿å­˜ç»“æœæ–‡ä»¶
        if save_results:
            # ä¿å­˜åˆ†æç»“æœJSON
            results_file = self.save_results(analysis, save_dir=save_dir)

            # ä¿å­˜è¯¦ç»†è¯·æ±‚CSVæ–‡ä»¶
            csv_files = self.save_detailed_requests_csv(metrics, save_dir=save_dir)

            # ä¿å­˜ç«¯å£å¸¦å®½CSVæ–‡ä»¶
            ports_csv = self.save_ports_bandwidth_csv(metrics, save_dir=save_dir, config=config)

            output_files = {}
            if results_file:
                output_files["åˆ†æç»“æœæ–‡ä»¶"] = results_file

            # æ·»åŠ CSVæ–‡ä»¶ä¿¡æ¯
            if csv_files:
                if "read_requests_csv" in csv_files:
                    output_files["è¯»è¯·æ±‚CSV"] = csv_files["read_requests_csv"]
                if "write_requests_csv" in csv_files:
                    output_files["å†™è¯·æ±‚CSV"] = csv_files["write_requests_csv"]

            if ports_csv:
                output_files["ç«¯å£å¸¦å®½CSV"] = ports_csv

            if output_files:
                analysis["è¾“å‡ºæ–‡ä»¶"] = {
                    **output_files,
                    "ä¿å­˜æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
                }

        return analysis
